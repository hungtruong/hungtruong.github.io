{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14639751,"sourceType":"datasetVersion","datasetId":9344612}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Qwen3-TTS Voice Clone Demo\n\nThis notebook demonstrates how to run Qwen3-TTS Voice Cloning.","metadata":{}},{"cell_type":"code","source":"# 1. Install Dependencies\n# Install system dependencies first (fixes 'sox: not found' errors)\n!sudo apt-get update && sudo apt-get install -y sox libsox-dev ffmpeg\n\n!pip install -U qwen-tts\n# flash-attn is recommended for performance\n!pip install -U flash-attn --no-build-isolation\n!pip install pyngrok\n!pip install modelscope\n!pip install boto3 requests beautifulsoup4 pysbd\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Imports\nimport torch\nimport soundfile as sf\nfrom IPython.display import Audio\nfrom qwen_tts import Qwen3TTSModel\nimport os\nimport threading\nimport time","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Load Model (Voice Clone Base 1.7B) - Dual GPU Support\nimport torch\nfrom qwen_tts import Qwen3TTSModel\n\nmodels_pool = []\ngpu_count = torch.cuda.device_count()\n\nprint(f\"Detected {gpu_count} GPU(s).\")\n\nif gpu_count >= 2:\n    print(\"üöÄ Dual-GPU Mode Activated!\")\n    # Load Model 1 on GPU 0\n    print(\"Loading Model 0 on cuda:0 ...\")\n    model_0 = Qwen3TTSModel.from_pretrained(\n        \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n        device_map=\"cuda:0\",\n        dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n    )\n    models_pool.append(model_0)\n    \n    # Load Model 2 on GPU 1\n    print(\"Loading Model 1 on cuda:1 ...\")\n    model_1 = Qwen3TTSModel.from_pretrained(\n        \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n        device_map=\"cuda:1\",\n        dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n    )\n    models_pool.append(model_1)\n    \n    # For backward compatibility if single model is referenced\n    model = model_0 \n\nelse:\n    print(\"Single-GPU Mode.\")\n    print(\"Loading Model on cuda:0 ...\")\n    model = Qwen3TTSModel.from_pretrained(\n        \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n        device_map=\"cuda:0\",\n        dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n    )\n    models_pool.append(model)\n\nprint(f\"‚úÖ Loaded {len(models_pool)} model instance(s).\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Batch Blog Scraper\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport pysbd\nfrom urllib.parse import urljoin\n\n# --- BATCH CONFIGURATION ---\nARCHIVES_URL = \"https://www.hung-truong.com/blog/archives/\"\nPOST_OFFSET = 0    # Skip the first N posts\nBATCH_LIMIT = 5    # Max posts to process in one run\n\ndef get_post_links(archives_url):\n    try:\n        response = requests.get(archives_url)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, 'html.parser')\n        links = []\n        for a in soup.find_all('a', href=True):\n            href = a['href']\n            if re.match(r'.*/blog/\\d{4}/\\d{2}/\\d{2}/.+', href):\n                full_url = href if href.startswith(\"http\") else urljoin(\"https://www.hung-truong.com\", href)\n                if full_url not in links:\n                    links.append(full_url)\n        return links\n    except Exception as e:\n        print(f\"Error fetching archives: {e}\")\n        return []\n\ndef parse_post_content(url):\n    print(f\"Checking {url}...\")\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, 'html.parser')\n        if soup.find('div', class_='audio-header'):\n            print(f\"‚è© Skipping {url} (Audio header found)\")\n            return None\n        content_div = soup.find('div', class_='content')\n        if not content_div:\n            return None\n        slug = url.rstrip('/').split('/')[-1]\n        post_data = {'url': url, 'slug': slug, 'title': None, 'segments': []}\n        seg = pysbd.Segmenter(language=\"en\", clean=False)\n        for unwanted in content_div.find_all(['figcaption', 'pre']):\n            unwanted.decompose()\n        title_tag = soup.find('h1', class_='postTitle')\n        if title_tag:\n            post_data['title'] = title_tag.get_text().strip()\n            post_data['segments'].append({'text': post_data['title'], 'type': 'header'})\n        meta_tag = soup.find('p', class_='meta')\n        if meta_tag:\n            meta_text = meta_tag.get_text().strip()\n            date_text = meta_text.split('|')[0].strip() if \"|\" in meta_text else meta_text\n            post_data['segments'].append({'text': f\"Published on {date_text}.\", 'type': 'paragraph_end'})\n        tags_to_find = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote']\n        all_elements = content_div.find_all(tags_to_find)\n        elements = [el for el in all_elements if not any(parent in all_elements for parent in el.parents)]\n        for el in elements:\n            if 'meta' in el.get('class', []): continue\n            text = el.get_text(separator=' ', strip=True)\n            text = re.sub(r'\\s+', ' ', text)\n            if not text: continue\n            if el.name.startswith('h'):\n                if \"Leave a Comment\" in text: break\n                if post_data['title'] and text == post_data['title']: continue\n                post_data['segments'].append({'text': text, 'type': 'header'})\n            else:\n                sentences = seg.segment(text)\n                processed_sentences = []\n                current_chunk = \"\"\n                for sent in sentences:\n                    sent = sent.strip()\n                    if not sent: continue\n                    if current_chunk: current_chunk += \" \" + sent\n                    else: current_chunk = sent\n                    if len(current_chunk.split()) >= 3:\n                        processed_sentences.append(current_chunk)\n                        current_chunk = \"\"\n                if current_chunk:\n                    if processed_sentences: processed_sentences[-1] += \" \" + current_chunk\n                    else: processed_sentences.append(current_chunk)\n                for i, s in enumerate(processed_sentences):\n                    t = 'paragraph_end' if i == len(processed_sentences) - 1 else 'sentence'\n                    post_data['segments'].append({'text': s, 'type': t})\n        return post_data\n    except Exception as e:\n        print(f\"‚ùå Error parsing {url}: {e}\")\n        return None\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport requests\nimport boto3\nimport os\nimport numpy as np\nimport soundfile as sf\nimport subprocess\nfrom IPython.display import Audio\nfrom datetime import datetime\nimport time\nimport concurrent.futures\nimport threading\nimport queue\n\n# --- SECRETS & CONFIG ---\nuser_secrets = UserSecretsClient()\nS3_ACCESS_KEY = user_secrets.get_secret(\"S3_ACCESS_KEY\")\nS3_BUCKET_NAME = user_secrets.get_secret(\"S3_BUCKET_NAME\")\nS3_ENDPOINT_URL = user_secrets.get_secret(\"S3_ENDPOINT_URL\")\nS3_SECRET_KEY = user_secrets.get_secret(\"S3_SECRET_KEY\")\n\nGITHUB_REPO = \"hungtruong/jekyll-blog\"\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\nPUBLIC_URL_BASE = \"https://pub-2289fc0aae4245debaa2fd741bdf5605.r2.dev/blogaudio/\"\n\n# Check Models\nif 'models_pool' not in locals() or not models_pool:\n    raise ValueError(\"‚ùå No models found! Please run Cell 3 first.\")\n\nprint(f\"‚ö° Using {len(models_pool)} model instance(s) for generation.\")\n\n# Helper for VTT Time\ndef format_vtt_time(seconds):\n    m, s = divmod(seconds, 60)\n    h, m = divmod(m, 60)\n    return f\"{int(h):02d}:{int(m):02d}:{s:06.3f}\"\n\ndef get_audio_duration(filename):\n    try:\n        result = subprocess.run(\n            [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\", \"-of\", \"default=noprint_wrappers=1:nokey=1\", filename],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True\n        )\n        try:\n            return float(result.stdout.strip())\n        except ValueError:\n            return 0.0\n    except Exception as e:\n        print(f\"Error checking duration: {e}\")\n        return 0.0\n\n# Reference Audio\nlocal_ref_audio = \"/kaggle/input/voice-cloning-dataset/longblog2.wav\"\nref_text_content = \"I was at Whole Foods today getting some groceries when I came across this mini food testing area at the end of an aisle. There were two nice sales people (one lady and one dude) who were hawking cereal. The type of cereal was super organic and it came in a pouch. The lady bragged that all of the ingredients were on the front of the bag in large type. The cereal was available for testing in cereal form, baked into a cookie, and blended into a smoothie (which was apparently made with apple cider and yogurt or something). Sidenote: While I was deciding what to taste test (I eventually went with the smoothie and it was not bad, and followed up with a chunk of cookie), an old Asian lady walked up to me and started talking in Chinese. I tried to tell her that I don‚Äôt really speak Chinese, but I forgot how to say ‚ÄúI don‚Äôt know Chinese‚Äù in Chinese. It‚Äôs kind of absurd, anyway, to say you don‚Äôt speak a language in that very language you‚Äôre saying you don't speak. Anyway, she mumbled some more stuff and then said ‚ÄúChinese.‚Äù Like, yeah, lady, we're both Chinese. I guess she walked away after that. So anyway, here's the real part of the story. I'm tasting the cookie and am about to leave when another woman walks up to the food tasting area. The sales guy asks if she wants to buy some cereal and she's like ‚Äúoh, I already have some at home! I love it! I'm just going to have some samples.‚Äù\"\n\n# --- WORKER FUNCTION ---\ndef generate_segment(task):\n    \"\"\"\n    Task tuple: (index, text, item_type)\n    Returns: (index, audio_array, silence_array, sr, text)\n    \"\"\"\n    idx, text, item_type = task\n    \n    try:\n        # Get a model from the pool (Queue)\n        model_instance = model_queue.get() \n        \n        try:\n            # Generate\n            wavs, sr = model_instance.generate_voice_clone(\n                text=text,\n                language=\"English\",\n                ref_audio=local_ref_audio,\n                ref_text=ref_text_content,\n            )\n            audio_chunk = wavs[0]\n            \n            # Silence\n            if item_type == 'header': silence_dur = 1.5\n            elif item_type == 'paragraph_end': silence_dur = 1.0\n            else: silence_dur = 0.5\n            \n            silence_samples = int(silence_dur * sr)\n            silence_chunk = np.zeros(silence_samples, dtype=np.float32)\n            \n            return (idx, audio_chunk, silence_chunk, sr, text)\n            \n        finally:\n            # Always return model to queue\n            model_queue.put(model_instance)\n            \n    except Exception as e:\n        print(f\"‚ùå Error in segment {idx}: {e}\")\n        return (idx, None, None, 24000, text) # Return placeholder on fail\n\n# --- BATCH EXECUTION ---\nif 'batch_queue' not in locals() or not batch_queue:\n    print(\"‚ö†Ô∏è No posts in batch_queue.\")\nelif S3_ACCESS_KEY == \"YOUR_ACCESS_KEY\":\n    print(\"‚ö†Ô∏è PLEASE SET YOUR S3 CONFIGURATIONS IN SECRETS ‚ö†Ô∏è\")\nelse:\n    print(f\"üöÄ Starting Dual-GPU Batch Generation for {len(batch_queue)} posts...\")\n\n    # Init Model Queue\n    model_queue = queue.Queue()\n    for m in models_pool:\n        model_queue.put(m)\n\n    # Init S3 Client\n    try:\n        s3 = boto3.client(\n            's3',\n            endpoint_url=S3_ENDPOINT_URL,\n            aws_access_key_id=S3_ACCESS_KEY,\n            aws_secret_access_key=S3_SECRET_KEY\n        )\n    except Exception as e:\n        print(f\"‚ùå Failed to init S3 client: {e}\")\n        s3 = None\n\n    if s3:\n        for idx_post, post in enumerate(batch_queue):\n            print(f\"\\n[{idx_post+1}/{len(batch_queue)}] Processing: {post['slug']}\")\n            \n            lines_to_process = post['segments']\n            tasks = []\n            valid_count = 0\n            \n            # Prepare Tasks\n            for i, item in enumerate(lines_to_process):\n                text = item.get('text', '').strip()\n                if not text or len(text) < 2: continue\n                tasks.append((valid_count, text, item.get('type', 'sentence')))\n                valid_count += 1\n            \n            print(f\"   Generating {len(tasks)} segments with {len(models_pool)} threads...\")\n            \n            # Parallel Execution\n            results = []\n            with concurrent.futures.ThreadPoolExecutor(max_workers=len(models_pool)) as executor:\n                # Submit all\n                futures = {executor.submit(generate_segment, task): task for task in tasks}\n                \n                # Collect as they complete\n                for i, future in enumerate(concurrent.futures.as_completed(futures)):\n                    res = future.result()\n                    results.append(res)\n                    if i % 10 == 0:\n                        print(f\"   Completed {i}/{len(tasks)} segments...\")\n\n            # Sort by original index\n            results.sort(key=lambda x: x[0])\n            \n            # Reassemble\n            all_wavs = []\n            vtt_lines = [\"WEBVTT\\n\"]\n            total_samples = 0\n            sr = None # Will determine from first valid segment\n            \n            for res in results:\n                idx, audio_chunk, silence_chunk, res_sr, text = res\n                if audio_chunk is None: continue\n                \n                # Check SR consistency\n                if sr is None:\n                    sr = res_sr\n                    print(f\"   Detected Sample Rate: {sr} Hz\")\n                elif sr != res_sr:\n                    print(f\"‚ö†Ô∏è Warning: Sample Rate Mismatch at segment {idx}. Expected {sr}, got {res_sr}. Timing will be off!\")\n                \n                # Audio Length\n                audio_len = len(audio_chunk)\n                \n                # VTT Times\n                start_time_str = format_vtt_time(total_samples / sr)\n                end_time_str = format_vtt_time((total_samples + audio_len) / sr)\n                \n                vtt_lines.append(f\"{start_time_str} --> {end_time_str}\")\n                vtt_lines.append(f\"{text}\\n\")\n                \n                # Append Audio + Silence\n                all_wavs.append(audio_chunk)\n                all_wavs.append(silence_chunk)\n                \n                total_samples += audio_len + len(silence_chunk)\n\n            # --- SAVING ---\n            if all_wavs and sr:\n                BASE_FILENAME = post['slug']\n                OUTPUT_FILENAME_MP3 = f\"{BASE_FILENAME}.mp3\"\n                OUTPUT_FILENAME_VTT = f\"{BASE_FILENAME}.vtt\"\n\n                # WAV\n                temp_wav = \"temp_output.wav\"\n                final_wav = np.concatenate(all_wavs)\n                sf.write(temp_wav, final_wav, sr)\n                final_duration_wav = len(final_wav) / sr\n                \n                print(f\"   WAV Duration: {final_duration_wav:.3f}s\")\n                \n                # MP3 (Use CBR 192k and explicit AR to avoid drift)\n                subprocess.run(\n                    [\n                        \"ffmpeg\", \"-y\", \"-i\", temp_wav, \n                        \"-codec:a\", \"libmp3lame\", \n                        \"-b:a\", \"192k\",       # Constant Bitrate for better timing consistency\n                        \"-ar\", str(sr),       # Enforce same sample rate\n                        \"-map_metadata\", \"-1\", \n                        OUTPUT_FILENAME_MP3\n                    ],\n                    check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n                )\n                \n                # Check Drift from MP3 encoding\n                final_duration_mp3 = get_audio_duration(OUTPUT_FILENAME_MP3)\n                drift = final_duration_mp3 - final_duration_wav\n                print(f\"   MP3 Duration: {final_duration_mp3:.3f}s (Drift: {drift:.4f}s)\")\n                \n                if abs(drift) > 0.5:\n                     print(\"‚ö†Ô∏è Significant MP3 drift detected! VTT might be desynchronized.\")\n\n                # VTT\n                with open(OUTPUT_FILENAME_VTT, \"w\", encoding=\"utf-8\") as f:\n                    f.write(\"\\n\".join(vtt_lines))\n                    \n                # Upload\n                print(f\"   Uploading...\")\n                with open(OUTPUT_FILENAME_MP3, \"rb\") as f:\n                    s3.upload_fileobj(f, S3_BUCKET_NAME, os.path.basename(OUTPUT_FILENAME_MP3))\n                with open(OUTPUT_FILENAME_VTT, \"rb\") as f:\n                    s3.upload_fileobj(f, S3_BUCKET_NAME, os.path.basename(OUTPUT_FILENAME_VTT))\n                    \n                # GitHub Dispatch\n                if GITHUB_TOKEN and \"YOUR_GITHUB\" not in GITHUB_TOKEN:\n                    print(f\"   Triggering Workflow...\")\n                    dispatch_url = f\"https://api.github.com/repos/{GITHUB_REPO}/dispatches\"\n                    headers = {\n                        \"Accept\": \"application/vnd.github.v3+json\",\n                        \"Authorization\": f\"token {GITHUB_TOKEN}\"\n                    }\n                    payload = {\n                        \"event_type\": \"audio-ready\",\n                        \"client_payload\": {\n                            \"slug\": BASE_FILENAME,\n                            \"mp3_url\": f\"{PUBLIC_URL_BASE}{os.path.basename(OUTPUT_FILENAME_MP3)}\",\n                            \"vtt_url\": f\"{PUBLIC_URL_BASE}{os.path.basename(OUTPUT_FILENAME_VTT)}\"\n                        }\n                    }\n                    r = requests.post(dispatch_url, headers=headers, json=payload)\n                    if r.status_code == 204:\n                         print(\"   ‚úÖ Dispatch Sent.\")\n                    else:\n                         print(f\"   ‚ùå Dispatch Failed: {r.status_code}\")\n                         \n                print(f\"‚úÖ Finished: {post['slug']}\")\n\n    print(\"\\\\nüèÅ Dual-GPU Batch Processing Complete.\")\n","metadata":{},"outputs":[],"execution_count":null}]}