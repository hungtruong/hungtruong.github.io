{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "820ba52e",
            "metadata": {},
            "source": [
                "# Qwen3-TTS Voice Clone Demo\n",
                "\n",
                "This notebook demonstrates how to run Qwen3-TTS Voice Cloning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "16e0a1b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "# Install system dependencies first (fixes 'sox: not found' errors)\n",
                "!sudo apt-get update && sudo apt-get install -y sox libsox-dev ffmpeg\n",
                "\n",
                "!pip install -U qwen-tts\n",
                "# flash-attn is recommended for performance\n",
                "!pip install -U flash-attn --no-build-isolation\n",
                "!pip install pyngrok\n",
                "!pip install modelscope\n",
                "!pip install boto3 requests beautifulsoup4 pysbd\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e100f80f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Imports\n",
                "import torch\n",
                "import soundfile as sf\n",
                "from IPython.display import Audio\n",
                "from qwen_tts import Qwen3TTSModel\n",
                "import os\n",
                "import threading\n",
                "import time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4b60fa4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Load Model (Voice Clone Base 1.7B) - Dual GPU Support\n",
                "import torch\n",
                "from qwen_tts import Qwen3TTSModel\n",
                "\n",
                "models_pool = []\n",
                "gpu_count = torch.cuda.device_count()\n",
                "\n",
                "print(f\"Detected {gpu_count} GPU(s).\")\n",
                "\n",
                "if gpu_count >= 2:\n",
                "    print(\"\ud83d\ude80 Dual-GPU Mode Activated!\")\n",
                "    # Load Model 1 on GPU 0\n",
                "    print(\"Loading Model 0 on cuda:0 ...\")\n",
                "    model_0 = Qwen3TTSModel.from_pretrained(\n",
                "        \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n",
                "        device_map=\"cuda:0\",\n",
                "        dtype=torch.bfloat16,\n",
                "        attn_implementation=\"flash_attention_2\",\n",
                "    )\n",
                "    models_pool.append(model_0)\n",
                "    \n",
                "    # Load Model 2 on GPU 1\n",
                "    print(\"Loading Model 1 on cuda:1 ...\")\n",
                "    model_1 = Qwen3TTSModel.from_pretrained(\n",
                "        \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n",
                "        device_map=\"cuda:1\",\n",
                "        dtype=torch.bfloat16,\n",
                "        attn_implementation=\"flash_attention_2\",\n",
                "    )\n",
                "    models_pool.append(model_1)\n",
                "    \n",
                "    # For backward compatibility if single model is referenced\n",
                "    model = model_0 \n",
                "\n",
                "else:\n",
                "    print(\"Single-GPU Mode.\")\n",
                "    print(\"Loading Model on cuda:0 ...\")\n",
                "    model = Qwen3TTSModel.from_pretrained(\n",
                "        \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n",
                "        device_map=\"cuda:0\",\n",
                "        dtype=torch.bfloat16,\n",
                "        attn_implementation=\"flash_attention_2\",\n",
                "    )\n",
                "    models_pool.append(model)\n",
                "\n",
                "print(f\"\u2705 Loaded {len(models_pool)} model instance(s).\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "blog_scraper",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Batch Blog Scraper\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import re\n",
                "import pysbd\n",
                "from urllib.parse import urljoin\n",
                "\n",
                "# --- BATCH CONFIGURATION ---\n",
                "ARCHIVES_URL = \"https://www.hung-truong.com/blog/archives/\"\n",
                "POST_OFFSET = 0    # Skip the first N posts (useful if running iteratively)\n",
                "BATCH_LIMIT = 5    # Max posts to process in one run\n",
                "\n",
                "def get_post_links(archives_url):\n",
                "    try:\n",
                "        response = requests.get(archives_url)\n",
                "        response.raise_for_status()\n",
                "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                "        \n",
                "        links = []\n",
                "        # Archive pages usually list posts reasonably directly. \n",
                "        # Often checking for links that look like /blog/YYYY/MM/DD/slug\n",
                "        for a in soup.find_all('a', href=True):\n",
                "            href = a['href']\n",
                "            # Match strictly posts\n",
                "            if re.match(r'.*/blog/\\d{4}/\\d{2}/\\d{2}/.+', href):\n",
                "                full_url = href if href.startswith(\"http\") else urljoin(\"https://www.hung-truong.com\", href)\n",
                "                if full_url not in links:\n",
                "                    links.append(full_url)\n",
                "        return links\n",
                "    except Exception as e:\n",
                "        print(f\"Error fetching archives: {e}\")\n",
                "        return []\n",
                "\n",
                "def parse_post_content(url):\n",
                "    print(f\"Checking {url}...\")\n",
                "    try:\n",
                "        response = requests.get(url)\n",
                "        response.raise_for_status()\n",
                "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                "        \n",
                "        # 1. Check for Existing Audio\n",
                "        if soup.find('div', class_='audio-header'):\n",
                "            print(f\"\u23e9 Skipping {url} (Audio header found)\")\n",
                "            return None\n",
                "\n",
                "        content_div = soup.find('div', class_='content')\n",
                "        if not content_div:\n",
                "            print(f\"\u26a0\ufe0f Content div not found for {url}\")\n",
                "            return None\n",
                "\n",
                "        # Prepare Data Structure\n",
                "        slug = url.rstrip('/').split('/')[-1]\n",
                "        post_data = {\n",
                "            'url': url,\n",
                "            'slug': slug,\n",
                "            'title': None,\n",
                "            'segments': []\n",
                "        }\n",
                "\n",
                "        # Initialize PySBD\n",
                "        seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
                "        \n",
                "        # A. Remove Captions and Code Blocks\n",
                "        for unwanted in content_div.find_all(['figcaption', 'pre']):\n",
                "            unwanted.decompose()\n",
                "            \n",
                "        # B. Extract Title\n",
                "        title_tag = soup.find('h1', class_='postTitle')\n",
                "        if title_tag:\n",
                "            post_data['title'] = title_tag.get_text().strip()\n",
                "            print(f\"   Found Title: {post_data['title']}\")\n",
                "            post_data['segments'].append({'text': post_data['title'], 'type': 'header'})\n",
                "\n",
                "        # C. Extract Date\n",
                "        meta_tag = soup.find('p', class_='meta')\n",
                "        if meta_tag:\n",
                "            meta_text = meta_tag.get_text().strip()\n",
                "            date_text = meta_text.split('|')[0].strip() if \"|\" in meta_text else meta_text\n",
                "            post_data['segments'].append({'text': f\"Published on {date_text}.\", 'type': 'paragraph_end'})\n",
                "\n",
                "        # D. Content Body\n",
                "        # We include li and blockquote to avoid missing content.\n",
                "        tags_to_find = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote']\n",
                "        all_elements = content_div.find_all(tags_to_find)\n",
                "        # Filter out elements that are nested within other tags_to_find to avoid duplicates\n",
                "        elements = [el for el in all_elements if not any(parent in all_elements for parent in el.parents)]\n",
                "        \n",
                "        for el in elements:\n",
                "            if 'meta' in el.get('class', []): continue\n",
                "            \n",
                "            # Use separator=' ' and normalize whitespace to fix \"broken\" sentences caused by newlines or tags\n",
                "            text = el.get_text(separator=' ', strip=True)\n",
                "            text = re.sub(r'\\s+', ' ', text)\n",
                "            if not text: continue\n",
                "\n",
                "            if el.name.startswith('h'):\n",
                "                if \"Leave a Comment\" in text:\n",
                "                    break # Stop at comments\n",
                "                if post_data['title'] and text == post_data['title']:\n",
                "                    continue # Skip redundant title\n",
                "                post_data['segments'].append({'text': text, 'type': 'header'})\n",
                "            else:\n",
                "                # Sentence Splitting & Merging\n",
                "                sentences = seg.segment(text)\n",
                "                current_chunk = \"\"\n",
                "                processed_sentences = []\n",
                "                \n",
                "                for sent in sentences:\n",
                "                    sent = sent.strip()\n",
                "                    if not sent: continue\n",
                "                    if current_chunk: current_chunk += \" \" + sent\n",
                "                    else: current_chunk = sent\n",
                "                    \n",
                "                    if len(current_chunk.split()) >= 3:\n",
                "                        processed_sentences.append(current_chunk)\n",
                "                        current_chunk = \"\"\n",
                "                \n",
                "                if current_chunk:\n",
                "                    if processed_sentences: processed_sentences[-1] += \" \" + current_chunk\n",
                "                    else: processed_sentences.append(current_chunk)\n",
                "                \n",
                "                for i, sent in enumerate(processed_sentences):\n",
                "                    t = 'paragraph_end' if i == len(processed_sentences) - 1 else 'sentence'\n",
                "                    post_data['segments'].append({'text': sent, 'type': t})\n",
                "                    \n",
                "        return post_data\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"\u274c Error parsing {url}: {e}\")\n",
                "        return None\n",
                "\n",
                "# --- EXECUTION ---\n",
                "print(f\"Fetching archives from {ARCHIVES_URL}...\")\n",
                "all_links = get_post_links(ARCHIVES_URL)\n",
                "\n",
                "# Ensure sorting (Assumes page order is Newest first, or we can look for dates in URL)\n",
                "# Given /blog/YYYY/MM/DD format, reverse lexical sort helps if list isn't ordered\n",
                "# But usually archives are ordered. Let's trust page order but maybe reverse if it's oldest first?\n",
                "# Blog archives usually: Newest on top.\n",
                "print(f\"Found {len(all_links)} total posts.\")\n",
                "\n",
                "# Apply Offset\n",
                "if len(all_links) > POST_OFFSET:\n",
                "    eligible_links = all_links[POST_OFFSET:]\n",
                "    print(f\"Starting check from index {POST_OFFSET} (skipping first {POST_OFFSET}).\")\n",
                "else:\n",
                "    print(\"Offset larger than total posts. Exiting.\")\n",
                "    eligible_links = []\n",
                "\n",
                "batch_queue = []\n",
                "\n",
                "for link in eligible_links:\n",
                "    if len(batch_queue) >= BATCH_LIMIT:\n",
                "        print(f\"Reached batch limit of {BATCH_LIMIT}. Stopping discovery.\")\n",
                "        break\n",
                "        \n",
                "    post_obj = parse_post_content(link)\n",
                "    if post_obj:\n",
                "        print(f\"\u2705 Added to queue: {post_obj['slug']}\")\n",
                "        batch_queue.append(post_obj)\n",
                "\n",
                "print(f\"\\n{'-'*30}\")\n",
                "print(f\"Ready to process {len(batch_queue)} posts.\")\n",
                "for p in batch_queue:\n",
                "    print(f\"- {p['slug']} ({len(p['segments'])} segments)\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "batch_gen_s3",
            "metadata": {},
            "outputs": [],
            "source": [
                "from kaggle_secrets import UserSecretsClient\n",
                "import requests\n",
                "import boto3\n",
                "import os\n",
                "import numpy as np\n",
                "import soundfile as sf\n",
                "import subprocess\n",
                "from IPython.display import Audio\n",
                "from datetime import datetime\n",
                "import time\n",
                "import concurrent.futures\n",
                "import threading\n",
                "import queue\n",
                "\n",
                "# --- SECRETS & CONFIG ---\n",
                "user_secrets = UserSecretsClient()\n",
                "S3_ACCESS_KEY = user_secrets.get_secret(\"S3_ACCESS_KEY\")\n",
                "S3_BUCKET_NAME = user_secrets.get_secret(\"S3_BUCKET_NAME\")\n",
                "S3_ENDPOINT_URL = user_secrets.get_secret(\"S3_ENDPOINT_URL\")\n",
                "S3_SECRET_KEY = user_secrets.get_secret(\"S3_SECRET_KEY\")\n",
                "\n",
                "GITHUB_REPO = \"hungtruong/jekyll-blog\"\n",
                "GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
                "PUBLIC_URL_BASE = \"https://pub-2289fc0aae4245debaa2fd741bdf5605.r2.dev/blogaudio/\"\n",
                "\n",
                "# Check Models\n",
                "if 'models_pool' not in locals() or not models_pool:\n",
                "    raise ValueError(\"\u274c No models found! Please run Cell 3 first.\")\n",
                "\n",
                "print(f\"\u26a1 Using {len(models_pool)} model instance(s) for generation.\")\n",
                "\n",
                "# Helper for VTT Time\n",
                "def format_vtt_time(seconds):\n",
                "    m, s = divmod(seconds, 60)\n",
                "    h, m = divmod(m, 60)\n",
                "    return f\"{int(h):02d}:{int(m):02d}:{s:06.3f}\"\n",
                "\n",
                "def get_audio_duration(filename):\n",
                "    try:\n",
                "        result = subprocess.run(\n",
                "            [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\", \"-of\", \"default=noprint_wrappers=1:nokey=1\", filename],\n",
                "            stdout=subprocess.PIPE,\n",
                "            stderr=subprocess.STDOUT,\n",
                "            text=True\n",
                "        )\n",
                "        try:\n",
                "            return float(result.stdout.strip())\n",
                "        except ValueError:\n",
                "            return 0.0\n",
                "    except Exception as e:\n",
                "        print(f\"Error checking duration: {e}\")\n",
                "        return 0.0\n",
                "\n",
                "# Reference Audio\n",
                "local_ref_audio = \"/kaggle/input/voice-cloning-dataset/longblog2.wav\"\n",
                "ref_text_content = \"I was at Whole Foods today getting some groceries when I came across this mini food testing area at the end of an aisle. There were two nice sales people (one lady and one dude) who were hawking cereal. The type of cereal was super organic and it came in a pouch. The lady bragged that all of the ingredients were on the front of the bag in large type. The cereal was available for testing in cereal form, baked into a cookie, and blended into a smoothie (which was apparently made with apple cider and yogurt or something). Sidenote: While I was deciding what to taste test (I eventually went with the smoothie and it was not bad, and followed up with a chunk of cookie), an old Asian lady walked up to me and started talking in Chinese. I tried to tell her that I don\u2019t really speak Chinese, but I forgot how to say \u201cI don\u2019t know Chinese\u201d in Chinese. It\u2019s kind of absurd, anyway, to say you don\u2019t speak a language in that very language you\u2019re saying you don't speak. Anyway, she mumbled some more stuff and then said \u201cChinese.\u201d Like, yeah, lady, we're both Chinese. I guess she walked away after that. So anyway, here's the real part of the story. I'm tasting the cookie and am about to leave when another woman walks up to the food tasting area. The sales guy asks if she wants to buy some cereal and she's like \u201coh, I already have some at home! I love it! I'm just going to have some samples.\u201d\"\n",
                "\n",
                "# --- WORKER FUNCTION ---\n",
                "def generate_segment(task):\n",
                "    \"\"\"\n",
                "    Task tuple: (index, text, item_type)\n",
                "    Returns: (index, audio_array, silence_array, sr, text)\n",
                "    \"\"\"\n",
                "    idx, text, item_type = task\n",
                "    \n",
                "    try:\n",
                "        # Get a model from the pool (Queue)\n",
                "        model_instance = model_queue.get() \n",
                "        \n",
                "        try:\n",
                "            # Generate\n",
                "            wavs, sr = model_instance.generate_voice_clone(\n",
                "                text=text,\n",
                "                language=\"English\",\n",
                "                ref_audio=local_ref_audio,\n",
                "                ref_text=ref_text_content,\n",
                "            )\n",
                "            audio_chunk = wavs[0]\n",
                "            \n",
                "            # Silence\n",
                "            if item_type == 'header': silence_dur = 1.5\n",
                "            elif item_type == 'paragraph_end': silence_dur = 1.0\n",
                "            else: silence_dur = 0.5\n",
                "            \n",
                "            silence_samples = int(silence_dur * sr)\n",
                "            silence_chunk = np.zeros(silence_samples, dtype=np.float32)\n",
                "            \n",
                "            return (idx, audio_chunk, silence_chunk, sr, text)\n",
                "            \n",
                "        finally:\n",
                "            # Always return model to queue\n",
                "            model_queue.put(model_instance)\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"\u274c Error in segment {idx}: {e}\")\n",
                "        return (idx, None, None, 24000, text) # Return placeholder on fail\n",
                "\n",
                "# --- BATCH EXECUTION ---\n",
                "if 'batch_queue' not in locals() or not batch_queue:\n",
                "    print(\"\u26a0\ufe0f No posts in batch_queue.\")\n",
                "elif S3_ACCESS_KEY == \"YOUR_ACCESS_KEY\":\n",
                "    print(\"\u26a0\ufe0f PLEASE SET YOUR S3 CONFIGURATIONS IN SECRETS \u26a0\ufe0f\")\n",
                "else:\n",
                "    print(f\"\ud83d\ude80 Starting Dual-GPU Batch Generation for {len(batch_queue)} posts...\")\n",
                "\n",
                "    # Init Model Queue\n",
                "    model_queue = queue.Queue()\n",
                "    for m in models_pool:\n",
                "        model_queue.put(m)\n",
                "\n",
                "    # Init S3 Client\n",
                "    try:\n",
                "        s3 = boto3.client(\n",
                "            's3',\n",
                "            endpoint_url=S3_ENDPOINT_URL,\n",
                "            aws_access_key_id=S3_ACCESS_KEY,\n",
                "            aws_secret_access_key=S3_SECRET_KEY\n",
                "        )\n",
                "    except Exception as e:\n",
                "        print(f\"\u274c Failed to init S3 client: {e}\")\n",
                "        s3 = None\n",
                "\n",
                "    if s3:\n",
                "        for idx_post, post in enumerate(batch_queue):\n",
                "            print(f\"\\n[{idx_post+1}/{len(batch_queue)}] Processing: {post['slug']}\")\n",
                "            \n",
                "            lines_to_process = post['segments']\n",
                "            tasks = []\n",
                "            valid_count = 0\n",
                "            \n",
                "            # Prepare Tasks\n",
                "            for i, item in enumerate(lines_to_process):\n",
                "                text = item.get('text', '').strip()\n",
                "                if not text or len(text) < 2: continue\n",
                "                tasks.append((valid_count, text, item.get('type', 'sentence')))\n",
                "                valid_count += 1\n",
                "            \n",
                "            print(f\"   Generating {len(tasks)} segments with {len(models_pool)} threads...\")\n",
                "            \n",
                "            # Parallel Execution\n",
                "            results = []\n",
                "            with concurrent.futures.ThreadPoolExecutor(max_workers=len(models_pool)) as executor:\n",
                "                # Submit all\n",
                "                futures = {executor.submit(generate_segment, task): task for task in tasks}\n",
                "                \n",
                "                # Collect as they complete\n",
                "                for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
                "                    res = future.result()\n",
                "                    results.append(res)\n",
                "                    if i % 10 == 0:\n",
                "                        print(f\"   Completed {i}/{len(tasks)} segments...\")\n",
                "\n",
                "            # Sort by original index\n",
                "            results.sort(key=lambda x: x[0])\n",
                "            \n",
                "            # Reassemble\n",
                "            all_wavs = []\n",
                "            vtt_lines = [\"WEBVTT\\n\"]\n",
                "            total_samples = 0\n",
                "            sr = None # Will determine from first valid segment\n",
                "            \n",
                "            for res in results:\n",
                "                idx, audio_chunk, silence_chunk, res_sr, text = res\n",
                "                if audio_chunk is None: continue\n",
                "                \n",
                "                # Check SR consistency\n",
                "                if sr is None:\n",
                "                    sr = res_sr\n",
                "                    print(f\"   Detected Sample Rate: {sr} Hz\")\n",
                "                elif sr != res_sr:\n",
                "                    print(f\"\u26a0\ufe0f Warning: Sample Rate Mismatch at segment {idx}. Expected {sr}, got {res_sr}. Timing will be off!\")\n",
                "                \n",
                "                # Audio Length\n",
                "                audio_len = len(audio_chunk)\n",
                "                \n",
                "                # VTT Times\n",
                "                start_time_str = format_vtt_time(total_samples / sr)\n",
                "                end_time_str = format_vtt_time((total_samples + audio_len) / sr)\n",
                "                \n",
                "                vtt_lines.append(f\"{start_time_str} --> {end_time_str}\")\n",
                "                vtt_lines.append(f\"{text}\\n\")\n",
                "                \n",
                "                # Append Audio + Silence\n",
                "                all_wavs.append(audio_chunk)\n",
                "                all_wavs.append(silence_chunk)\n",
                "                \n",
                "                total_samples += audio_len + len(silence_chunk)\n",
                "\n",
                "            # --- SAVING ---\n",
                "            if all_wavs and sr:\n",
                "                BASE_FILENAME = post['slug']\n",
                "                OUTPUT_FILENAME_MP3 = f\"{BASE_FILENAME}.mp3\"\n",
                "                OUTPUT_FILENAME_VTT = f\"{BASE_FILENAME}.vtt\"\n",
                "\n",
                "                # WAV\n",
                "                temp_wav = \"temp_output.wav\"\n",
                "                final_wav = np.concatenate(all_wavs)\n",
                "                sf.write(temp_wav, final_wav, sr)\n",
                "                final_duration_wav = len(final_wav) / sr\n",
                "                \n",
                "                print(f\"   WAV Duration: {final_duration_wav:.3f}s\")\n",
                "                \n",
                "                # MP3 (Use CBR 192k and explicit AR to avoid drift)\n",
                "                subprocess.run(\n",
                "                    [\n",
                "                        \"ffmpeg\", \"-y\", \"-i\", temp_wav, \n",
                "                        \"-codec:a\", \"libmp3lame\", \n",
                "                        \"-b:a\", \"192k\",       # Constant Bitrate for better timing consistency\n",
                "                        \"-ar\", str(sr),       # Enforce same sample rate\n",
                "                        \"-map_metadata\", \"-1\", \n",
                "                        OUTPUT_FILENAME_MP3\n",
                "                    ],\n",
                "                    check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
                "                )\n",
                "                \n",
                "                # Check Drift from MP3 encoding\n",
                "                final_duration_mp3 = get_audio_duration(OUTPUT_FILENAME_MP3)\n",
                "                drift = final_duration_mp3 - final_duration_wav\n",
                "                print(f\"   MP3 Duration: {final_duration_mp3:.3f}s (Drift: {drift:.4f}s)\")\n",
                "                \n",
                "                if abs(drift) > 0.5:\n",
                "                     print(\"\u26a0\ufe0f Significant MP3 drift detected! VTT might be desynchronized.\")\n",
                "\n",
                "                # VTT\n",
                "                with open(OUTPUT_FILENAME_VTT, \"w\", encoding=\"utf-8\") as f:\n",
                "                    f.write(\"\\n\".join(vtt_lines))\n",
                "                    \n",
                "                # Upload\n",
                "                print(f\"   Uploading...\")\n",
                "                with open(OUTPUT_FILENAME_MP3, \"rb\") as f:\n",
                "                    s3.upload_fileobj(f, S3_BUCKET_NAME, os.path.basename(OUTPUT_FILENAME_MP3))\n",
                "                with open(OUTPUT_FILENAME_VTT, \"rb\") as f:\n",
                "                    s3.upload_fileobj(f, S3_BUCKET_NAME, os.path.basename(OUTPUT_FILENAME_VTT))\n",
                "                    \n",
                "                # GitHub Dispatch\n",
                "                if GITHUB_TOKEN and \"YOUR_GITHUB\" not in GITHUB_TOKEN:\n",
                "                    print(f\"   Triggering Workflow...\")\n",
                "                    dispatch_url = f\"https://api.github.com/repos/{GITHUB_REPO}/dispatches\"\n",
                "                    headers = {\n",
                "                        \"Accept\": \"application/vnd.github.v3+json\",\n",
                "                        \"Authorization\": f\"token {GITHUB_TOKEN}\"\n",
                "                    }\n",
                "                    payload = {\n",
                "                        \"event_type\": \"audio-ready\",\n",
                "                        \"client_payload\": {\n",
                "                            \"slug\": BASE_FILENAME,\n",
                "                            \"mp3_url\": f\"{PUBLIC_URL_BASE}{os.path.basename(OUTPUT_FILENAME_MP3)}\",\n",
                "                            \"vtt_url\": f\"{PUBLIC_URL_BASE}{os.path.basename(OUTPUT_FILENAME_VTT)}\"\n",
                "                        }\n",
                "                    }\n",
                "                    r = requests.post(dispatch_url, headers=headers, json=payload)\n",
                "                    if r.status_code == 204:\n",
                "                         print(\"   \u2705 Dispatch Sent.\")\n",
                "                    else:\n",
                "                         print(f\"   \u274c Dispatch Failed: {r.status_code}\")\n",
                "                         \n",
                "                print(f\"\u2705 Finished: {post['slug']}\")\n",
                "\n",
                "    print(\"\\\\n\ud83c\udfc1 Dual-GPU Batch Processing Complete.\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}