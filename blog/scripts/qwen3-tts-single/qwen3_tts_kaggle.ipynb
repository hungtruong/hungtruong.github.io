{
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "kaggle": {
            "accelerator": "gpu",
            "dataSources": [
                {
                    "sourceId": 12136667,
                    "sourceType": "datasetVersion",
                    "datasetId": 7643116
                },
                {
                    "sourceId": 14631139,
                    "sourceType": "datasetVersion",
                    "datasetId": 9344612
                }
            ],
            "dockerImageVersionId": 31260,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": true
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "# Qwen3-TTS Voice Clone Demo\n\nThis notebook demonstrates how to run Qwen3-TTS Voice Cloning.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# 1. Install Dependencies\n# Install system dependencies first (fixes 'sox: not found' errors)\n!sudo apt-get update && sudo apt-get install -y sox libsox-dev ffmpeg\n\n!pip install -U qwen-tts\n# flash-attn is recommended for performance\n!pip install -U flash-attn --no-build-isolation\n!pip install pyngrok\n!pip install modelscope\n!pip install boto3 requests beautifulsoup4 pysbd\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# 2. Imports\nimport torch\nimport soundfile as sf\nfrom IPython.display import Audio\nfrom qwen_tts import Qwen3TTSModel\nimport os\nimport threading\nimport time",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# 3. Load Model (Voice Clone Base 1.7B)\nprint(\"Loading Model...\")\nmodel = Qwen3TTSModel.from_pretrained(\n    \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n    device_map=\"cuda:0\",\n    dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n)\nprint(\"Model loaded.\")",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# 5. Blog Scraper (Updated for Headers & Structure)\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import re\n",
                "import pysbd\n",
                "\n",
                "TARGET_URL = \"https://www.hung-truong.com/blog/\" \n",
                "# Or set a specific post URL: \n",
                "# TARGET_URL = \"https://www.hung-truong.com/blog/2026/01/22/how-low-can-wegovy/\"\n",
                "\n",
                "def get_latest_post_url(index_url):\n",
                "    try:\n",
                "        response = requests.get(index_url)\n",
                "        response.raise_for_status()\n",
                "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                "        # Find the first link that looks like a blog post\n",
                "        for a in soup.find_all('a', href=True):\n",
                "            href = a['href']\n",
                "            if re.match(r'.*/blog/\\d{4}/\\d{2}/\\d{2}/.+', href):\n",
                "                if href.startswith(\"/\"):\n",
                "                    return \"https://www.hung-truong.com\" + href\n",
                "                return href\n",
                "    except Exception as e:\n",
                "        print(f\"Error fetching index: {e}\")\n",
                "        return None\n",
                "    return None\n",
                "\n",
                "# Determine actual URL\n",
                "if \"index.html\" in TARGET_URL or TARGET_URL.endswith(\"/blog/\") or TARGET_URL == \"https://www.hung-truong.com/blog\":\n",
                "    print(\"Searching for latest post...\")\n",
                "    latest_url = get_latest_post_url(TARGET_URL)\n",
                "    if latest_url:\n",
                "        print(f\"Found latest post: {latest_url}\")\n",
                "        TARGET_URL = latest_url\n",
                "    else:\n",
                "        print(\"Could not find latest post link. Using original URL.\")\n",
                "\n",
                "print(f\"Scraping: {TARGET_URL}\")\n",
                "\n",
                "# Global variables to store scraped data\n",
                "scraped_data = []\n",
                "scraped_filename = None\n",
                "\n",
                "# Extract filename from URL (Always extracted first)\n",
                "clean_url = TARGET_URL.rstrip('/')\n",
                "scraped_filename = clean_url.split('/')[-1]\n",
                "print(f\"Target Base Filename: {scraped_filename}\")\n",
                "\n",
                "# Scrape Content\n",
                "try:\n",
                "    response = requests.get(TARGET_URL)\n",
                "    response.raise_for_status()\n",
                "    soup = BeautifulSoup(response.text, 'html.parser')\n",
                "    \n",
                "    content_div = soup.find('div', class_='content')\n",
                "    \n",
                "    if content_div:\n",
                "        # Initialize PySBD Segmenter\n",
                "        seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
                "        \n",
                "        # A. Remove Code Blocks (Keep figcaption if you want, but user said NO)\n",
                "        for unwanted in content_div.find_all(['figcaption', 'pre']):\n",
                "            unwanted.decompose()\n",
                "            \n",
                "        # B. Extract Title\n",
                "        title_tag = soup.find('h1', class_='postTitle')\n",
                "        title_text = None\n",
                "        if title_tag:\n",
                "            title_text = title_tag.get_text().strip()\n",
                "            print(f\"Title: {title_text}\")\n",
                "            scraped_data.append({'text': title_text, 'type': 'header'})\n",
                "        \n",
                "        # C. Extract Date\n",
                "        meta_tag = soup.find('p', class_='meta')\n",
                "        if meta_tag:\n",
                "            meta_text = meta_tag.get_text().strip()\n",
                "            if \"|\" in meta_text:\n",
                "                date_text = meta_text.split('|')[0].strip()\n",
                "                scraped_data.append({'text': f\"Published on {date_text}.\", 'type': 'paragraph_end'})\n",
                "            else:\n",
                "                scraped_data.append({'text': f\"Published on {meta_text}.\", 'type': 'paragraph_end'})\n",
                "\n",
                "        # D. Extract Text and Headers in Order\n",
                "        # We include li and blockquote to avoid missing content.\n",
                "        tags_to_find = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote']\n",
                "        all_elements = content_div.find_all(tags_to_find)\n",
                "        # Filter out elements that are nested within other tags_to_find to avoid duplicates\n",
                "        elements = [el for el in all_elements if not any(parent in all_elements for parent in el.parents)]\n",
                "        \n",
                "        for el in elements:\n",
                "            if 'meta' in el.get('class', []):\n",
                "                continue\n",
                "            \n",
                "            # Use separator=' ' and normalize whitespace to fix \"broken\" sentences caused by newlines or tags\n",
                "            text = el.get_text(separator=' ', strip=True)\n",
                "            text = re.sub(r'\\s+', ' ', text)\n",
                "            if not text: continue\n",
                "\n",
                "            if el.name.startswith('h'):\n",
                "                if \"Leave a Comment\" in text:\n",
                "                    print(\"Reached comment section. Stopping.\")\n",
                "                    break\n",
                "                if title_text and text == title_text:\n",
                "                    continue\n",
                "                scraped_data.append({'text': text, 'type': 'header'})\n",
                "            else:\n",
                "                sentences = seg.segment(text)\n",
                "                processed_sentences = []\n",
                "                current_sentence = \"\"\n",
                "                for sent in sentences:\n",
                "                    sent = sent.strip()\n",
                "                    if not sent: continue\n",
                "                    if current_sentence: current_sentence += \" \" + sent\n",
                "                    else: current_sentence = sent\n",
                "                    if len(current_sentence.split()) >= 3:\n",
                "                        processed_sentences.append(current_sentence)\n",
                "                        current_sentence = \"\"\n",
                "                if current_sentence:\n",
                "                    if processed_sentences: processed_sentences[-1] += \" \" + current_sentence\n",
                "                    else: processed_sentences.append(current_sentence)\n",
                "\n",
                "                for i, sent in enumerate(processed_sentences):\n",
                "                    if i == len(processed_sentences) - 1:\n",
                "                        scraped_data.append({'text': sent, 'type': 'paragraph_end'})\n",
                "                    else:\n",
                "                        scraped_data.append({'text': sent, 'type': 'sentence'})\n",
                "        \n",
                "        print(f\"Scraped {len(scraped_data)} segments.\")\n",
                "    else:\n",
                "        print(\"\u274c Could not find <div class='content'>\")\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"\u274c Scraping Error: {e}\")\n"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n# 6. Batch Generation & S3 Upload (MP3 & VTT)\nimport requests\nimport boto3\nimport os\nimport numpy as np\nimport soundfile as sf\nimport subprocess\nfrom IPython.display import Audio\nfrom datetime import datetime\n\n# --- CONFIGURATION ---\nS3_ACCESS_KEY = user_secrets.get_secret(\"S3_ACCESS_KEY\")\nS3_BUCKET_NAME = user_secrets.get_secret(\"S3_BUCKET_NAME\")\nS3_ENDPOINT_URL = user_secrets.get_secret(\"S3_ENDPOINT_URL\")\nS3_SECRET_KEY = user_secrets.get_secret(\"S3_SECRET_KEY\")\n\n# GitHub Dispatch Config\nGITHUB_REPO = \"hungtruong/jekyll-blog\"\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\") # Needs 'repo' scope\n# You might want to get this from a secret if it's a private storage\nPUBLIC_URL_BASE = \"https://pub-2289fc0aae4245debaa2fd741bdf5605.r2.dev/blogaudio/\"\n\n# Input Config\nlines_to_process = []\nif 'scraped_data' in locals() and scraped_data:\n    print(f\"Using {len(scraped_data)} items from scraper.\")\n    lines_to_process = scraped_data\nelse:\n    # Fallback for manual text URL\n    TEXT_FILE_URL = \"https://gist.githubusercontent.com/hungtruong/5a6f3a7d835784a2f8e6bf9120272f8e/raw/5f567eddd7b09089186aa07c393544b96d65031c/blog.txt\"\n    print(f\"Scraped data not found. Fetching from URL: {TEXT_FILE_URL}\")\n    try:\n        response = requests.get(TEXT_FILE_URL)\n        response.raise_for_status()\n        text_content = response.text\n        # Convert plain text to simple sentence structure\n        raw_lines = [line.strip() for line in text_content.split('\\n') if line.strip()]\n        lines_to_process = [{'text': l, 'type': 'sentence'} for l in raw_lines]\n    except Exception as e:\n        print(f\"Error fetching text file: {e}\")\n        lines_to_process = []\n\n# Output Filenames\nif 'scraped_filename' in locals() and scraped_filename:\n    BASE_FILENAME = scraped_filename\n    print(f\"Using base filename: {BASE_FILENAME}\")\nelse:\n    BASE_FILENAME = f\"generated_audio_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n    print(f\"Using default filename: {BASE_FILENAME}\")\n\nOUTPUT_FILENAME_MP3 = f\"{BASE_FILENAME}.mp3\"\nOUTPUT_FILENAME_VTT = f\"{BASE_FILENAME}.vtt\"\n\n# Reference Audio\nlocal_ref_audio = \"/kaggle/input/voice-cloning-dataset/longblog2.wav\"\nref_text_content = \"I was at Whole Foods today getting some groceries when I came across this mini food testing area at the end of an aisle. There were two nice sales people (one lady and one dude) who were hawking cereal. The type of cereal was super organic and it came in a pouch. The lady bragged that all of the ingredients were on the front of the bag in large type. The cereal was available for testing in cereal form, baked into a cookie, and blended into a smoothie (which was apparently made with apple cider and yogurt or something). Sidenote: While I was deciding what to taste test (I eventually went with the smoothie and it was not bad, and followed up with a chunk of cookie), an old Asian lady walked up to me and started talking in Chinese. I tried to tell her that I don\u2019t really speak Chinese, but I forgot how to say \u201cI don\u2019t know Chinese\u201d in Chinese. It\u2019s kind of absurd, anyway, to say you don\u2019t speak a language in that very language you\u2019re saying you don't speak. Anyway, she mumbled some more stuff and then said \u201cChinese.\u201d Like, yeah, lady, we're both Chinese. I guess she walked away after that. So anyway, here's the real part of the story. I'm tasting the cookie and am about to leave when another woman walks up to the food tasting area. The sales guy asks if she wants to buy some cereal and she's like \u201coh, I already have some at home! I love it! I'm just going to have some samples.\u201d\"\n\n# Helper for VTT Time\ndef format_vtt_time(seconds):\n    m, s = divmod(seconds, 60)\n    h, m = divmod(m, 60)\n    return f\"{int(h):02d}:{int(m):02d}:{s:06.3f}\"\n\ndef get_audio_duration(filename):\n    try:\n        result = subprocess.run(\n            [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\", \"-of\", \"default=noprint_wrappers=1:nokey=1\", filename],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True\n        )\n        return float(result.stdout.strip())\n    except Exception as e:\n        print(f\"Error checking duration: {e}\")\n        return 0.0\n\n# --- EXECUTION ---\nif not lines_to_process:\n    print(\"\u274c No lines to process. Aborting.\")\nelif S3_ACCESS_KEY == \"YOUR_ACCESS_KEY\":\n    print(\"\u26a0\ufe0f PLEASE SET YOUR S3 CONFIGURATIONS IN SECRETS \u26a0\ufe0f\")\nelse:\n    print(f\"Processing {len(lines_to_process)} items...\")\n    try:\n        all_wavs = []\n        vtt_lines = [\"WEBVTT\\n\"]\n        total_samples = 0 # Track samples for precision\n        \n        for i, item in enumerate(lines_to_process):\n            text = item.get('text', '').strip()\n            item_type = item.get('type', 'sentence')\n            \n            if not text or len(text) < 2: continue\n            \n            if i % 5 == 0:\n                print(f\"Progress: {i}/{len(lines_to_process)} - Current Logical Duration: {total_samples / 24000:.2f}s\") # 24k is placeholder if sr unknown\n            \n            # Generate Audio\n            wavs, sr = model.generate_voice_clone(\n                text=text,\n                language=\"English\",\n                ref_audio=local_ref_audio,\n                ref_text=ref_text_content,\n            )\n            audio_chunk = wavs[0]\n            \n            # VTT Timestamp\n            start_time_str = format_vtt_time(total_samples / sr)\n            total_samples += len(audio_chunk)\n            end_time_str = format_vtt_time(total_samples / sr)\n            \n            vtt_lines.append(f\"{start_time_str} --> {end_time_str}\")\n            vtt_lines.append(f\"{text}\\n\")\n            \n            all_wavs.append(audio_chunk)\n            \n            # Silence Logic\n            if item_type == 'header':\n                silence_dur = 1.5\n            elif item_type == 'paragraph_end':\n                silence_dur = 1.0\n            else: # sentence\n                silence_dur = 0.5\n                \n            silence_samples = int(silence_dur * sr)\n            all_wavs.append(np.zeros(silence_samples, dtype=np.float32))\n            total_samples += silence_samples\n            \n        if all_wavs:\n            # 1. Save Temp WAV\n            temp_wav = \"temp_output.wav\"\n            final_wav = np.concatenate(all_wavs)\n            \n            final_duration_wav = len(final_wav) / sr\n            sf.write(temp_wav, final_wav, sr)\n            print(f\"WAV saved. Samples: {len(final_wav)}, Duration: {final_duration_wav:.3f}s\")\n            \n            # 2. Convert to MP3\n            print(\"Converting to MP3...\")\n            # Use -map_metadata -1 to avoid tag-related shifts\n            !ffmpeg -y -i {temp_wav} -codec:a libmp3lame -qscale:a 2 -map_metadata -1 {OUTPUT_FILENAME_MP3}\n            \n            # 3. Validation\n            final_duration_mp3 = get_audio_duration(OUTPUT_FILENAME_MP3)\n            drift = final_duration_mp3 - final_duration_wav\n            print(f\"Encoded MP3 Duration: {final_duration_mp3:.3f}s\")\n            print(f\"Total Drift (MP3 vs WAV): {drift:.4f}s\")\n            \n            if abs(drift) > 0.1:\n                print(f\"\u26a0\ufe0f Warning: Significant drift detected ({drift:.4f}s). This may be due to MP3 encoder padding.\")\n\n            # 4. Save VTT\n            print(\"Saving VTT...\")\n            with open(OUTPUT_FILENAME_VTT, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\\n\".join(vtt_lines))\n                \n            # 5. Upload Both\n            print(f\"Uploading to S3 bucket: {S3_BUCKET_NAME}...\")\n            s3 = boto3.client(\n                's3',\n                endpoint_url=S3_ENDPOINT_URL,\n                aws_access_key_id=S3_ACCESS_KEY,\n                aws_secret_access_key=S3_SECRET_KEY\n            )\n            \n            s3_key_mp3 = os.path.basename(OUTPUT_FILENAME_MP3)\n            s3_key_vtt = os.path.basename(OUTPUT_FILENAME_VTT)\n            \n            with open(OUTPUT_FILENAME_MP3, \"rb\") as f:\n                s3.upload_fileobj(f, S3_BUCKET_NAME, s3_key_mp3)\n            print(f\"\u2705 Uploaded: {s3_key_mp3}\")\n            \n            with open(OUTPUT_FILENAME_VTT, \"rb\") as f:\n                s3.upload_fileobj(f, S3_BUCKET_NAME, s3_key_vtt)\n            print(f\"\u2705 Uploaded: {s3_key_vtt}\")\n\n            # 6. GitHub Dispatch\n            if GITHUB_TOKEN and GITHUB_TOKEN != \"YOUR_GITHUB_PERSONAL_ACCESS_TOKEN\":\n                print(f\"Triggering GitHub Workflow for {GITHUB_REPO}...\")\n                dispatch_url = f\"https://api.github.com/repos/{GITHUB_REPO}/dispatches\"\n                headers = {\n                    \"Accept\": \"application/vnd.github.v3+json\",\n                    \"Authorization\": f\"token {GITHUB_TOKEN}\"\n                }\n                payload = {\n                    \"event_type\": \"audio-ready\",\n                    \"client_payload\": {\n                        \"slug\": BASE_FILENAME,\n                        \"mp3_url\": f\"{PUBLIC_URL_BASE}{s3_key_mp3}\",\n                        \"vtt_url\": f\"{PUBLIC_URL_BASE}{s3_key_vtt}\"\n                    }\n                }\n                dispatch_response = requests.post(dispatch_url, headers=headers, json=payload)\n                print(f\"GitHub Dispatch Status Code: {dispatch_response.status_code}\")\n                if dispatch_response.status_code == 204:\n                    print(\"\u2705 GitHub Workflow triggered successfully!\")\n                else:\n                    print(f\"\u274c GitHub Dispatch Failed: {dispatch_response.text}\")\n            else:\n                print(\"\u23ed\ufe0f Skipping GitHub Dispatch: GITHUB_TOKEN not set or placeholder.\")\n            \n    except Exception as e:\n        print(f\"\u274c Error: {e}\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        }
    ]
}