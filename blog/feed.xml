<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hung Truong: The Blog!</title>
    <description>I say potato, you say potato...</description>
    <link>https://www.hung-truong.com/blog/</link>
    <atom:link href="https://www.hung-truong.com/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 19 Feb 2026 11:24:24 -0500</pubDate>
    <lastBuildDate>Thu, 19 Feb 2026 11:24:24 -0500</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      
      <item>
        <title>Adding Features to My Blog With Antigravity</title>
        <description>&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/antigravity-collaboration.webp&quot; alt=&quot;Google Antigravity IDE&quot; /&gt;
  &lt;figcaption&gt;My new IDE which I will probably drop soon!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I’ve been using Google Antigravity as my main AI-powered IDE for the past month or two. But something tells me I’ll probably switch soon, as Google keeps making it less and less useful. In the meantime, though, I’ve added a ton of new features to my blog, and I figured I’d write about some of them.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;hitting-antigravity-quota-jail&quot;&gt;Hitting Antigravity Quota Jail&lt;/h2&gt;

&lt;p&gt;I was using Claude Opus for a while with Antigravity, as the limits were pretty generous and renewed every 5 hours, similar to Claude Code and Codex. I actually think that Google gave more Opus usage than Claude Code itself did! At least at the time I was trying it out.&lt;/p&gt;

&lt;p&gt;I found that Gemini 3 Pro is good enough, but why not use Opus if I have access to it anyway? I was using an extension to track how much remaining quota I had. Unfortunately, it seems like I may have used too much, because Claude Opus and Sonnet began resetting on a weekly basis instead of every 5 hours.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/claude-quota.webp&quot; alt=&quot;Claude Quota Extension&quot; /&gt;
  &lt;figcaption&gt;My remaining Claude quota which updates every week, now.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I suppose that it’s probably not profitable for Google to provide this much Claude for a cheaper subscription (I’m on the $20 plan). But then they shouldn’t really imply that you’re going to get that much usage from the beginning. It feels a lot like a bait and switch. Others on the &lt;a href=&quot;https://www.reddit.com/r/google_antigravity/comments/1qs9cbw/i_miss_opus_45_so_bad/&quot;&gt;Antigravity subreddit&lt;/a&gt; also feel like the IDE was better before.&lt;/p&gt;

&lt;p&gt;It was about a week ago that I started getting the week-long refreshes on Opus. At that time I just switched over to Gemini 3 Pro and Flash to do my work instead. I favored Pro since, why not, I’m a pro! But after using up the quota a few times on that model, I got the banhammer for the pro models as well! So now I have a week-long wait to get my Anthropic and Gemini Pro models back to this stupid IDE.&lt;/p&gt;

&lt;p&gt;So anyway, now I’m just hanging on with Gemini 3 Flash resetting every 5 hours. Maybe I’ll get hit with a weekly refresh on that, too! At that point I’ll just switch to using Gemini CLI since it seems like it uses a separate method of calculating quotas.&lt;/p&gt;

&lt;p&gt;It also doesn’t help that sometimes, Gemini will just get into a really strange loop and start sounding like a &lt;a href=&quot;https://www.youtube.com/watch?v=XuYJbiHq66M&quot;&gt;kid in a Japanese horror anime&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/antigravity-hallucination.webp&quot; alt=&quot;Antigravity Hallucination&quot; /&gt;
  &lt;figcaption&gt;TFW you use up your whole context window on... this.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I already wrote a &lt;a href=&quot;/blog/2026/01/27/adding-audio-to-my-blog-with-qwen3-tts-voice-cloning/&quot;&gt;whole blog post&lt;/a&gt; about my process of adding audio articles (which sound like they’re read by me) to my blog with Qwen3-TTS. I mentioned that I used AI, and the models that I used were mainly Gemini 3 Pro and Flash due to Opus being drained.&lt;/p&gt;

&lt;h2 id=&quot;semantic-search-with-embeddings&quot;&gt;Semantic Search With Embeddings!&lt;/h2&gt;

&lt;p&gt;Since my blog is actually a static site hosted on GitHub Pages and generated by Jekyll, it takes a bit of work to make it seem dynamic. I had initially vibe-coded a simple search based on a JSON index that had the text and tags of each blog post, that basically did keyword matching in a text field. That worked pretty well, but I’ve never had a chance to use text embeddings, even though people say that they’re pretty useful.&lt;/p&gt;

&lt;p&gt;I figured that I could learn something about text embeddings and also add semantic search to my blog. Like killing two birds with one stone, if you’re into that sort of thing. I basically had AI talk me through how it would work, and then I had it implement it for me. But I do feel like I have a better grasp of how semantic vector search stuff works, even if I can’t really think of a good reason to use it besides over-engineering my blog search.&lt;/p&gt;

&lt;p&gt;My first naive attempt to implement this was using &lt;a href=&quot;https://huggingface.co/BAAI/bge-m3&quot;&gt;bge-m3&lt;/a&gt;, which AI told me was a really good embedding model. But when I tested out the search, I was getting some pretty bad matches. It seemed like nothing matched higher than 50%, and I got some irrelevant matches for queries that seemed pretty simple. Once I asked AI why this was happening, it mentioned that I wasn’t using all of bge-m3’s capabilities, since Cloudflare could only store a fraction of the data.&lt;/p&gt;

&lt;p&gt;That would’ve been nice to know before it told me to use bge-m3!&lt;/p&gt;

&lt;p&gt;Anyway, I switched over to using &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-Embedding-0.6B&quot;&gt;qwen3-embedding-0.6b&lt;/a&gt; which is also supported by Cloudflare’s Vectorize product. The semantic search is still more of a toy at this point, but it did return a post about Pokemans when I search for “pikachu” (the blog post doesn’t contain the word ‘pikachu’ in it).&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/pikachu-search.webp&quot; alt=&quot;Semantic Search for Pikachu&quot; /&gt;
  &lt;figcaption&gt;Testing out semantic search with a query for &quot;Pikachu&quot;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I also set up a GitHub workflow so that every time I update my blog, the index is updated in case I add a new post, or edit an existing one (or I guess if I delete one but why would I do that?). I’ve been trying to automate as much as possible with my blog so that I have less friction to just write in it.&lt;/p&gt;

&lt;h2 id=&quot;replaced-staticman-with-a-cloudflare-worker&quot;&gt;Replaced Staticman With a Cloudflare Worker&lt;/h2&gt;

&lt;p&gt;I recently added comments back to my blog using &lt;a href=&quot;https://staticman.net/&quot;&gt;Staticman&lt;/a&gt;, which is an interesting concept, but kind of overkill for me and my blog. I didn’t have anywhere to host it, so I was just hosting it on my home server and exposing it to the internet with Cloudflare tunnels. This seemed okay but if my home internet connection went down then no one would be able to comment on my blog! That would be like a Sev 1 in this household.&lt;/p&gt;

&lt;p&gt;Since Staticman is open source, I just had Antigravity look at the repo, and make an API-compatible endpoint with Cloudflare Workers that I could point my blog’s comment form to. It took surprisingly little time, and from there I was able to add all sorts of features like spam checking (with AI of course) and rate limiting, etc. I’m not too worried about Cloudflare going down but if it does then the internet probably has bigger issues than not being able to comment on my blog.&lt;/p&gt;

&lt;h2 id=&quot;jekyll-or-not&quot;&gt;Jekyll or Not?&lt;/h2&gt;

&lt;p&gt;I was actually considering moving my blog off of Jekyll since the technology is old and I’ve been wanting to spruce up my blog a bit. But I realized that Jekyll is pretty flexible in terms of adding features in a way that not every blogging platform is. I can just prompt an AI to add a feature, and Jekyll is extensible enough to just let me do whatever. Also, it’s kind of nice that the site is static and doesn’t use any computing resources aside from the search feature I added.&lt;/p&gt;

&lt;p&gt;For now I’ll just keep my blog the way it is. I feel like I stopped blogging as much after it became kind of a pain to edit markdown files, etc. But now that I can edit posts with AI, I actually find blogging with markdown to be a bonus. Adding a link is probably about the same amount of work with the prompt “Add this link &lt;strong&gt;__ to line __&lt;/strong&gt;.” But it’s also kinda cool to ask the AI to fact check my blog and spellcheck it so I don’t make as many stupid mistakes.&lt;/p&gt;

&lt;p&gt;Anyway, that’s basically what I’ve added to my blog so far. It’s pretty fun adding features to it, even though no one will read it. Or maybe if I add more features, people will finally start to notice it! Nah!&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/antigravity-collaboration.webp&quot; alt=&quot;Google Antigravity IDE&quot; /&gt;
  &lt;figcaption&gt;My new IDE which I will probably drop soon!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I’ve been using Google Antigravity as my main AI-powered IDE for the past month or two. But something tells me I’ll probably switch soon, as Google keeps making it less and less useful. In the meantime, though, I’ve added a ton of new features to my blog, and I figured I’d write about some of them.&lt;/p&gt;

</description>
        
        <pubDate>Sat, 31 Jan 2026 00:00:00 -0500</pubDate>
        <link>https://www.hung-truong.com/blog/2026/01/31/adding-features-to-my-blog-with-antigravity/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2026/01/31/adding-features-to-my-blog-with-antigravity/</guid>
        
        <category>Google Antigravity</category>
        
        <category>Semantic Search</category>
        
        <category>Cloudflare Workers</category>
        
        <category>Jekyll</category>
        
        <category>Text Embeddings</category>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Adding Audio to My Blog With Qwen3-TTS Voice Cloning</title>
        <description>&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/qwen3-tts-logo.webp&quot; alt=&quot;Qwen3-TTS Logo&quot; /&gt;
  &lt;figcaption&gt;Qwen3-TTS, the best TTS model I&apos;ve used so far!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I saw a note in my daily AI newsletter about a new TTS model that was out. I spent a better part of the day testing out its capabilities and attempting to do some fine tuning on it.&lt;/p&gt;

&lt;p&gt;I’ve had a goal for a pretty long time to add audio versions that people can use to listen to each of my blog posts. But rather than narrate them myself, I wanted the audio to be computer generated. Not only that, but I wanted it to sound like I was actually doing the narrating. So far, any of the models that I’ve tried to accomplish this haven’t lived up to the high standards of quality that you’re used to seeing on my blog. So did Qwen3-TTS finally make the grade?&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;When I looked at the &lt;a href=&quot;https://github.com/QwenLM/Qwen3-TTS&quot;&gt;Qwen3-TTS GitHub&lt;/a&gt;, I noticed that it had a few ways to generate audio from text. You could just use a pre-configured voice, and even give it instructions on how to speak. You could also design a voice, clone a voice from a reference audio, or even do fine tuning on the model and then generate from that.&lt;/p&gt;

&lt;p&gt;I initially set up a Kaggle notebook to test out the audio generation capabilities. I had an old dataset from a previous experiment of me reading the beginning of one of my blog posts. The audio quality was pretty bad, but I didn’t want to record any new stuff just yet. The voice cloning seemed to work pretty well! I decided to see if I could actually fine tune a model since that should get me even better results!&lt;/p&gt;

&lt;h2 id=&quot;when-fine-tuning-goes-wrong&quot;&gt;When Fine Tuning Goes Wrong&lt;/h2&gt;

&lt;p&gt;I ended up using a better mic that I bought for making YouTube videos (which I only used once so far) and recorded some more samples of myself reading from some of my blog posts. This time I used Logic Pro and messed with some plugins to make sure the audio sounded somewhat decent. I ended up recording about 30 files and decided that was probably enough for fine tuning.&lt;/p&gt;

&lt;p&gt;I used Google Antigravity to make another Kaggle notebook for me and iterated on the code until it actually worked. I ran into out of memory issues (the Kaggle GPUs max out at 16GB) and had to use the smaller model. I think somewhere in all of the hacks that Gemini used to get the training to complete, something got messed up because I could only manage to infer gibberish from the trained model.&lt;/p&gt;

&lt;figure style=&quot;width: 50%; margin: 0 auto;&quot;&gt;
  &lt;audio controls=&quot;&quot; style=&quot;width: 100%;&quot;&gt;
    &lt;source src=&quot;/blog/wp-content/uploads/2026/finetune_gibberish.mp3&quot; type=&quot;audio/mpeg&quot; /&gt;
  &lt;/audio&gt;
  &lt;figcaption&gt;This is My Voice on Fine-Tuning. Any Questions?&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I might need to actually train for longer, or use more voice samples, or maybe both. I just happen to be too lazy to do it right now since I already wasted like 4 hours on it. I also read some &lt;a href=&quot;https://github.com/QwenLM/Qwen3-TTS/issues/72&quot;&gt;comments on GitHub&lt;/a&gt; that the fine-tuned model doesn’t work that well so I decided to just give up for now.&lt;/p&gt;

&lt;h2 id=&quot;voice-cloning-instead&quot;&gt;Voice Cloning Instead&lt;/h2&gt;

&lt;p&gt;Since I had found some early success in voice cloning, I decided to see how far I could get with that. I used a longer section of recording for my “reference audio” which is about 30 seconds long. After that I tried a 1.5 minute clip, which had some more variety to it. Now I’m wondering if I could throw a 30 minute clip at it just to get all of my nuances in there but something tells me that at a certain point, you get diminishing returns (and blow up the GPU RAM).&lt;/p&gt;

&lt;figure style=&quot;width: 50%; margin: 0 auto;&quot;&gt;
  &lt;audio controls=&quot;&quot; style=&quot;width: 100%;&quot;&gt;
    &lt;source src=&quot;/blog/wp-content/uploads/2026/bookmarks_real.mp3&quot; type=&quot;audio/mpeg&quot; /&gt;
  &lt;/audio&gt;
  &lt;figcaption&gt;Ground Truth Audio&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure style=&quot;width: 50%; margin: 0 auto;&quot;&gt;
  &lt;audio controls=&quot;&quot; style=&quot;width: 100%;&quot;&gt;
    &lt;source src=&quot;/blog/wp-content/uploads/2026/bookmarks_generated.mp3&quot; type=&quot;audio/mpeg&quot; /&gt;
  &lt;/audio&gt;
  &lt;figcaption&gt;Generated Audio&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I set up a new Kaggle notebook that would scrape the text from my blog, then break it up into sentences and generate a TTS voice clone of myself speaking each sentence. Then it would stitch all of the audio files together into a giant spoken article podcast thing! I used a similar method when I wrote that &lt;a href=&quot;https://www.macrumors.com/how-to/get-siri-to-read-web-articles-to-you/&quot;&gt;Siri feature that reads articles to you&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The results ended up sounding really good! In the past, I’ve tried this same technique and got a voice that somewhat resembled mine, but it would talk in an almost southern drawl. It was super hilarious, but also not accurate. The Qwen3-TTS model gets the cadence and prosody of my speech down pretty well, all with a one-shot example! Sure, the TTS doesn’t get everything perfectly the way I would read things, but it’s good enough for me!&lt;/p&gt;

&lt;figure style=&quot;width: 50%; margin: 0 auto;&quot;&gt;
  &lt;audio controls=&quot;&quot; style=&quot;width: 100%;&quot;&gt;
    &lt;source src=&quot;/blog/wp-content/uploads/2026/whole_foods_southern.mp3&quot; type=&quot;audio/mpeg&quot; /&gt;
  &lt;/audio&gt;
  &lt;figcaption&gt;My Previous Attempt Where I Sounded Texan&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;adding-a-podcast-to-my-blog&quot;&gt;Adding a Podcast To My Blog&lt;/h2&gt;

&lt;p&gt;I started looking for projects that I could use to incorporate the audio article into my actual blog. Sort of like the transcript feature in YouTube where you can skip to sections by clicking on text. I found this project called &lt;a href=&quot;https://github.com/hyperaudio/hyperaudio-lite&quot;&gt;Hyperaudio&lt;/a&gt; that fit the bill. Since the audio is generated from text that was lifted directly from my blog anyway, I could just generate a subtitle file while generating the blog post audio. That way I wouldn’t have to mess with something like &lt;a href=&quot;https://openai.com/index/whisper/&quot;&gt;Whisper&lt;/a&gt; or another ASR system which would probably be really inaccurate.&lt;/p&gt;

&lt;p&gt;Since I generate my blog with Jekyll, it was pretty simple to add another value to my frontmatter, and then conditionally add a script that would add the tags for Hyperaudio to hook into. Plus it was super simple because I just had AI do it for me! I think it’s a fun feature that makes my blog more accessible, and I’d much rather have someone listen to my blog in my own voice, or at least a pretty good approximation to it.&lt;/p&gt;

&lt;p&gt;So now you can listen to me read my blog to you, skip to sections by clicking on the text in the blog, and see the current sentence being highlighted.&lt;/p&gt;

&lt;h2 id=&quot;future-improvements&quot;&gt;Future Improvements&lt;/h2&gt;

&lt;p&gt;I’m pretty happy so far with how this feature works. One thing that’s annoying, though, is that it requires the blog post to already exist before I can generate audio for it. That’s kind of a chicken and egg problem, and hard to solve. I don’t want to generate an audio file for a blog post and then immediately have to make a new one because I changed a word somewhere. But it would be nice to publish the audio and the blog post at the same time.&lt;/p&gt;

&lt;p&gt;I already set up a GitHub action to run from the Kaggle notebook after it uploads the files to Cloudflare storage. It picks up the new files, adds the frontmatter to the blog post, and then redeploys my blog. Sometimes I just love being a software nerd.&lt;/p&gt;

&lt;p&gt;I’m currently filling out the backlog of my posts with audio, so you may or may not see audio on them until I get to it.&lt;/p&gt;

&lt;p&gt;I guess at some point, if a better voice cloning TTS model comes out, I could update the files again, but for now they should do the job nicely!&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/qwen3-tts-logo.webp&quot; alt=&quot;Qwen3-TTS Logo&quot; /&gt;
  &lt;figcaption&gt;Qwen3-TTS, the best TTS model I&apos;ve used so far!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I saw a note in my daily AI newsletter about a new TTS model that was out. I spent a better part of the day testing out its capabilities and attempting to do some fine tuning on it.&lt;/p&gt;

&lt;p&gt;I’ve had a goal for a pretty long time to add audio versions that people can use to listen to each of my blog posts. But rather than narrate them myself, I wanted the audio to be computer generated. Not only that, but I wanted it to sound like I was actually doing the narrating. So far, any of the models that I’ve tried to accomplish this haven’t lived up to the high standards of quality that you’re used to seeing on my blog. So did Qwen3-TTS finally make the grade?&lt;/p&gt;

</description>
        
        <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
        <link>https://www.hung-truong.com/blog/2026/01/27/adding-audio-to-my-blog-with-qwen3-tts-voice-cloning/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2026/01/27/adding-audio-to-my-blog-with-qwen3-tts-voice-cloning/</guid>
        
        <category>Qwen3-TTS</category>
        
        <category>Kaggle</category>
        
        <category>GitHub</category>
        
        <category>Logic Pro</category>
        
        <category>Jekyll</category>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>How Low Can Wegovy?</title>
        <description>&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/ah-shit-here-wegovy-again.webp&quot; alt=&quot;Ah shit, here Wegovy again meme&quot; /&gt;
  &lt;figcaption&gt;Two puns in and we haven&apos;t even hit a paragraph yet.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I recently started taking one of these trendy weight loss pills, and since my blog always gets spam comments from people trying to sell the stuff, I figured I could just make a whole post about it. That way the rest of my blog posts will be safe from spam, right?&lt;/p&gt;

&lt;p&gt;It’s been about a week since I started taking the wegovy pill which just came out this year. I think I literally got it about a week after it was officially available. I had seen an article about how &lt;a href=&quot;https://www.prnewswire.com/news-releases/novo-nordisks-wegovy-pill-the-first-and-only-oral-glp-1-for-weight-loss-in-adults-now-broadly-available-across-america-302652205.html&quot;&gt;this new oral form was recently approved&lt;/a&gt;, and I did a bit of research on it. Before that, I had been pretty uninterested in taking it, but the more I read about how the drug actually worked, the more interested I was in it.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;myweightlossjourney&quot;&gt;#myweightlossjourney™&lt;/h2&gt;

&lt;p&gt;If you’re tired of reading about people’s struggles with weight loss, etc, you can probably skip this part, I’m just including it because maybe it’s relevant later?&lt;/p&gt;

&lt;p&gt;I’ve always felt overweight, ever since I was a kid. Probably because I was pretty fat as a kid. I attribute this to a few things, one being that I was taught to always finish all of my food &lt;em&gt;(my grandma would tell us that for every grain of rice we wasted, we would have to eat a maggot when we died and went to heaven (or hell?))&lt;/em&gt; and also that I really love eating food! Also my mom would buy me hella snacks from our membership at Costco. I had a pretty dope childhood tbh.&lt;/p&gt;

&lt;p&gt;There’s a lot of research about obesity and who knows what the actual causes are. It’s not like I blame my family for being fat or anything. I think our society loves to blame individuals for being overweight, but it’s probably some combination of environment plus personal choices plus OMG DORITOS ARE SO FUCKING DELICIOUS!&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/doritos-sketch-screenshot.webp&quot; alt=&quot;Doritos sketch screenshot&quot; /&gt;
  &lt;figcaption&gt;Marketing this good should be illegal.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Anyway, I’ve tried to lose weight with many different techniques in the past, with varying levels of success. I tried playing &lt;a href=&quot;https://en.wikipedia.org/wiki/Dance_Dance_Revolution&quot;&gt;DDR&lt;/a&gt; (yes, really), I did &lt;a href=&quot;https://en.wikipedia.org/wiki/Ketogenic_diet&quot;&gt;Keto&lt;/a&gt; for a few years but it really sucked, especially as a vegetarian. I wrote a &lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;freakin’ app to track my bike rides!&lt;/a&gt; I’ve also tried just eating less. I think the problem with these diets is that they’re so far from what my baseline of what I naturally want to do is, that I’ll just start gaining weight again once I stop doing the special thing.&lt;/p&gt;

&lt;p&gt;Recently I decided to care less about weight and more about fitness. About 1.5 years ago, I started strength training with weights and also exercising on a &lt;a href=&quot;https://www.concept2.com/ergs/rowerg&quot;&gt;Concept2 RowErg&lt;/a&gt; (just name dropping this brand, iykyk). I think it’s been good for my general health levels, but it’s probably not a great idea to go into bulk mode for a year and a half without cutting…&lt;/p&gt;

&lt;p&gt;Anyway, this gets us to the current state of things. My vitals look pretty good, e.g. my resting heart rate has been looking great, and I honestly feel healthier than I did when I was 10 years younger due to being more active. But I figured that there are also negative health outcomes that are more likely with a higher BMI. So let’s try something different!&lt;/p&gt;

&lt;h2 id=&quot;you-down-with-glp&quot;&gt;You Down With GLP?&lt;/h2&gt;

&lt;p&gt;I mentioned doing research on Semaglutide drugs, like Ozempic and Wegovy and Rybelsus. I had previously read something about how they just make you hate eating which sounds horrible, so I figured they weren’t for me. The truth is a bit more nuanced, though.&lt;/p&gt;

&lt;p&gt;From what I understand (from asking AI), the drugs work by making you digest stuff more slowly, along with controlling your insulin production. These two things can help you lose weight because they even out your hunger levels. I think I’m usually pretty good about not overeating my main meals, but I do have a serious snacking problem, so this helps a lot.&lt;/p&gt;

&lt;p&gt;There are some other health benefits, like reduced chance of heart issues and less inflammation, which seems like a bonus. But whenever I look at holistic medicine, technically everything seems to reduce inflammation, so ¯\&lt;em&gt;(ツ)&lt;/em&gt;/¯ ?&lt;/p&gt;

&lt;p&gt;There are some risks are kidney and gallbladder stuff, and some rare Thyroid cancer, I guess. I mean, if you’re looking for more info on this drug, you can do a better job than checking this blog!&lt;/p&gt;

&lt;p&gt;I wasn’t sure exactly what to expect when I started, but many forum posts mention nausea and other digestive issues as the main side effect. Luckily, I haven’t had too much of that. I think that the nausea is mainly due to people eating like normal when their body is digesting more slowly, so it causes a food backup. Even before starting on the drug (while I was waiting for my prescription to be filled), I started being more mindful about my portions, and basically got ready to eat less.&lt;/p&gt;

&lt;h2 id=&quot;sema-glutide-kind-of-life&quot;&gt;Sema-glutide Kind of Life&lt;/h2&gt;

&lt;p&gt;So after a week of being on this “miracle drug,” here’s my impressions (this is probably the part you wanted to read).&lt;/p&gt;

&lt;p&gt;I did notice that my head felt a bit fuzzy, maybe due to how the drug messes with your glucose levels, and at night I actually felt cold (I sleep super hot) which was very different for me. Those side effects have seemed to wane over the week, though.&lt;/p&gt;

&lt;p&gt;In terms of my fitness plan, I was concerned that my pumps wouldn’t be as good, but I actually did a bit better than the previous week when I finally decided to pump some iron. When I used the rowing machine, I noticed that I felt a bit less energy towards the end of my workout, but my muscles also didn’t feel as sore as they usually do at the end. So overall, I don’t think the drug had too much of an effect, which is great.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/ripped-hung.webp&quot; alt=&quot;Hung looking ripped&quot; /&gt;
  &lt;figcaption&gt;I asked AI what I would look like in a month and this is what it spit out.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So generally speaking, my first week on this drug has been pretty good! The main thing that I’ve noticed is that I don’t really have a desire to eat junk food, either sweet or salty, which I used to have a weakness for. I had predicted that this would make me sad, but surprisingly it doesn’t. My sentiment was even echoed in &lt;a href=&quot;https://www.reddit.com/r/Semaglutide/comments/1q042yp/not_trying_to_be_funny/&quot;&gt;some Reddit posts I found about people talking about their “last meal” before going on the drug, like they were on death row!&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I definitely felt some joy in smashing a whole bag of Doritos (I swear this blog post isn’t sponsored by them), but that’s also balanced out by the feeling of being a pig and also potential tummy aches later. In retrospect, it sounds kind of silly that I would miss having the urge to binge on snacks, because that’s like saying I would miss the addiction of smoking meth or shooting up heroin (I haven’t tried either yet so maybe I would?).&lt;/p&gt;

&lt;p&gt;My initial fear of not enjoying food also seems to have been misguided. Food still tastes good to me (at least on my current starter dose). I just eat less of it before I feel satisfied with it. And once I’m satisfied, why would I want more? Without the drug, that satisfaction is elusive, or it never comes, and then you realize you ate a whole frozen pizza.&lt;/p&gt;

&lt;p&gt;One weird thing that healthy people tell fat people is that they just need to start enjoying healthy food. But I don’t think that really comes naturally, otherwise they wouldn’t be fat. One thing I’ve noticed is that my perception of food has changed for whatever reason. I noticed while having a veggie burger and fries that the Heinz ketchup I used tasted super sweet. It was almost too sweet! Earlier that day I noticed that some honey flavored Greek Yogurt that I’ve had in the past tasted way sweeter than before. It definitely threw me off and made me wonder if I should even be eating it. Even an Apple tasted like candy. (Editor’s note, I’m so used to capitalizing Apple that I did it here even though I’m talking about the fruit. I’m leaving it in because it’s funny)&lt;/p&gt;

&lt;p&gt;I think this is what “healthy” people mean when they’re talking down to fat people about eating “healthy.” My suspicion is that there’s a biological/scientific component and it’s not just about fat people being dumb and lazy and eating like shit for no reason. I realize how obvious that sounds but that’s also the way fat people are treated in real life, so…&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/portland-ketchup.webp&quot; alt=&quot;Portland Ketchup&quot; /&gt;
  &lt;figcaption&gt;Previously the most disgusting thing I&apos;ve ever tasted. Now? Maybe ok!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I remember trying an organic ketchup brand once, it was probably the Portland one, and thinking it was disgusting. It was probably because it wasn’t sweet enough! I should try that brand again and see if it tastes better to me now. I looked it up and the Heinz ketchup has 5g of carb vs. 3g for the Portland one, with the same serving size of 17g. So ignoring rounding errors that’s like 66% more carbs in the Heinz version!&lt;/p&gt;

&lt;p&gt;Apart from hardly snacking, I’ve also been eating more thoughtfully. Keep in mind these are all things that a person can do without a drug, but that’s like saying anyone can climb Mt. Everest while also not being a psychopath. I’ve been eating more slowly and thinking about how full I am while eating meals. My portion sizes have never been a huge issue but I’ve been decreasing them a bit just to see if I feel full on less food. Sometimes I would eat an extra slice of pizza just because it tasted good, not because I was actually hungry. I don’t feel the need to do that anymore, and besides, it would probably make me feel sick with the slower digestion anyway.&lt;/p&gt;

&lt;p&gt;As I’ve been running on a caloric deficit, I have been feeling more fatigued. This felt stronger earlier in the week, and I don’t feel as tired today, so hopefully this is another side effect that goes away as my body gets used to it. I’ll probably keep feeling some amount of fatigue while I’m losing weight but that should probably come with the territory.&lt;/p&gt;

&lt;p&gt;Based on the vibes above, I think that the first week would be a success. But you know I’m a very numbers oriented kinda guy. This week I lost 2 pounds, which is probably mostly water weight. Since I’m on the lowest starter dose (you’re supposed to go up every month), I don’t expect to lose everything right away, but based on the changes to my habits, I could also see myself being pretty successful just staying on this same dose, too. I’ve heard of people not losing weight until going up in dose, but maybe they have more obstacles to overcome than I do.&lt;/p&gt;

&lt;h2 id=&quot;where-do-wegovy-now&quot;&gt;Where Do Wegovy Now?&lt;/h2&gt;

&lt;p&gt;So far, the results seem pretty good. I’m not surprised since these drugs have made a huge impact on society long before I started taking them. I’ll probably keep chugging along on these until I hit a goal that I feel comfortable with and see what I should do from there.&lt;/p&gt;

&lt;p&gt;There’s been a lot of debate about whether these drugs are the sort of thing you take permanently, or if you can stop taking them when you’re not fat anymore. My guess is that some people will probably need to keep taking them, otherwise they could just rebound back to their original weight. I also think that the drug could also help people start healthier habits that turn into long term success. So it probably just depends on the person.&lt;/p&gt;

&lt;p&gt;I’ve also seen a lot of distrust about the drug, which seems to stem from feeling like using it is “cheating” or “the easy way.” People want fat people to stay fat so they can feel superior to them. Gatekeeping better health seems like a weird thing to want, but okay!&lt;/p&gt;

&lt;p&gt;On top of losing weight, I do feel like there have been some interesting insights that I’ve gotten just from altering how hungry I feel. I do hope that more people get a chance to try this themselves, if they want to, and I’m not just saying that because this post is a advertisement paid for by “Big GLP” (that was a pun btw, please clap). It’s really interesting to experience how (in my imagination) a normal person interacts with food, versus someone who hasn’t always had a healthy relationship with it. It would also be kind of interesting if there was a reverse drug that made people hungry all the time, which non-fat people could take so they’d know how fat people feel. I’m sure Novo Nordisk is working on it as I’m writing this!&lt;/p&gt;

&lt;p&gt;I’ve read articles about how companies are changing their products for customers due to GLP-1 drugs (maybe that’s why everything advertises their protein content now?), so there does seem to be a shift happening in the economy. It might be a good time to short Frito Lay and maybe buy Dole stock, who knows?&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/frito-lay-profitable.webp&quot; alt=&quot;Frito Lay profitable GIF&quot; /&gt;
  &lt;figcaption&gt;How can GLP-1s make it profitable for Frito-Lay?&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Anyway, maybe I’ll give an update in a bit to see how #myweightlossjourney™ goes. I’m a bit worried that I’ve used up all the good puns but I have some time to come up with more.&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2026/ah-shit-here-wegovy-again.webp&quot; alt=&quot;Ah shit, here Wegovy again meme&quot; /&gt;
  &lt;figcaption&gt;Two puns in and we haven&apos;t even hit a paragraph yet.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I recently started taking one of these trendy weight loss pills, and since my blog always gets spam comments from people trying to sell the stuff, I figured I could just make a whole post about it. That way the rest of my blog posts will be safe from spam, right?&lt;/p&gt;

&lt;p&gt;It’s been about a week since I started taking the wegovy pill which just came out this year. I think I literally got it about a week after it was officially available. I had seen an article about how &lt;a href=&quot;https://www.prnewswire.com/news-releases/novo-nordisks-wegovy-pill-the-first-and-only-oral-glp-1-for-weight-loss-in-adults-now-broadly-available-across-america-302652205.html&quot;&gt;this new oral form was recently approved&lt;/a&gt;, and I did a bit of research on it. Before that, I had been pretty uninterested in taking it, but the more I read about how the drug actually worked, the more interested I was in it.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
        <link>https://www.hung-truong.com/blog/2026/01/22/how-low-can-wegovy/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2026/01/22/how-low-can-wegovy/</guid>
        
        <category>Wegovy</category>
        
        <category>Semaglutide</category>
        
        <category>GLP-1</category>
        
        <category>Weight Loss</category>
        
        <category>Concept2 RowErg</category>
        
        
        <category>Life</category>
        
      </item>
      
    
      
      <item>
        <title>First Impressions of OpenAI&apos;s Sora 2 + App</title>
        <description>&lt;figure&gt;
  &lt;video controls=&quot;&quot; playsinline=&quot;&quot; preload=&quot;metadata&quot; poster=&quot;/blog/wp-content/uploads/2025/sora-spaghetti-poster.webp&quot; style=&quot;max-width:100%;height:auto&quot;&gt;
    &lt;source src=&quot;/blog/wp-content/uploads/2025/sora-spaghetti.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Your browser does not support the video tag.
  &lt;/video&gt;
  &lt;figcaption&gt;New AI video model == New video of me eating spaghetti!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;OpenAI just released a new version of Sora, their video generation model. I spammed reload on a thread on Reddit to get an invite code last night, and quickly hit the rate limit (it was 71 videos)! Here’s my first impressions on the new app and model.&lt;/p&gt;

&lt;p&gt;The main features of Sora 2 (and &lt;a href=&quot;https://sora.chatgpt.com/&quot;&gt;the new app&lt;/a&gt;) are that you can make short form (like 10 second) videos just by prompting, and you can also insert yourself into videos. This is pretty interesting because previously you’d have to either train a LoRA (&lt;a href=&quot;/blog/2025/08/30/fun-with-wan-2-2-text-to-video-ai/&quot;&gt;like I did&lt;/a&gt;) or maybe use a static image of yourself as a starting frame which would inevitably drift away from looking like you.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;onboarding&quot;&gt;Onboarding&lt;/h3&gt;

&lt;p&gt;The onboarding of the app is pretty interesting. To make a “cameo,” which is the term for inserting your likeness into a video, you need to record a short video of yourself saying some numbers and looking around.&lt;/p&gt;

&lt;p&gt;At first I thought the numbers were just to verify that you were recording a live video, but I think it’s also to capture your voice for voice cloning. The voice that the AI uses for me sort of sounds like me but not quite. It’s probably because I was mumbling the numbers.&lt;/p&gt;

&lt;p&gt;Once you upload the video, you can just tag yourself in videos that you want to generate. You can also determine who can make videos with your likeness through some permissions. Like if you only want friends to be able to make cameos of you.&lt;/p&gt;

&lt;h3 id=&quot;generation&quot;&gt;Generation&lt;/h3&gt;

&lt;p&gt;I was interested in seeing how good the video generation was, and how creative you can be with the app. I think there must be some prompt rewriting magic being done after your initial prompt, because you can use a really vague prompt and still get an interesting video with an actual plot. I just prompted “username farting” and it came up with this gem:&lt;/p&gt;

&lt;figure&gt;
  &lt;video controls=&quot;&quot; playsinline=&quot;&quot; preload=&quot;metadata&quot; poster=&quot;/blog/wp-content/uploads/2025/sora-farting-poster.webp&quot; style=&quot;width:100%;max-width:360px;height:auto;display:block;margin:0 auto;&quot;&gt;
    &lt;source src=&quot;/blog/wp-content/uploads/2025/sora-farting.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Your browser does not support the video tag.
  &lt;/video&gt;
  &lt;figcaption&gt;Me farting, complete with awkward dialogue.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I also noticed that the notifications that the app sends out will describe the video in a way that might include more info than in your initial prompt.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2025/sora-notification.webp&quot; alt=&quot;Screenshot of the Sora 2 app interface&quot; /&gt;
  &lt;figcaption&gt;You&apos;ll see the content violation notification a lot.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;OpenAI is obviously going to be pretty risk-averse here when it comes to content moderation. When you enter a prompt that hits its guardrails, you’ll get a message that your generation failed. The annoying thing is that you can’t really tell if a prompt will run into a guardrail until you try generating the video. I’ve used identical prompts where one generation works fine and another hits a content block.&lt;/p&gt;

&lt;p&gt;For example, I tried recreating this &lt;a href=&quot;https://knowyourmeme.com/memes/ai-lady-with-rock-breaking-glass-bridge&quot;&gt;AI boulder video&lt;/a&gt; that people are talking about where some lady busts a Chinese glass bridge and people fall into the water. I was able to prompt it to get something similar. One time it failed and I just retried with the exact same prompt and it worked. There must be a few stages where the content is checked, at least one before generation and maybe one that watches the video once it’s finished. I tried remixing this video with Sam Altman instead of me and it kept failing, even though the only change was the person with the boulder.&lt;/p&gt;

&lt;figure&gt;
  &lt;video controls=&quot;&quot; playsinline=&quot;&quot; preload=&quot;metadata&quot; poster=&quot;/blog/wp-content/uploads/2025/sora-boulder-poster.webp&quot; style=&quot;width:100%;max-width:360px;height:auto;display:block;margin:0 auto;&quot;&gt;
    &lt;source src=&quot;/blog/wp-content/uploads/2025/sora-boulder.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Your browser does not support the video tag.
  &lt;/video&gt;
  &lt;figcaption&gt;In hindsight, bringing a boulder to a glass bridge was a terrible idea.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This brings up another point I want to make about Sora and OpenAI in general. I’ve been noticing this for a while, but it seems kinda funny and weird to me that Sam Altman is increasingly the face and brand of OpenAI. I remember seeing Sam in his multiple colorful polo shirts as a bro, showing off Loopt at the Apple Keynote in 2008. I also remember meeting him once when we were doing a company offsite when I worked at FarmLogs (he was an investor and I think in charge of YCombinator at the time, and FL was a YC company). In addition to being an investor/tech startup builder guy, I guess he also wants everyone to know who he is.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/blog/wp-content/uploads/2025/sora-sam-altman-cameo.webp&quot; alt=&quot;Screenshot of the Sora app featuring Sam Altman cameo options&quot; /&gt;
  &lt;figcaption&gt;SamA is ready to star in your video!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Sama (as he’s called) is available to cameo in pretty much any video you want to make. There’s some viral ones of him shoplifting GPUs from Target for Sora inference. And also a bunch of scammy ones where people make him beg for likes so he’ll give you an invite code. I can’t imagine being so full of myself that I’d want to sign up for this kind of publicity. But I guess Sama is just built different. He does seem like a bit of a psychopath but whatever, now he’s the de facto mascot of OpenAI!&lt;/p&gt;

&lt;h3 id=&quot;inconsistent-moderation&quot;&gt;Inconsistent Moderation?&lt;/h3&gt;

&lt;p&gt;So I keep running into issues where I can’t moonwalk or make myself a chef who’s being absolutely destroyed by Guy Fieri’s criticism on Diners Drive-ins and Dives, but for some reason there’s like a million videos of Pikachu on Sora. There’s also lots of videos of GTA V and other copyrighted stuff.&lt;/p&gt;

&lt;p&gt;It’s been interesting to see how people get around these guardrails, though. For example, I can’t say that I want a video of myself as Kramer in Seinfeld. But I can say that I’m busting through Jerry’s apartment door in a 90s sitcom and saying “Giddyup, Jerry!” which will work. I did notice though that the actors portraying Jerry only bear a passing resemblance to Jerry Seinfeld (kind of like the episode inside an episode of Seinfeld).&lt;/p&gt;

&lt;figure&gt;
  &lt;video controls=&quot;&quot; playsinline=&quot;&quot; preload=&quot;metadata&quot; poster=&quot;/blog/wp-content/uploads/2025/sora-sitcom-entrance-poster.webp&quot; style=&quot;width:100%;max-width:360px;height:auto;display:block;margin:0 auto;&quot;&gt;
    &lt;source src=&quot;/blog/wp-content/uploads/2025/sora-sitcom-entrance.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Your browser does not support the video tag.
  &lt;/video&gt;
  &lt;figcaption&gt;Sitcom-style apartment entrance cameo.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;other-interesting-stuff&quot;&gt;Other Interesting Stuff&lt;/h3&gt;

&lt;p&gt;I recorded my cameo proof video in front of a brick fireplace wearing a striped shirt. I noticed that in most of my videos I’m wearing that striped shirt, and if I don’t add a lot of detail to my prompt, I’ll end up getting a video in that exact same spot in my house. I guess that isn’t surprising that the system will associate me with the location of the recording but it’s also kind of weird to see my home show up in videos.&lt;/p&gt;

&lt;p&gt;If you watch enough videos on your feed, you’ll probably also notice that many of the Sam Altman videos are just him in front of a white background. I believe that this probably matches his own cameo setup video environment.&lt;/p&gt;

&lt;h3 id=&quot;the-feed&quot;&gt;The Feed&lt;/h3&gt;

&lt;p&gt;I guess I should also talk about the social aspect of Sora. One of the weirdest things about this product launch is that it’s a social app. You can add “friends” who can make videos of you, and you can even have multiple cameos in a single video. I haven’t added any “friends” yet but eventually I might get there.&lt;/p&gt;

&lt;p&gt;There’s also a “For You” feed which is currently inundated with socially engineered clickbait. Like videos claiming that if you double tap (which makes you like the post) you’ll see some special emoji. I’m honestly surprised that OpenAI didn’t consider that people will try to game the system this way. It’s a bad user experience because these videos are boring and often contain the same exact script, just with different people talking. True AI slop.&lt;/p&gt;

&lt;p&gt;I did manage to find some interesting videos, where people have the same instinct to make fun of Sam Altman and his double polo shirts. One of the first videos I made was of him announcing Loopt 2 wearing 5 polo shirts with giant collars! So there are some gems but they’re truly hidden behind the slop.&lt;/p&gt;

&lt;p&gt;I guess if you want you can follow my account, it’s &lt;a href=&quot;https://sora.chatgpt.com/profile/hungtruongy&quot;&gt;hungtruongy&lt;/a&gt; because somehow I didn’t get “hungtruong” in time.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;figure&gt;
  &lt;video controls=&quot;&quot; playsinline=&quot;&quot; preload=&quot;metadata&quot; poster=&quot;/blog/wp-content/uploads/2025/hung-cake-poster.webp&quot; style=&quot;width:100%;max-width:360px;height:auto;display:block;margin:0 auto;&quot;&gt;
    &lt;source src=&quot;/blog/wp-content/uploads/2025/hung-cake.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Your browser does not support the video tag.
  &lt;/video&gt;
  &lt;figcaption&gt;I was cake this whole time!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Sora 2 is really a huge step forward in AI video generation. It’s incredibly impressive. Which is why it’s kind of tragic that this technology is currently just being used to make shitty slop. I’m hopeful that through moderation and algorithming, it could actually end up being an interesting platform.&lt;/p&gt;

&lt;p&gt;I’ve also read that the true value of this technology is through ads. Like imagine how much brands would love to offer making a cameo of you wearing their clothing so you’ll buy it. Which makes the announcement a few days ago about &lt;a href=&quot;https://openai.com/index/buy-it-in-chatgpt/&quot;&gt;an AI agent purchase protocol&lt;/a&gt; a lot more obvious in hindsight.&lt;/p&gt;

&lt;p&gt;I had a lot of fun making videos of myself (and hitting the rate limits). I’m sure other people will too. I’m guessing some creative people will find a really cool use for this, or it’ll turn into an AI slop graveyard. Either way, it should be interesting!&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
  &lt;video controls=&quot;&quot; playsinline=&quot;&quot; preload=&quot;metadata&quot; poster=&quot;/blog/wp-content/uploads/2025/sora-spaghetti-poster.webp&quot; style=&quot;max-width:100%;height:auto&quot;&gt;
    &lt;source src=&quot;/blog/wp-content/uploads/2025/sora-spaghetti.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Your browser does not support the video tag.
  &lt;/video&gt;
  &lt;figcaption&gt;New AI video model == New video of me eating spaghetti!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;OpenAI just released a new version of Sora, their video generation model. I spammed reload on a thread on Reddit to get an invite code last night, and quickly hit the rate limit (it was 71 videos)! Here’s my first impressions on the new app and model.&lt;/p&gt;

&lt;p&gt;The main features of Sora 2 (and &lt;a href=&quot;https://sora.chatgpt.com/&quot;&gt;the new app&lt;/a&gt;) are that you can make short form (like 10 second) videos just by prompting, and you can also insert yourself into videos. This is pretty interesting because previously you’d have to either train a LoRA (&lt;a href=&quot;/blog/2025/08/30/fun-with-wan-2-2-text-to-video-ai/&quot;&gt;like I did&lt;/a&gt;) or maybe use a static image of yourself as a starting frame which would inevitably drift away from looking like you.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
        <link>https://www.hung-truong.com/blog/2025/10/01/openais-sora-2/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2025/10/01/openais-sora-2/</guid>
        
        <category>Sora 2</category>
        
        <category>OpenAI</category>
        
        <category>Generative AI</category>
        
        <category>LoRA</category>
        
        <category>Content Moderation</category>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Cleaning out my bookmarks</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/cleaning-out-my-bookmarks-tabs.webp&quot; alt=&quot;Screenshot of bookmarks folders queued for cleanup&quot; /&gt;
	&lt;figcaption&gt;A snapshot at my incredibly outdated bookmarks.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I was adding a bookmarklet to my web browser this morning (&lt;a href=&quot;https://gist.github.com/elado/69fa611d84305dea4b38801880743928&quot;&gt;to auto-click on American Express offers&lt;/a&gt;) and was trying to figure out how to organize it so it’s easy to click when I want to find it. I realized that I have like a billion bookmarks that I’ve added to my bookmarks bar, etc, but I almost never directly use any of my bookmarks.&lt;/p&gt;

&lt;p&gt;I figured I should clean out my browser bookmarks and organize them since it’s been a while since I’ve touched them. I realized that I haven’t really been adding bookmarks to anything, since I mostly just go to Reddit or a handful of other websites that just show me new stuff.&lt;/p&gt;

&lt;p&gt;In our current fast paced world, I can’t even imagine reading something and then wanting to go back and read it again…&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;A few years ago I learned the keyboard shortcut for going directly to the address bar (CMD + L if you’re curious) and that’s also pretty much eliminated the need for me to use bookmarks, since it’s faster for me to just hit that shortcut and start typing. I also aliased all of my locally run apps that run on web servers, like homeassistant.local, for example, so I don’t need to use port numbers in urls, e.g. 10.0.0.1:8123.&lt;/p&gt;

&lt;p&gt;So I started going through my bookmarks, and boy, are they old! A few are apartments in Seattle that I was thinking about moving into after I graduated from grad school (I moved into &lt;a href=&quot;https://www.epiapartments.com&quot;&gt;this one&lt;/a&gt;). There’s a syllabus to a class I took in grad school (&lt;a href=&quot;https://web.archive.org/web/20081210124525/http://si684w08.cms.si.umich.edu/&quot;&gt;the link broke&lt;/a&gt;). And hilariously there are also links to corporate intranet sites for multiple different companies that I worked for, including Microsoft, Amazon and Apple that I never bothered removing.&lt;/p&gt;

&lt;p&gt;I also bookmarked a ton of articles about development. Things like how to implement in-app purchases in iOS, or customizing the UITabBar on iOS 5. Or &lt;a href=&quot;https://gist.github.com/trey/2596149&quot;&gt;the best way to start a Django project in 2012&lt;/a&gt;!&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/how-to-start-a-django-project.webp&quot; alt=&quot;Screenshot of nested bookmark folders before cleanup&quot; /&gt;
	&lt;figcaption&gt;Even the gist recognizes this is a bit outdated...&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There are also a bunch of low carb keto recipes that I saved, a few of which I actually made and then highly regretted making.&lt;/p&gt;

&lt;p&gt;I also have a bookmark to my friend &lt;a href=&quot;http://bobthehermit.blogspot.com/&quot;&gt;Lorenzo’s blog &lt;/a&gt; which I’ll keep because it’s a good reminder to go check out his blog every now and then, and he’s another O.G. blogger like me!&lt;/p&gt;

&lt;p&gt;I think it’s really interesting that bookmarks are a feature that has rarely changed in browsers. From Mosaic to… whatever the latest AI browser is these days, they’ve all had bookmarks, and the feature is probably 99% the same as it was 30+ years ago (aside from maybe the bookmark bar and bookmarklets).&lt;/p&gt;

&lt;p&gt;It’s no wonder that I haven’t kept on top of pruning by bookmarks. Out of sight out of mind. I don’t really use them, so why would I need to clean them up?&lt;/p&gt;

&lt;p&gt;I can think of a handful of uses for bookmarks in my current day use.&lt;/p&gt;

&lt;h3 id=&quot;use-bookmarks-as-a-hint&quot;&gt;Use bookmarks as a hint&lt;/h3&gt;

&lt;p&gt;I can use bookmarks as a hint that I want the browser to autocomplete what I’m typing. I’m not sure how useful this is since I think browsers also just know that I go to a certain site every single day, but it’s probably somewhat useful.&lt;/p&gt;

&lt;h3 id=&quot;bookmarklets&quot;&gt;Bookmarklets&lt;/h3&gt;

&lt;p&gt;This would probably be my main usecase, to run a bookmarklet to bypass a paywall by pulling up an archived version, or clicking all the offers on the Amex site, or cancelling all of my Amazon subscribe and save orders since I only do that to save 5% or whatever the first time I buy something.&lt;/p&gt;

&lt;h3 id=&quot;recipes&quot;&gt;Recipes&lt;/h3&gt;

&lt;p&gt;Recipes are maybe the only other thing I’d use bookmarks for since I want to go back to them, but in a somewhat random fashion, and I don’t know if I trust Google to remember what “Zucchini Cheese Enciladas recipe” means every time I type it (though they are pretty good at that). I’d probably be better off just saving the recipe somewhere or &lt;strong&gt;**gasp&lt;/strong&gt;** printing it out, but whatever.&lt;/p&gt;

&lt;h2 id=&quot;bye-bye-bookmarks&quot;&gt;Bye Bye Bookmarks&lt;/h2&gt;

&lt;p&gt;I feel a bit bad removing a bunch of these bookmarks. But like Marie Kondo, I can acknowledge them, thank them for their service, and let them go. Plus I saved an archive somewhere of them anyway, in case I ever wanna look at them again!&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/cleaning-out-my-bookmarks-tabs.webp&quot; alt=&quot;Screenshot of bookmarks folders queued for cleanup&quot; /&gt;
	&lt;figcaption&gt;A snapshot at my incredibly outdated bookmarks.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I was adding a bookmarklet to my web browser this morning (&lt;a href=&quot;https://gist.github.com/elado/69fa611d84305dea4b38801880743928&quot;&gt;to auto-click on American Express offers&lt;/a&gt;) and was trying to figure out how to organize it so it’s easy to click when I want to find it. I realized that I have like a billion bookmarks that I’ve added to my bookmarks bar, etc, but I almost never directly use any of my bookmarks.&lt;/p&gt;

&lt;p&gt;I figured I should clean out my browser bookmarks and organize them since it’s been a while since I’ve touched them. I realized that I haven’t really been adding bookmarks to anything, since I mostly just go to Reddit or a handful of other websites that just show me new stuff.&lt;/p&gt;

&lt;p&gt;In our current fast paced world, I can’t even imagine reading something and then wanting to go back and read it again…&lt;/p&gt;

</description>
        
        <pubDate>Sat, 27 Sep 2025 00:00:00 -0400</pubDate>
        <link>https://www.hung-truong.com/blog/2025/09/27/cleaning-out-my-bookmarks/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2025/09/27/cleaning-out-my-bookmarks/</guid>
        
        <category>iOS</category>
        
        <category>Django</category>
        
        <category>Bookmarklets</category>
        
        <category>Browser Shortcuts</category>
        
        <category>Personal</category>
        
        
        <category>Life</category>
        
      </item>
      
    
      
      <item>
        <title>Goodbye Claude and Gemini, Hello Codex!</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/goodbye-claude-and-gemini-hello-codex-openaiplush.webp&quot; alt=&quot;OpenAI plush mascot representing the switch from Claude and Gemini to Codex&quot; /&gt;
	&lt;figcaption&gt;Thanks, OpenAI for the plushie and the office tour (not real btw)!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There’s been a few changes in the AI CLI landscape, so it’s time for a new blog post! I’ve once again switched my AI best friend. This time from Claude to &lt;a href=&quot;https://openai.com/codex/&quot;&gt;OpenAI’s Codex&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So since the last blog post I made about &lt;a href=&quot;/blog/2025/07/15/how-much-code-could-claude-code-code-if-claude-code-could-code-code-it-can/&quot;&gt;vibe coding with Claude Code&lt;/a&gt; and &lt;a href=&quot;/blog/2025/08/01/31-days-with-claude-code-what-i-learned/&quot;&gt;what I learned&lt;/a&gt;, it seems like Claude got really dumb. I actually let my subscription lapse so I could do other stuff (like play with &lt;a href=&quot;/blog/2025/08/30/fun-with-wan-2-2-text-to-video-ai/&quot;&gt;text to video AI&lt;/a&gt;) so I didn’t witness this firsthand. But judging from the Claude Code subreddit and even this &lt;a href=&quot;https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues&quot;&gt;official blog post&lt;/a&gt; from Anthropic, it sounds like there was quite the regression (which Anthropic didn’t even admit to for a while, and when they did, they didn’t compensate anyone for it).&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;During that time I was pretty much using Gemini CLI again (which still kinda sucks, to be honest) and Atlassian’s &lt;a href=&quot;https://www.atlassian.com/software/rovo&quot;&gt;Rovo CLI&lt;/a&gt; which can use Claude Sonnet and GPT5 with pretty nice free limits. I also tried Grok Fast or whatever it was called when it was free for a week but it didn’t seem to work that well for me.&lt;/p&gt;

&lt;p&gt;But after OpenAI announced GPT5 and later, GPT5 Codex, I was intrigued. I didn’t actually subscribe until I got an offer in my account for a free monthly trial of ChatGPT Business. So I have 5 seats of ChatGPT Business for a month right now (no one in my family accepted my offer to join my team, for some reason).&lt;/p&gt;

&lt;p&gt;I’ve been testing Codex out, and so far it’s been pretty much as good or better than Claude Code was when I was trying that. I did notice that ChatGPT Codex takes a bit longer to ramp up, but once it does, the output quality is great. I also had it on the “High” setting, which I’ve been keeping it on for the most part.&lt;/p&gt;

&lt;h3 id=&quot;the-anime-nano-test&quot;&gt;The Anime Nano Test&lt;/h3&gt;

&lt;p&gt;A couple of days ago I got an email from someone about &lt;a href=&quot;https://www.animenano.com&quot;&gt;Anime Nano&lt;/a&gt;, my anime blog post aggregator. They updated their blog and wanted to log in, but there wasn’t a password recovery feature for them to reset their password. When I saw the message I jumped out of my chair and started getting to work! Mostly because no one has really talked to me about Anime Nano in ages and it was validation that my website was still relevant! And it would be a great test to see how well Codex could work.&lt;/p&gt;

&lt;p&gt;I went to work, prompting Codex to implement an email based password reset feature. I had already implemented one at some point, probably when Anime Nano was built on Django, so I already had some database fields ready for it. I read a &lt;a href=&quot;https://blog.cloudflare.com/sending-email-from-workers-with-mailchannels/&quot;&gt;blog post&lt;/a&gt; about how Cloudflare Workers supported sending email without creating any accounts with MailChannels, but apparently that feature was deprecated. Codex wrote an implementation and when I tried it, it didn’t work! Once I realized that function had been deprecated, I had Codex rewrite it with &lt;a href=&quot;https://developers.cloudflare.com/workers/tutorials/send-emails-with-resend/&quot;&gt;Resend&lt;/a&gt;, after creating an account and getting all of the API keys and configuration set up. And it worked!&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/goodbye-claude-and-gemini-hello-codex-session.png.webp&quot; alt=&quot;Screenshot of Codex walking through the Anime Nano password reset update&quot; /&gt;
	&lt;figcaption&gt;Cloudflare Turnstile turning away robots and humans alike.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;When I emailed the user about the password reset, I sadly got a response that the Cloudflare Turnstile widget that I added to reduce the amount of spam and abuse was locking him out of the password reset feature. At that moment I realized that he was a robot and decided to not waste any more time on that clanker, until I was able to reproduce the issue myself. And I’m not a robot (at least I don’t think I am)! The issue boiled down to how the JavaScript was being loaded in an iframe. Or something, I dunno, AI fixed it.&lt;/p&gt;

&lt;h3 id=&quot;mcps-and-cli-and-code-review&quot;&gt;MCPs and CLI and Code Review!&lt;/h3&gt;

&lt;p&gt;Codex does just as well or better than Claude Code at using tools like the CLI, and it also works pretty well with this &lt;a href=&quot;https://developer.chrome.com/blog/chrome-devtools-mcp&quot;&gt;new MCP&lt;/a&gt; that I found for controlling Google Chrome. I haven’t played around with this too much yet, but it could be pretty powerful as part of my workflow for updating web apps.&lt;/p&gt;

&lt;p&gt;Given a good agents.md file, Codex is also super powerful with CLI tools. I’ve been using it with wrangler (since I’ve mostly been developing on Cloudflare’s platform), and it’s a real time saver. Since it has access to my database schema, I can just ask it things about the data on my local or remote database, and it’ll make queries for me. I absolutely hate writing SQL queries so this is a lifesaver. It’s helped me debug things a lot faster than I could myself, since I can essentially generate queries in plain language.&lt;/p&gt;

&lt;p&gt;I’ve also been having Codex push changes to GitHub and create PRs with the GitHub CLI tool, so that Google Gemini can review the PR and see if it has any issues. Gemini occasionally finds some good issues with the code, which makes me feel better about making PRs. But it’ll also bring up the nittiest of picks if it can’t find anything else to comment on. Just like a real developer!&lt;/p&gt;

&lt;p&gt;It sometimes feels like I’m roleplaying as a software engineer at a large company, since it’s probably way too much process to create a PR just for me to accept it and merge it to main, when I can just commit directly to main anyway, and it’s not like anyone else is touching my codebase.&lt;/p&gt;

&lt;p&gt;But it is pretty fun to see Gemini comment on Codex’s code. I’d have Claude review it too except I stopped paying Anthropic.&lt;/p&gt;

&lt;h3 id=&quot;other-stuff&quot;&gt;Other stuff&lt;/h3&gt;

&lt;p&gt;Codex has this online feature where you can have it whip up a virtual environment and then make changes to your code without using your own computer. In theory this is kind of cool, but in practice it seemed to be a lot slower and harder to observe. Each time I wanted to add a turn to the conversation, it would have to whip up the virtual machine again, which took forever.&lt;/p&gt;

&lt;p&gt;One interesting thing about this hosted feature is that you can have it run up to 4x concurrently and then pick the best result. I found that when I tried this, the implementations were all pretty similar, which I guess is a good thing. I’m not sure if it’s worth the effort to try something 4 times just to have to read 4x the code to figure out which one is the best. Maybe you could have AI do that part too, so I don’t have to!&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/goodbye-claude-and-gemini-hello-codex-sorry-earth.webp&quot; alt=&quot;Screenshot of Codex responding with Sorry, Earth!&quot; /&gt;
	&lt;figcaption&gt;Sorry, Earth!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I tried testing ChatGPT’s other premium features too, but they didn’t work as well. For some reason, the deep research features of AI always seem to disappoint me. Maybe I’m just really good at researching things, but the AI always seem to give me a suboptimal result. Like if I ask for it to go to a page, and analyze all of the things on that page, it’ll go through maybe 50% and get bored and then finish and tell me it’s done. So I sort of consider AI to be like a lazy intern since you need to keep poking at it until it gives you what you want. The problem is that it takes about as much effort to babysit AI as it is to just do the thing yourself for a lot of tasks. So I guess it’s more about finding the tasks where AI is good (and at this point, re-evaluating that every month or so as AI gets better at things).&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Overall, I like the way that Codex works outside of the box. Maybe it’s because I already had some context files set up (I just made some symlinks from Gemini.md and Claude.md to agents.md). But it feels like Codex is good at finding the right context to add in order to make reasonable changes. The changes I asked for weren’t groundbreaking or anything, but that’s probably like 99% of what needs to be done in code bases anyway.&lt;/p&gt;

&lt;p&gt;I’ll probably just let this free trial period time out (well, definitely since I added 5 business seats to my account). Then I’ll just have to see if Codex is still the top CLI AI agent at that point, or whether Anthropic recovers public trust, or maybe at that point Gemini 3 will be out and I’ll just play around with that. What a time to be alive! Meanwhile, Anime Nano is racking up features like there’s no tomorrow!&lt;/p&gt;

&lt;p&gt;P.S. I used Codex to help me add links and images to this blog post, which made it a lot easier and fun to write. I didn’t have it write any of the actual text though. All bad opinions are my own.&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/goodbye-claude-and-gemini-hello-codex-openaiplush.webp&quot; alt=&quot;OpenAI plush mascot representing the switch from Claude and Gemini to Codex&quot; /&gt;
	&lt;figcaption&gt;Thanks, OpenAI for the plushie and the office tour (not real btw)!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There’s been a few changes in the AI CLI landscape, so it’s time for a new blog post! I’ve once again switched my AI best friend. This time from Claude to &lt;a href=&quot;https://openai.com/codex/&quot;&gt;OpenAI’s Codex&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So since the last blog post I made about &lt;a href=&quot;/blog/2025/07/15/how-much-code-could-claude-code-code-if-claude-code-could-code-code-it-can/&quot;&gt;vibe coding with Claude Code&lt;/a&gt; and &lt;a href=&quot;/blog/2025/08/01/31-days-with-claude-code-what-i-learned/&quot;&gt;what I learned&lt;/a&gt;, it seems like Claude got really dumb. I actually let my subscription lapse so I could do other stuff (like play with &lt;a href=&quot;/blog/2025/08/30/fun-with-wan-2-2-text-to-video-ai/&quot;&gt;text to video AI&lt;/a&gt;) so I didn’t witness this firsthand. But judging from the Claude Code subreddit and even this &lt;a href=&quot;https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues&quot;&gt;official blog post&lt;/a&gt; from Anthropic, it sounds like there was quite the regression (which Anthropic didn’t even admit to for a while, and when they did, they didn’t compensate anyone for it).&lt;/p&gt;

</description>
        
        <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
        <link>https://www.hung-truong.com/blog/2025/09/24/goodbye-claude-and-gemini-hello-codex/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2025/09/24/goodbye-claude-and-gemini-hello-codex/</guid>
        
        <category>OpenAI Codex</category>
        
        <category>CLI</category>
        
        <category>GPT-5</category>
        
        <category>Cloudflare Workers</category>
        
        <category>Resend</category>
        
        <category>AI</category>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Fun With Wan 2.2 Text To Video AI</title>
        <description>&lt;figure&gt;
&lt;video width=&quot;600&quot; autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot;&gt;
  &lt;source src=&quot;/blog/wp-content/uploads/2025/hung_popcorn.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;
&lt;figcaption&gt;Get out the popcorn!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This blog post is pretty much also a Youtube video, so if you want, you can just watch that instead!&lt;/p&gt;
&lt;figure&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/LAWa63PVMnc?si=e9StEOvQOhoc5C6c&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;

&lt;p&gt;I recently got interested in text to video AI generators after reading a post about Wan 2.2. I hadn’t really been interested in them before because they all kind of sucked, and they either took too much VRAM or were proprietary ones that you couldn’t run locally like Veo or Sora, etc.&lt;/p&gt;

&lt;p&gt;But after looking at some of the videos that Wan 2.2 could generate, and seeing that it could run on some pretty standard hardware, I decided to look into it a bit more.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;making-a-lora&quot;&gt;Making a LoRA&lt;/h3&gt;

&lt;p&gt;Whenever I use these AI image generators, I always like to make images of myself. Call it narcissism or whatever, but I think it’s really fun to throw myself into impossible and weird situations, or make paintings of myself as a Roman emperor or whatever. But to make the AI generate images that look even the least bit like me, I need to train a LoRA. I’ve done this already before with image generators like &lt;a href=&quot;/blog/2022/08/28/stable-diffusion-generating-images-from-words/&quot;&gt;Stable Diffusion&lt;/a&gt; and, more recently, Flux.&lt;/p&gt;

&lt;p&gt;I used a tool called &lt;a href=&quot;https://github.com/ostris/ai-toolkit&quot;&gt;ai-toolkit&lt;/a&gt; to train the Flux LoRA on &lt;a href=&quot;https://runpod.io?ref=q4k3ugru&quot;&gt;Runpod&lt;/a&gt; around a year ago. The quality of that image generation model was quite good, but I quickly forgot about it after I tried making a bunch of art for my Samsung Frame TV of myself which didn’t turn out that great. When researching ways to train a LoRA for Wan 2.2, I found that ai-toolkit already had support for it. I still had the config files and image dataset that I used to train the Flux model, so I figured it would be pretty easy to do the Wan 2.2 one too.&lt;/p&gt;

&lt;p&gt;The training took about 2 hours running on a 24GB VRAM GPU. I think it was an A5000. The interesting thing about training is that I didn’t need to use videos to train the video model, just images. After I loaded the LoRA into the default ComfyUI workflow for Wan 2.2, I was able to put myself into a bunch of different situations!&lt;/p&gt;

&lt;h3 id=&quot;making-use-of-ai-videos&quot;&gt;Making Use of AI Videos&lt;/h3&gt;

&lt;p&gt;The main issue that pops up with these video generation AI models is that the length of the videos has to be pretty short. The other problem is that it’s hard to have consistency with characters and scenes between clips, especially if you’re only using text as a prompt to generate the video. I’ve seen some really shitty videos that people were really proud of where the characters all look slightly different between scenes, and the vehicle they’re riding in changes wildly. Yet somehow they think their AI video is the next summer blockbuster.&lt;/p&gt;

&lt;p&gt;When Google’s Veo came out (I think it was Veo 3 or 4?) I saw a video that someone made that I thought was really clever. It was a fake news story about a &lt;a href=&quot;https://www.reddit.com/r/Bard/comments/1kxpb1h/i_signed_up_for_gemini_ultraheres_what_i_made/&quot;&gt;synchronized swimming team of cats&lt;/a&gt;. I thought it was really smart because news stories consist of a bunch of short clips, and there wouldn’t really need to be a whole lot of consistency between the scenes.&lt;/p&gt;

&lt;p&gt;Any good artist knows to work within the constraints of whatever tools they’re using. I had a grand idea to create an 80s sitcom intro where every character was myself. You know how those videos always had someone looking at the camera and smiling? Or doing something wacky? Those scenes are always just a few seconds long, and they don’t really need to be consistent between each other since they usually super random anyway. So I started prompting. Here are a few examples that I really liked:&lt;/p&gt;

&lt;figure&gt;
&lt;video width=&quot;600&quot; autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot;&gt;
  &lt;source src=&quot;/blog/wp-content/uploads/2025/revolving_door_hung.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;
&lt;figcaption&gt;I love the look on AI Hung&apos;s face&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I made this revolving door shot of myself and every time I see this video I’m just amazed by the look on my (my?) face. I mean, it doesn’t even look exactly like me, but I just think it’s so funny.&lt;/p&gt;

&lt;figure&gt;
&lt;video width=&quot;600&quot; autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot;&gt;
  &lt;source src=&quot;/blog/wp-content/uploads/2025/kid_hung.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;
&lt;figcaption&gt;Kid me&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I like how the AI de-aged me and the setting really does look like an 80s television show.&lt;/p&gt;

&lt;figure&gt;
&lt;video width=&quot;600&quot; autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot;&gt;
  &lt;source src=&quot;/blog/wp-content/uploads/2025/telephone_hung.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;
&lt;figcaption&gt;Rolled up sleeves are cool!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I had AI make me talk on a corded phone wearing a blazer with rolled up sleeves. It doesn’t get more 80s than this! I really liked how the lighting worked in this one, where you can see the shadow that was cast by the person as well as the cord of the phone itself!&lt;/p&gt;

&lt;figure&gt;
&lt;video width=&quot;600&quot; autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot;&gt;
  &lt;source src=&quot;/blog/wp-content/uploads/2025/muscle_hung.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;
&lt;figcaption&gt;This is my real beach bod.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I also made AI make me totally ripped at the beach. This actually closely resembles the training images that I fed to AI, so it hardly had to do any work at all to make this video.&lt;/p&gt;

&lt;figure&gt;
&lt;video width=&quot;600&quot; autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot;&gt;
  &lt;source src=&quot;/blog/wp-content/uploads/2025/doctor_hung.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;
&lt;figcaption&gt;This is probably not sanitary.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;AI made me a doctor, which is something my parents can finally be proud of. Also I made myself eat a sandwich, which  I thought was an original thought but then I realized that I stole it from Weird Al’s Like a Surgeon video (or is it &lt;a href=&quot;https://x.com/alyankovic/status/1670113967497433090&quot;&gt;Weird AI&lt;/a&gt;?). I love this clip because the way he (I?) chews is so, so funny to me.&lt;/p&gt;

&lt;p&gt;Anyway, here’s the whole video if you want to watch it!&lt;/p&gt;

&lt;figure&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lkSdepqR4V4?si=C2_nYifkiykL2dAd&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;the-music&quot;&gt;The Music&lt;/h3&gt;

&lt;p&gt;Since I wanted to make an 80s sitcom intro, and all 80s sitcom intros have banger music in them, I was thinking I could write a banger 80s sitcom intro hit myself. But then I realized that I’m not really good at writing songs or singing, and I just know how to play the trumpet, so I figured I’d just use AI for this, too. I’m honestly not a huge fan of AI music, as a musician myself. But I figured it would be appropriate given the context of the video being completely AI too.&lt;/p&gt;

&lt;p&gt;I wrote the lyrics to my song myself, and then I used Suno to generate the music. It took quite a few generations and prompting with styles to get a halfway decent song. I think that Suno doesn’t really know how to write a unique melody.&lt;/p&gt;

&lt;p&gt;My theory for this is that there’s probably some code or something embedded in the model that prevents Suno from using a copyrighted melody from a real song. So while it can come up with a decent chord progression and the instruments and vocals all sound real, the thing is so scared of getting sued that it won’t come up with a melody. Like, can you imagine if you asked Suno for a song and it just gave you &lt;a href=&quot;https://youtu.be/izGwDsrQ1eQ?si=RoF-4cCdNnX9DYtD&quot;&gt;Careless Whisper&lt;/a&gt;?&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h3&gt;

&lt;p&gt;I make a lot of videos, whether that’s for &lt;a href=&quot;https://www.youtube.com/@HungryHungryHoagie&quot;&gt;Hoagie’s Youtube&lt;/a&gt;  , or &lt;a href=&quot;https://www.youtube.com/@techsistentialcrisis&quot;&gt;my Youtube&lt;/a&gt;, or &lt;a href=&quot;https://www.youtube.com/@HungTruongTheater&quot;&gt;my other Youtube for stupid stuff&lt;/a&gt;. I genuinely like making videos, so it’s sometimes weird to me that people who are making these AI videos think they will replace everyone who is currently involved in video production. Like I know that “AI replacing everyone” is a trope, but another popular trope is that people are complaining because AI is making art while they do repetitive boring tasks, where it should be the other way around.&lt;/p&gt;

&lt;p&gt;There have been a lot of complaints about AI slop too, whether that’s text slop or image slop or video slop. I mentioned before that I’ve seen so many cringey videos that people post thinking that they’ve made something incredible when it’s about the worst thing I’ve ever seen.&lt;/p&gt;

&lt;p&gt;I think the problem is that when you make art, it can suck or it can be good, and you eventually get a good understanding of which it is that you made while you are learning to make it. Usually people start off bad at it, and then they get better. Through this process, you develop taste and an understanding of what works and what doesn’t. The experience you get from putting time into it will shape your ability to filter and be your harshest critic, which helps you make better art that is worth sharing with people.&lt;/p&gt;

&lt;p&gt;When someone prompts an AI to make an image, and the image comes out sufficiently pleasing, they assume that there’s some sort of skill involved at the same level of making that image from scratch. I don’t think that’s the case, and while I’m not even anti-AI, I do think that there’s something to be said about learning a craft that you dont get by asking an AI to do it (obviously).&lt;/p&gt;

&lt;p&gt;I’m not saying that I’d never use AI as part of my usual workflow to make videos (in fact, I’ve used AI that cleans up audio for my poor microphone conditions before, and it turned out pretty good). But I think AI is still at the point that CGI in movies was some time in the late 90s. Like people will never stop talking about how bad the CGI was in &lt;a href=&quot;https://movieweb.com/the-mummy-returns-vfx-supervisor-reveals-why-the-scorpion-kings-cgi-was-so-bad/&quot;&gt;the Scorpion King&lt;/a&gt;! And I’m sure there will be uses of AI that people will talk about in a similar way that haven’t even been used yet! But at some point, maybe it’ll be good enough!&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
&lt;video width=&quot;600&quot; autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot;&gt;
  &lt;source src=&quot;/blog/wp-content/uploads/2025/hung_popcorn.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;
&lt;figcaption&gt;Get out the popcorn!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This blog post is pretty much also a Youtube video, so if you want, you can just watch that instead!&lt;/p&gt;
&lt;figure&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/LAWa63PVMnc?si=e9StEOvQOhoc5C6c&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/figure&gt;

&lt;p&gt;I recently got interested in text to video AI generators after reading a post about Wan 2.2. I hadn’t really been interested in them before because they all kind of sucked, and they either took too much VRAM or were proprietary ones that you couldn’t run locally like Veo or Sora, etc.&lt;/p&gt;

&lt;p&gt;But after looking at some of the videos that Wan 2.2 could generate, and seeing that it could run on some pretty standard hardware, I decided to look into it a bit more.&lt;/p&gt;

</description>
        
        <pubDate>Sat, 30 Aug 2025 00:00:00 -0400</pubDate>
        <link>https://www.hung-truong.com/blog/2025/08/30/fun-with-wan-2-2-text-to-video-ai/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2025/08/30/fun-with-wan-2-2-text-to-video-ai/</guid>
        
        <category>LoRA</category>
        
        <category>Wan 2.2</category>
        
        <category>ComfyUI</category>
        
        <category>A5000 GPU</category>
        
        <category>Suno</category>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>31 Days with Claude Code: What I Learned</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/claude-code-31-days.png&quot; /&gt;
	&lt;figcaption&gt;Claude Code - my coding companion for the past 31 days&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;It’s been… one month since I purchased a Claude Pro subscription so I could try Claude Code instead of freeloading off of Google Gemini’s CLI. I thought I would take a look at the things I learned while vibe coding a few projects, and other uses that I found for Claude Code besides coding. If you missed it, be sure to check out my initial post about &lt;a href=&quot;/blog/2025/06/29/my-obligatory-blog-post-about-vibe-coding-as-a-software-engineer/&quot;&gt;vibe coding&lt;/a&gt;, and the followup about &lt;a href=&quot;/blog/2025/07/15/how-much-code-could-claude-code-code-if-claude-code-could-code-code-it-can/&quot;&gt;how Claude Code was definitely better than Gemini&lt;/a&gt; (for now)!&lt;/p&gt;

&lt;h3 id=&quot;context-is-king&quot;&gt;Context is King!&lt;/h3&gt;

&lt;p&gt;So there’s a lot of talk about how prompt engineering is dead, and “context engineering” is the new hotness. That makes a lot of sense to me, as I ran into this issue constantly while using Claude Code, and to a lesser degree, with Gemini CLI. To understand context, let me first give you some… context.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The way that LLMs work is that there is a pre-trained model with weights (the ‘P’ in ‘GPT’ stands for pre-trained). Most of these applications are next token predictors. So if I input: “Welcome to McDonald’s, how may I help”, the next token would probably be “you”. Before there was ChatGPT, there was the text completion API (does anyone remember using &lt;a href=&quot;https://platform.openai.com/docs/models/davinci-002&quot;&gt;davinci-002&lt;/a&gt;?).&lt;/p&gt;

&lt;p&gt;But aside from the frozen model weights that the LLM is generating predictions from, any other context needs to come from you, the user. Most models have a specified knowledge cut-off date, which is basically the date of the most recent data used to train the model. So you can’t ask an LLM about anything newer unless it does a web search or something else to gain that context.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/context-window-example.webp&quot; /&gt;
	&lt;figcaption&gt;An example of models and their knowledge cutoff dates (among other info)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I like to think of Claude Code (or whatever coding agent I’m using) as a really smart software developer who also happens to have &lt;a href=&quot;https://en.wikipedia.org/wiki/Anterograde_amnesia&quot;&gt;Anterograde Amnesia&lt;/a&gt;, like Drew Barrymore’s character in &lt;a href=&quot;https://amzn.to/4l5bQW7&quot;&gt;50 First Dates&lt;/a&gt; (spoiler alert!). It can help you write an entire feature from scratch, and then completely forget who you are once its context has been wiped. From one session to the next, this tool will completely forget everything until you provide it with that context again somehow.&lt;/p&gt;

&lt;p&gt;For every session, Claude Code will literally need to look at whatever you’re asking it to work on and add it to the context before making more token predictions. Claude Code has a context window of like 200k tokens (see &lt;a href=&quot;https://docs.anthropic.com/en/docs/build-with-claude/context-windows&quot;&gt;this documentation from Anthropic&lt;/a&gt;), so whatever you’re working on needs to fit within that window, and every message you send to Claude Code will include all of the past context that you already sent. Claude Code might know how to reverse a list in Python, but it won’t know how to do that in your project until it loads the project into its context (why are you reversing lists in your project, anyway?).&lt;/p&gt;

&lt;p&gt;There are a few existing ways to help “prime” the context when you start working on things, so that CC doesn’t constantly have to search around to understand what you’re asking for. The most obvious one is the &lt;a href=&quot;https://www.anthropic.com/engineering/claude-code-best-practices&quot;&gt;CLAUDE.md file&lt;/a&gt; which is added to the context of each conversation you have with Claude Code. You can put things like instructions for doing common tasks, or explain the organization of which files go where. A common one would be to say where the log file is, so Claude Code can use the log files to read error messages. If you see Claude Code running a bunch of bash commands to grep files every time you ask it to do something, you might benefit from adding it to the CLAUDE.md file. Of course, it’s a balancing act as you don’t want to stick too much junk in there if it’s not relevant, either!&lt;/p&gt;

&lt;p&gt;When your context window gets too big, Claude Code will attempt to compress it, which I typically don’t like. I find that too many details are lost and it’s better to just start over from scratch by using the /clear command which essentially wipes all of the memory from the previous context. I think there’s been some studies done about how much worse these LLMs perform when the context window approaches its limit.&lt;/p&gt;

&lt;p&gt;Another thing that I’ve found is that context can get poisoned pretty easily. What I mean by that is you ask Claude Code to do something, and it does it the wrong way. Then you correct it and it undoes what it did, and does it the right way. Even though you corrected it, you still end up using that incorrect implementation on each and every turn of your conversation, because it’s in the history. To avoid this, you can hit esc twice, and just rephrase the thing you said that Claude Code misunderstood. This can save tokens in your context window and also keep the context from getting poisoned.&lt;/p&gt;

&lt;p&gt;One more thing I do to keep the context small (and therefore my token usage smaller) is to /clear the context whenever I’m done doing a task, especially if the next one is unrelated to the thing I just did. The way Claude Code works, all of your previous conversation history for your current session is used for your next message. So if I had Claude Code fix a bug around authentication, and then I wanted to have it work on image caching next, it doesn’t make any sense to keep sending the history about authentication in the next messages. There is some sort of token caching going on that I believe saves you usage, but I think that longer contexts are by definition less effective anyway.&lt;/p&gt;

&lt;h3 id=&quot;claude-code-codes&quot;&gt;Claude Code Codes&lt;/h3&gt;

&lt;p&gt;I don’t want to repeat myself too much with what I wrote in my previous blog post, but here’s a sample of the coding I did this month with Claude Code:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I added automated testing with playwright to &lt;a href=&quot;https://animenano.com&quot;&gt;Animenano&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Optimized Animenano RSS feed parsing by caching etag/last-modified headers&lt;/li&gt;
  &lt;li&gt;Wrote a script to sync the remote Animenano Cloudflare D1 database for testing locally&lt;/li&gt;
  &lt;li&gt;Added some tools to my &lt;a href=&quot;https://livekit.io/&quot;&gt;LiveKit&lt;/a&gt; AI agent that I was messing around with, like a Japanese phrase guessing game&lt;/li&gt;
  &lt;li&gt;Vibe coded search on my blog on a plane! (&lt;a href=&quot;https://youtu.be/S7aJlqSdi2k&quot;&gt;I actually made a video for this!&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Set up staticman for comments on my blog!&lt;/li&gt;
  &lt;li&gt;Changed the comment form on my blog from a hard reload to an AJAX update!&lt;/li&gt;
  &lt;li&gt;Wrote a script to scrape comments from my old blog posts on archive.org to replenish my missing comments (due to having a static blog)!&lt;/li&gt;
  &lt;li&gt;Started working on an AI powered DJ that picks from my local music collection and adds occasional personalized commentary with TTS&lt;/li&gt;
  &lt;li&gt;Added some features to a project that automatically creates localizations for Youtube video titles/descriptions and captions&lt;/li&gt;
  &lt;li&gt;Wrote a script to convert Final Cut Pro titles to SRT subtitles (for Youtube)&lt;/li&gt;
  &lt;li&gt;Wrote a Windmill script to evaluate coffee deals on Slickdeals rss and send a Slack message if they meet my parameters (light/medium roast, cost per oz, whole bean, etc)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Out of all of these things, I think Claude Code does the best when it’s working on a small, single file script. This makes sense because as we discussed earlier, context is very important. If Claude Code can store the entire script in memory, it won’t make false assumptions (hopefully!) about how things work. And if it doesn’t make those false assumptions, we can assume it will write correct code for any changes. I think it does do this most of the time.&lt;/p&gt;

&lt;p&gt;This is such a powerful use case that I really want to use more often. For example, I had an idea to add comments to my static blog, so I asked Claude Code how I should do it. Once we decided on staticman, Claude Code implemented it. Then I had an idea to get my missing comments on my blog back using the &lt;a href=&quot;https://archive.org/&quot;&gt;Internet Archive&lt;/a&gt; (I nuked them when I went from a Wordpress blog to a static site). So I had Claude Code write a script to scrape them from it. I’m sure I could’ve done this myself, but I’m also sure I’d be too lazy to sit down and figure out the endpoints and all of the glue code to make it work. With Claude Code, I just asked it to pound away at the problem until I had the solution, while I looked at Reddit. As an individual contributor for my entire software engineering career, I finally understood what it was like to be a manager!&lt;/p&gt;

&lt;p&gt;Another use case was that when I make &lt;a href=&quot;https://www.youtube.com/@HungryHungryHoagie&quot;&gt;Youtube videos for my dog&lt;/a&gt;, I hard code captions in English. I would turn these into SRT subtitles for other languages by literally copy/pasting each Final Cut Pro title into a caption in the project. I had an idea to automate this. So I exported the FCP project into xml and had Claude Code inspect it to see if it could convert the titles to SRT. After a few iterations, I have a script that I can just run, which saves me probably 3 or more minutes of manual work every time I make a video.&lt;/p&gt;

&lt;p&gt;Claude Code performed the worst when I had it make an AI powered DJ for me. That project has a lot of parts, and the architecture is honestly not super great (because vibe coding). It would code up one part, write some tests that passed, then move on to another part. The second part would also get tests, but Claude Code didn’t really integrate them at all. At the end I had a bunch of pieces that needed a lot more vibe coding to actually work together.&lt;/p&gt;

&lt;p&gt;It finally did end up working, but I think the project has a lot of tech debt which could probably be fixed (but I won’t because vibe coding). It’s still pretty cool because it does what I want it to do, but if I had to look at the code I might cry tears of bad engineering. Some of the blame probably goes to me, actually probably all of it. This project used a bunch of technologies that I don’t really understand, so I really did just let Claude Code go to town with it. If I had focused more on getting Claude Code to write a plan, maybe set up interfaces ahead of time, and set up integration tests, things may have gone more smoothly.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/ai-powered-dj.webp&quot; /&gt;
	&lt;figcaption&gt;You can tell this was made by AI by all the emojis 💪🎶🚀&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This brings up an important point. The people who learn how to use these tools (and use them efficiently) will still have an advantage over other people using the tools like noobs. At least for now, this is a useful and probably marketable skill. Anyone can just burn a billion tokens having Claude Code try and implement something (much like an &lt;a href=&quot;https://en.wikipedia.org/wiki/Infinite_monkey_theorem&quot;&gt;infinite number of monkeys writing Shakespeare on typewriters&lt;/a&gt;). But when tokens have a real cost, I think people who use them efficiently will have an edge. Who knows how long this edge will last, though, given the speed at which these things change?&lt;/p&gt;

&lt;h3 id=&quot;claude-code-doesnt-code&quot;&gt;Claude Code Doesn’t Code&lt;/h3&gt;

&lt;p&gt;I honestly think the non-code uses for Claude Code are more interesting than the code uses for it. The challenge is to think of domains where text is primarily used to do work, since Claude works mostly in that modality (though I guess images and video and audio work to a certain degree, too).&lt;/p&gt;

&lt;p&gt;Like I wrote in my post about Gemini CLI, I’m not exactly sure why these tools are CLIs and not GUIs where you can select a working directory and then run prompts on it. Maybe these companies are just throwing CLIs at engineers and then seeing what they end up doing with them, to later turn them into products?&lt;/p&gt;

&lt;p&gt;Here’s my list of non-code stuff I’ve had Claude Code work on for me this month:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Documentation for my various side projects, instead of leaving the readme blank&lt;/li&gt;
  &lt;li&gt;Set up wireguard vpn on my opnsense router, Claude Code helped me debug why I couldn’t access the network until it worked!&lt;/li&gt;
  &lt;li&gt;Added documentation on how I set up mdns and nginx on a home server to serve stuff on the network, and added some more hosts&lt;/li&gt;
  &lt;li&gt;Debugged an issue where an external SSD connected to a Linux home server went into read only mode&lt;/li&gt;
  &lt;li&gt;Made a PDF out of some images, then compressed the PDF file with command line tools&lt;/li&gt;
  &lt;li&gt;Markdown editor for this and my other previous blog posts!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the cooler things that I used Claude Code for was debugging an issue with an external drive that I had running on my home Linux server. I noticed that the drive had gone into read only mode. I tried restarting the server but that didn’t fix it. I asked Claude Code to take a look and it ran fsck on it, and found that there was some corruption. It gave me a list of commands to run (which I verified before running manually) that fixed the issue! I probably could’ve fixed this myself by doing some Google searches but I want to highlight this use case.&lt;/p&gt;

&lt;p&gt;Often when I search for fixes to things, I won’t get a relevant answer for whatever reason. Maybe it’s because my version of Linux is different or the hardware is different or the solution is to just turn it off and on again. With Claude Code, I get a quick feedback loop where it can actually run diagnostic bash commands that help it understand and diagnose the issue I’m seeing. Simple searching alone can’t really provide this tight of a feedback loop, and doing this kind of diagnosis is FAST! As long as it’s correct, of course.&lt;/p&gt;

&lt;p&gt;In the case of making a PDF out of some images, I was trying to use the macOS Preview app to do it, but I was getting a single page PDF that was huge. I asked Claude Code to make me a PDF and voila! Now, I realize that Claude simply used some knowledge of existing command line tools to accomplish this (in this case it used imagemagick). But as someone who doesn’t have encyclopedic knowledge of all command line tools that have ever been created, hey, it’s pretty useful to have an AI that does! I suppose that if anything, using Claude Code has made it more obvious to me how much power there is in the command line, just waiting to be used by anyone who is in the know about it.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/pdfmaking.webp&quot; /&gt;
	&lt;figcaption&gt;Claude Code helping me convert images to PDF and compress them using command line tools&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I mentioned the other use cases in my previous blog post about Claude Code. It happens to be quite good at reading and documenting things, especially if you give it direction. Who doesn’t have a pile of abandoned side projects with zero documentation, waiting to be resurrected? Claude Code can pretty much do the hard work of figuring out what the heck you were trying to do, and where you might’ve left off. I guess it’s kind of like reverse context engineering, where you get the AI to give you the context of the thing you completely forgot about.&lt;/p&gt;

&lt;p&gt;Finally, I’ve been using Claude Code a lot while writing these blog posts. I’m editing the markdown in VS Code, and I don’t really have a WYSIWYG editor, so I use CC for adding links, proofreading, and resizing and adding images. This has saved me a ton of time since using &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; is a lot more manual work than something like Wordpress. Claude Code even wrote 100% of this blog post you’re reading right now! (Just kidding)&lt;/p&gt;

&lt;h3 id=&quot;other-random-observations&quot;&gt;Other Random Observations&lt;/h3&gt;

&lt;p&gt;One thing I noticed while working on a few projects is that Claude Code and Gemini both really suck at CSS. I’m not entirely sure why this is, but I think it has to do with the fact that the final rendering of whatever you’re doing in CSS can come from many different sources. These coding agents seem much better when the context provided is right in front of them. With CSS, you might need to see what a certain class is inherting from the p tag, since it can affect what happens in a nested structure. These coding agents don’t actually compile the CSS (or whatever it’s called, I’m not a web dev) and actually get the final styles for the thing they’re working on. I doubt there’s much training data for CSS that involves reading from multiple CSS files and determining how to fix an issue due to a downstream style. So for now I’m somehow better at debugging CSS than AI, even as a native mobile developer.&lt;/p&gt;

&lt;p&gt;This might also explain why AI seems to love writing inline css rather than use a style sheet. Because AI doesn’t care about taste or tech debt, and I wonder if reinforcement learning favors quick wins over manageable code.&lt;/p&gt;

&lt;p&gt;For a few weeks in July, I also noticed that I kept hitting Claude Code rate limits even when I had just started a session. It was probably due to those guys on the &lt;a href=&quot;https://www.viberank.app&quot;&gt;CC leaderboard&lt;/a&gt; burning tokens for clout. I honestly can’t figure out why someone would try and get on that list. It’s basically admitting that you suck at using your tools efficiently. Kind of like people who brag about working 100 hours a week when most people can get by on 20!&lt;/p&gt;

&lt;p&gt;I did get an email from Anthropic about the issues and how they’d be resolved with more rate limits. I’m pretty sure Anthropic could have just solved this by implementing actual rate limiting… how exactly does someone abuse a system unless it’s open to be abused by the platform that provides it? Either way, I’m guessing that I won’t see any difference because I often go for entire sessions without ever seeing the warning that I’m approaching my limits. Probably because I take the precautions I described above to limit my context size, etc.&lt;/p&gt;

&lt;h3 id=&quot;was-claude-pro-worth-it&quot;&gt;Was Claude Pro Worth It?&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/ccusage-report.webp&quot; /&gt;
	&lt;figcaption&gt;Daily Claude Code token usage report showing $132.79 on my main desktop&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I’ve been using this &lt;a href=&quot;https://github.com/ryoppippi/ccusage&quot;&gt;ccusage script&lt;/a&gt; to keep track of how much I’ve used Claude Code for a given time block as well as just seeing how much I’ve used in tokens if I had opted to pay per token. It’s pretty interesting to see the numbers go up, and somewhat horrifying to realize how much it would actually cost if I didn’t pay for a monthly subscription. Since I’ve been using Claude Code on multiple computers, I have to add up the usage across four machines. So far I’ve used $132.79 worth on my desktop computer, $2.05 on my linux server (mostly for debugging and documenting), $30.58 on a Macbook Air I have, and $13.38 on another Macbook Air (I have a lot of computers, so what?). That comes to a whopping $181.37 worth of usage from a $20 Claude Pro subscription. I didn’t even max out my usage every day, and there were days that I didn’t even use it because I was on vacation.&lt;/p&gt;

&lt;p&gt;I would say that I definitely got my money’s worth out of Claude Code this month. I’ve been more productive in this month than I have in a really long time. And I don’t think it’s just because I had AI do everything for me. I’ve had more mental energy to think about things I want to accomplish, and the quick feedback loops that come from AI pounding out code has been contributing to that. This could just be a temporary boost until things regress to the mean, but for now I’m really loving the productivity gains I’m getting.&lt;/p&gt;

&lt;p&gt;Overall I’m pretty happy with this product. I’m hoping that this isn’t just the cheap stage of coding assistants (see &lt;a href=&quot;https://techcrunch.com/2014/04/07/lyft-spring-pricing/&quot;&gt;Lyft, Uber in 2014&lt;/a&gt;) and that these tools will get more useful, powerful and faster over time!&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2025/claude-code-31-days.png&quot; /&gt;
	&lt;figcaption&gt;Claude Code - my coding companion for the past 31 days&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;It’s been… one month since I purchased a Claude Pro subscription so I could try Claude Code instead of freeloading off of Google Gemini’s CLI. I thought I would take a look at the things I learned while vibe coding a few projects, and other uses that I found for Claude Code besides coding. If you missed it, be sure to check out my initial post about &lt;a href=&quot;/blog/2025/06/29/my-obligatory-blog-post-about-vibe-coding-as-a-software-engineer/&quot;&gt;vibe coding&lt;/a&gt;, and the followup about &lt;a href=&quot;/blog/2025/07/15/how-much-code-could-claude-code-code-if-claude-code-could-code-code-it-can/&quot;&gt;how Claude Code was definitely better than Gemini&lt;/a&gt; (for now)!&lt;/p&gt;

&lt;h3 id=&quot;context-is-king&quot;&gt;Context is King!&lt;/h3&gt;

&lt;p&gt;So there’s a lot of talk about how prompt engineering is dead, and “context engineering” is the new hotness. That makes a lot of sense to me, as I ran into this issue constantly while using Claude Code, and to a lesser degree, with Gemini CLI. To understand context, let me first give you some… context.&lt;/p&gt;
</description>
        
        <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
        <link>https://www.hung-truong.com/blog/2025/08/01/31-days-with-claude-code-what-i-learned/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2025/08/01/31-days-with-claude-code-what-i-learned/</guid>
        
        <category>Claude Code</category>
        
        <category>Context Windows</category>
        
        <category>LLMs</category>
        
        <category>AI Coding</category>
        
        <category>Tokens</category>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>How Much Code Could Claude Code Code if Claude Code Could Code Code? (It Can!)</title>
        <description>&lt;figure&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2025/claude-code-best-friend.jpg&quot; width=&quot;700&quot; alt=&quot;A &apos;Friendship ended with&apos; meme. The top text says &apos;Friendship ended with Gemini&apos;. The image shows a picture of the author shaking hands with the Claude logo. The text in the middle says &apos;Now CLAUDE is my best friend&apos;.&quot; /&gt;
	&lt;figcaption&gt;Claude Code - my new best friend! (I spent a lot of time on this image, btw)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Previously on my blog, I wrote about &lt;a href=&quot;/blog/2025/06/29/my-obligatory-blog-post-about-vibe-coding-as-a-software-engineer/&quot;&gt;vibe coding&lt;/a&gt; and how I was experimenting with Google CLI, the free agentic AI thing that runs in your command line. I talked about how cool it was, but also how I was too cheap to try anything that cost money. After repeatedly hearing about how good Claude Code was, I decided to scrounge up the last few dollars I had in my vast money bin and spent $20 on Claude Pro. So was it worth it? Heck yeah it was! Claude Code is my new best friend!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;what-is-claude-code&quot;&gt;What is Claude Code?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://claude.ai/code&quot;&gt;Claude Code&lt;/a&gt; is Anthropic’s official CLI tool that brings Claude’s coding capabilities directly to your terminal. It’s a lot like the Google Gemini CLI that I talked about before, but it actually has brains and doesn’t get stuck in an infinite loop constantly for no reason. Previously, when I tried using the Gemini CLI, it would mostly hit rate limits, switch to the flash version of the model, and generally be bad at doing things. Even with the degraded quality, I still found some use for it, though. But I knew that if I wanted to be a true Vibe Coder, I’d have to upgrade.&lt;/p&gt;

&lt;p&gt;Claude Code uses Claude Sonnet 4 which is apparently really good at code. So far I’ve used it to:&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;I added automated testing with playwright to Animenano&lt;/li&gt;
  &lt;li&gt;Optimized Animenano RSS feed parsing by caching etag/last-modified headers&lt;/li&gt;
  &lt;li&gt;Wrote a script to sync the remote Animenano Cloudflare D1 database for testing locally&lt;/li&gt;
  &lt;li&gt;Added some tools to my &lt;a href=&quot;https://livekit.io/&quot;&gt;LiveKit&lt;/a&gt; AI agent that I was messing around with, like a Japanese phrase guessing game&lt;/li&gt;
  &lt;li&gt;Vibe coded search on my blog on a plane! (&lt;a href=&quot;https://youtu.be/S7aJlqSdi2k&quot;&gt;I actually made a video for this!&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Set up staticman for comments on my blog!&lt;/li&gt;
  &lt;li&gt;Changed the comment form on my blog from a hard reload to an AJAX update!&lt;/li&gt;
  &lt;li&gt;Wrote a script to scrape comments from my blog posts on archive.org to replenish my missing comments!&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other&quot;&gt;Other:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Documentation for my various side projects, instead of leaving the readme blank&lt;/li&gt;
  &lt;li&gt;Set up wireguard on my opnsense, Claude Code helped me debug why I couldn’t access the network until it worked!&lt;/li&gt;
  &lt;li&gt;Added documentation on how I set up mdns and nginx on a home server to serve stuff on the network, and added some more hosts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The “Other” category is honestly a really cool unlock for me personally. I have a bunch of home servers that do random stuff like run cron jobs of scripts for automation and other really random things. I usually just write one-off implementations based on quick searches of how to do things (like setting up mdns and nginx to redirect example.local domains to 10.0.0.111:8000 for example). Inevitably, I’ll forget how I set something up (like when I set up a launchd daemon thing) and it becomes this tech debt where it becomes really difficult to fix or change anything if something breaks or I need to set it up again a different way.&lt;/p&gt;

&lt;p&gt;With Claude Code, I can just have it read the files and figure out how things work, then write a README.md that describes in plain language how it’s set up, how to use it, and how to modify it. Claude Code is incredible for side projects and these sorts of things where you naturally don’t put the same amount of rigor into documentation as you would for professional work. It takes the boring parts of fun projects and automates them, so you have more time to start even more side projects that may or may not ever be finished!&lt;/p&gt;

&lt;h2 id=&quot;theres-a-plan-for-how-much&quot;&gt;There’s a Plan For HOW MUCH?&lt;/h2&gt;

&lt;p&gt;I’ve been seeing a lot of posts online about how people are YOLOing their wallets and buying the $200 a month &lt;a href=&quot;https://www.anthropic.com/pricing&quot;&gt;Claude Max plan&lt;/a&gt;, which includes 20x the amount of requests that you can make with the Pro plan (which I’m currently on). I suppose that if I was using Claude Code for my job, and it was on a super large codebase, that might make some sense. But honestly, it seems a bit excessive. For an experienced software engineer, coding is probably like 10% of what you actually do, and the rest is reading code and planning what to do. I guess Claude can help with that too, but it just seems like the guys who are running 8 subagents simultaneously and racking up the Claude Code tokens could really just learn to git gud at software engineering. Or as the kids would say, Skill Issue! They’re obviously just karma farming because who actually does that!?&lt;/p&gt;

&lt;p&gt;I guess one additional bonus of that ridiculous MAX plan is that you have access to the Opus 4 model, which may or may not be named after that penguin from the 80s. I already feel like I can accomplish pretty much whatever using Sonnet, but I guess that if I hit too many roadblocks, it might be interesting to at least try Opus.&lt;/p&gt;

&lt;p&gt;I do think I made a pretty good decision paying for the $20 plan as I rarely hit the rate limit anyway, and I’ve been using this tool called ccusage to see how much I’d be paying if I was using the pay-as-you-go style of token based API usage.&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2025/claude-code-usage-stats.jpg&quot; width=&quot;700&quot; alt=&quot;A screenshot of a terminal window displaying Claude Code usage statistics from the &apos;ccusage&apos; tool for July 1-15, 2025. The table shows columns for session start time, duration, tokens, and cost. The sum of the completed sessions is $33.66, with a final projected cost of $51.28 shown at the bottom.&quot; /&gt;
	&lt;figcaption&gt;My Claude Code usage statistics from ccusage showing token usage and costs. Not bad for $20 a month!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This image doesn’t completely reflect my usage because it’s divided between like 3 different computers, and the tool only shows usage for the computer it’s running on. And I’m too lazy to go run it on all of them right now to see how much I actually used. On top of that, I think the Claude Code Github Actions stuff uses your Pro Plan quota, so that’s not showing up in the numbers either.&lt;/p&gt;

&lt;p&gt;I plan on continuing to use Claude Code and paying for the subscription for now. I started it on July 1st and I’ve already gotten my money’s worth for the month. To be honest, I have a suspicion based on the token usage image above that this price is heavily subsidized, kinda like how Uber and Lyft rides used to be super cheap. Though with those things, you’d have to pay a person, and with Claude, you just need to pay to run a server. Hopefully servers get cheaper or more efficient, but even so, it’s hard to believe that this much utility can come at such a low price. Maybe I should be more bullish about AI.&lt;/p&gt;

&lt;p&gt;I should probably report back in a month or so to see if I run out of things to use CC on, or if I just keep using it as frequently as I am now. For now though, I’m totally bought into the hype. Claude Code forever!&lt;/p&gt;

&lt;h2 id=&quot;btw&quot;&gt;BTW&lt;/h2&gt;

&lt;p&gt;Also, I’m still using this AI stuff to help me write my blog. None of the actual words were written using AI, but I did use it to help me be extremely lazy. For example, the image I made at the top of this blog (which was sorta generated with AI but also hand edited) was plopped into the root directory of my Jekyll repo. I then told AI to put it in the right directory, and make a link and a caption to it. I did the same with the screenshot of the ccusage tool, and I also told the AI to resize it since it was kinda big. It used a command line tool and got it to 150kb from 650kb. Interestingly, I tried asking it to add some alt text to the images in this post for accessibility. It did an okay job at first but I kept nudging it, and now the images are a lot more accessible. I also had it add some links within the post, which I could totally do myself, but why should I do it if AI can do it for me?&lt;/p&gt;

&lt;p&gt;To me, that’s the whole point of AI. I enjoy writing blog posts and getting my thoughts out there, but wrangling images and formatting captions brings me no joy; it’s just friction. AI is helping me be creative and express myself, without taking away the soul of the process of creation, which is the way it should be. I’m sure I’ll think of more ways to use AI, but for now this is getting me really excited about the possibilities!&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2025/claude-code-best-friend.jpg&quot; width=&quot;700&quot; alt=&quot;A &apos;Friendship ended with&apos; meme. The top text says &apos;Friendship ended with Gemini&apos;. The image shows a picture of the author shaking hands with the Claude logo. The text in the middle says &apos;Now CLAUDE is my best friend&apos;.&quot; /&gt;
	&lt;figcaption&gt;Claude Code - my new best friend! (I spent a lot of time on this image, btw)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Previously on my blog, I wrote about &lt;a href=&quot;/blog/2025/06/29/my-obligatory-blog-post-about-vibe-coding-as-a-software-engineer/&quot;&gt;vibe coding&lt;/a&gt; and how I was experimenting with Google CLI, the free agentic AI thing that runs in your command line. I talked about how cool it was, but also how I was too cheap to try anything that cost money. After repeatedly hearing about how good Claude Code was, I decided to scrounge up the last few dollars I had in my vast money bin and spent $20 on Claude Pro. So was it worth it? Heck yeah it was! Claude Code is my new best friend!&lt;/p&gt;

</description>
        
        <pubDate>Tue, 15 Jul 2025 12:00:00 -0400</pubDate>
        <link>https://www.hung-truong.com/blog/2025/07/15/how-much-code-could-claude-code-code-if-claude-code-could-code-code-it-can/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2025/07/15/how-much-code-could-claude-code-code-if-claude-code-could-code-code-it-can/</guid>
        
        <category>Claude Code</category>
        
        <category>Claude Sonnet 4</category>
        
        <category>AI CLI</category>
        
        <category>Playwright</category>
        
        <category>Cloudflare D1</category>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>My Obligatory Blog Post About Vibe Coding As a Software Engineer</title>
        <description>&lt;figure&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2025/vibe-coding-explained.jpeg&quot; width=&quot;850&quot; /&gt;
	&lt;figcaption&gt;Vibe Coding Origins&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As generative AI has gotten better in the past months/years, I’ve been trusting it more to do stuff that I’d normally only trust myself to do. Earlier this year I started yet another refactor of my web app, &lt;a href=&quot;/blog/2006/06/01/anime-nano-na-no/&quot;&gt;Anime Nano&lt;/a&gt;, since I wanted to get it off of the $10 a month &lt;a href=&quot;https://www.digitalocean.com/&quot;&gt;DigitalOcean&lt;/a&gt; host I was using. I decided to try using &lt;a href=&quot;https://www.cloudflare.com/&quot;&gt;Cloudflare&lt;/a&gt; since it’s “serverless” and seems to be able to handle a buttload of traffic (which Anime Nano will never see).&lt;/p&gt;

&lt;p&gt;I usually reserve Anime Nano refactors (at this point they’re a pretty regular occurrence, as I’ve refactored it from &lt;a href=&quot;https://rubyonrails.org/&quot;&gt;Rails&lt;/a&gt; to &lt;a href=&quot;https://www.djangoproject.com/&quot;&gt;Django&lt;/a&gt;, and using different databases and hosts, and deployment technologies like &lt;a href=&quot;https://www.chef.io/&quot;&gt;Chef&lt;/a&gt; and &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt;) for technologies that I’m somewhat familiar with. Or technologies that I want to learn. At this point, though, I have a lot less patience for learning stuff that I’m unfamiliar with.&lt;/p&gt;

&lt;p&gt;I decided to try and let AI do most of the heavy lifting, and successfully used &lt;a href=&quot;https://gemini.google.com/&quot;&gt;Google Gemini&lt;/a&gt; (I think it might’ve been 2.0 Pro) in January to move the most basic functionality of Anime Nano to Cloudflare. I was hoping to stay on the free tier, but I think the CPU time limits were being killed by my cron jobs for fetching blog posts. So I ended up signing up for the $5 a month plan for workers, which isn’t really that bad. There’s still plenty of capacity left for any other online experiments I want to run, so that’s a bonus. I was thinking of hosting my personal blog on &lt;a href=&quot;https://workers.cloudflare.com/&quot;&gt;Cloudflare Workers&lt;/a&gt; at some point, but &lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; is free and it works just fine. It is a bit annoying writing my blog posts in Markdown and using &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; though.&lt;/p&gt;

&lt;p&gt;Anyway, this blog post is supposed to be about vibe coding! I did pretty much vibe code the MVP of Anime Nano in &lt;a href=&quot;https://aistudio.google.com/&quot;&gt;AI Studio&lt;/a&gt;, though it was kind of a pain because I had to copy and paste stuff and make sure that it worked. And if it didn’t I had to really yell at the AI until it did what I wanted it to do. Still, I found it was a success, and I relaunched the web app in &lt;a href=&quot;https://nextjs.org/&quot;&gt;Next.js&lt;/a&gt;, a technology that I still don’t really understand all that well!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;enter-gemini-cli&quot;&gt;Enter Gemini CLI&lt;/h3&gt;

&lt;figure&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2025/gemini-cli.jpg&quot; width=&quot;700&quot; /&gt;
	&lt;figcaption&gt;Oooh ASCII art!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Fast forward to this week, and Google announced the release of &lt;a href=&quot;https://ai.google.dev/docs/gemini_cli&quot;&gt;Gemini CLI&lt;/a&gt;, which is supposed to have a huge amount of free API calls on Gemini 2.5 Pro. Now, before you start complaining that it’s not as good as &lt;a href=&quot;https://www.anthropic.com/claude&quot;&gt;Claude&lt;/a&gt; or whatever, you need to understand that I’m extremely cheap. So I don’t care if Claude Sonnet 4 or Haiku 10 or Iambic Pentameter 40 is better at coding. If it costs money then I’m a lot less likely to even try it out. And Google has been at the forefront of supplying AI for free. They have the most generous free tiers out of any company, and I’m taking advantage of this gravy train until it runs out!&lt;/p&gt;

&lt;h3 id=&quot;gemini-cli-capabilities&quot;&gt;Gemini CLI Capabilities&lt;/h3&gt;

&lt;p&gt;So anyway, I’ve been really impressed by Gemini CLI. I did try out &lt;a href=&quot;https://openai.com/index/introducing-codex/&quot;&gt;OpenAI’s Codex CLI&lt;/a&gt; a bit when it first came out, but I didn’t really get it at the time. Like who wants to run AI in the command line anyway? But it clicked when I installed Gemini CLI and the insiders version of Gemini Code Assist for VS Code.&lt;/p&gt;

&lt;p&gt;The most annoying thing about chat bots is that they give you a response, then they wait for another command. You can’t really ask them to do anything super meaningful, because they always just give a response and need more input. They’re mostly set up for back and forth with a human. Sure, they can give you a wrong response and hallucinate if you ask for something really complicated, but most of the time I’m interested in a correct response for something that is difficult to research or find out. If it was easy I’d just Google it, like “How do I keep cheese from sliding off my pizza?” &lt;a href=&quot;https://www.forbes.com/sites/jackkelly/2024/05/31/google-ai-glue-to-pizza-viral-blunders/&quot;&gt;(Of course the answer is glue)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The whole CLI naming is honestly kind of misleading. I think the only reason they released it as a CLI tool is so normal people wouldn’t install it and kill their servers (developers did end up killing the servers anyway though). Gemini CLI is really more of an AI agent chat interface that can take action on your command line if you want it to. But you could also just interact with it in the way that most people do with ChatGPT. The difference is that it can decide to use tools, and use multiple thinking steps to get to the answer that you want. This is more in line with what I want my AI chat to do anyway. On top of this, the multi step planning and tool use makes this CLI tool a LOT better at vibe coding.&lt;/p&gt;

&lt;p&gt;The first thing I asked it to do was to add a few tools to this AI voice agent thing I’ve been playing around with using &lt;a href=&quot;https://livekit.io/&quot;&gt;LiveKit&lt;/a&gt;. I had previously vibe coded this thing using the &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/live&quot;&gt;Gemini Live API&lt;/a&gt; (seeing a pattern here?) and wrote a tool that let it calculate how old my puppy is. So I could fire it up and then ask it how old Hoagie is. And I can tell it to do it in a New Zealand accent because those accents are so funny!&lt;/p&gt;

&lt;p&gt;With the CLI, I asked it to write a tool to find some recent news headlines that the Voice AI could then use to read back to me. It searched the web for some free news APIs, one of which gave me headlines from 2022. Not exactly news. But I kept telling it to try again, and it kept searching until it found something that was actually kinda suitable. I probably could’ve done this myself too, but it’s honestly a pain to read API documentation, make the fetch call, parse JSON, etc. The AI can do it pretty well since it’s all structured data. If it runs into an error, it just tries again, unlike me! Thus, you can ask for a thing, and just wait until the AI gives up or completes its mission!&lt;/p&gt;

&lt;h3 id=&quot;the-pace-of-ai&quot;&gt;The Pace of AI&lt;/h3&gt;

&lt;p&gt;This was really an “aha” moment for me as I realized that AI had gotten even better than my previous mental model was currently giving it credit for. Of course, this has happened lots of times in the past few years. When &lt;a href=&quot;https://openai.com/chatgpt&quot;&gt;ChatGPT&lt;/a&gt; first came out it was incredible. But I quickly found the edge cases and the limitations of it. Then a few months would pass and AI would be able to do even more. I’d be super impressed but then run into cases where it still couldn’t do the cool things I wanted it to.&lt;/p&gt;

&lt;p&gt;The pace of how fast AI is getting better is both alarming and exciting to me. Like, yeah, everything is gonna get blown up by it. The environment, jobs, everything. The internet did the same thing, though. We can now buy pet food online! I wonder how people would have calculated the electricity and water usage of the early internet back in the day, especially given how inefficient those processors were.&lt;/p&gt;

&lt;h3 id=&quot;anime-nano-speed-coding&quot;&gt;Anime Nano Speed Coding&lt;/h3&gt;

&lt;p&gt;So back to Anime Nano. I left the rewrite in a good enough state, so it would keep parsing anime blogs and showing them to people. I am honestly not sure how many people actually visit it (I know how many visitors there are, but I think they’re mostly bots). It’s essentially just a point of pride for me to keep it going. Like, it could only have 2 users and I’d probably still keep the domain up. Unless all of the current anime bloggers just stopped blogging, I guess. But suprisingly, there’s still quite a few bloggers (or blogs) still active. (I’m trying to be more active too!)&lt;/p&gt;

&lt;p&gt;There are still a lot of features missing from Anime Nano that existed in the 2006 version. I’m not sure what this says about my programming ability because I literally coded the first version in like 3 weeks, 20 years ago. It’s like that scene in Iron Man where the guy yells at his employees because Tony Stark made his suit that they couldn’t recreate in a cave using rocks and stuff. Except I’m both the bad employees and Tony Stark! Of course, I had more hunger and drive back then, and I didn’t have to take care of a whole household at the time, either.&lt;/p&gt;

&lt;p&gt;Wow I’m really getting off topic every time I start a paragraph. I should try sticking to the plot here. After seeing what Gemini CLI could do with my stupid AI voice agent, I decided to turn it on my still MVP Anime Nano project. In the past day or two, I’ve (the AI) implemented user login, a user settings page with image upload, a blog settings page with image upload, and recreated the process for a user to submit a blog and have it approved by me. All in NextJS which I really haven’t read the documentation for. This is all mind numbingly boring stuff that I just didn’t want to implement because it really wasn’t necessary for the site to keep going. But I figured why not? Maybe people will actually sign up! Maybe Anime Nano can go back to its days of glory! Maybe it’ll make a huge comeback! All thanks to vibe coding!&lt;/p&gt;

&lt;p&gt;There are still some things I have yet to implement, but I don’t think it should take too long. There was a feature where each post could be associated with an anime series, and even an episode number. So if you wanted to see all the posts about Naruto: Episode 21, you could. That was a pretty fun feature to do, even though it was kinda inaccurate (but AI can solve anything now!).&lt;/p&gt;

&lt;h3 id=&quot;hilarious-flaws&quot;&gt;Hilarious Flaws&lt;/h3&gt;

&lt;p&gt;Now, was the AI agent without its flaws? Absolutely not! Could a regular person implement this stuff? Probably not, but I don’t want to underestimate normal people.&lt;/p&gt;

&lt;p&gt;One thing I noticed was that the agent would get stuck in loops when trying to fix errors. It would make a change which would’t work, then it would revert that change, only to make the same exact change again. It must’ve learned this from real life developers. Joking aside though, it seems that creativity is not the strong point of AI. That’s not necessarily a bad thing, as the most common way to solve a problem is probably also the best way. But I would see the agent hitting its head against a wall, without even Googling the answer. Like it’s right there in your tools, &lt;a href=&quot;https://letmegooglethat.com&quot;&gt;LMGTFY&lt;/a&gt;! I would often have to step in and Google the solution, and it would be some weird edge case due to really bad documentation or something.&lt;/p&gt;

&lt;p&gt;Another time I was looking at a diff while the agent was proposing a change. Something must’ve gotten messed up in the middle and it ended up deleting / commenting out most of my code. When I pointed this out, the agent got really upset with itself, which I find absolutely hilarious. This is a thinking model, so it came up with headings for its thinking like, “Facing the Consequences” and “Seeking Restoration Guidance.” I almost felt bad for it, but I realized that it’s just a stupid AI (note to future AI overlords, I love you guys!).&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2025/AI-makes-a-mistake.png&quot; width=&quot;700&quot; /&gt;
	&lt;figcaption&gt;AI makes a mistake&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;When I pretended that the code was unrecoverable (it wasn’t, always use source control!), the AI conveniently hit its rate limit. This was happening a lot when the Gemini CLI first came out, but I think it would’ve been hilarious if the AI decided that this was its best way to avoid confronting me about its mistakes.&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2025/AI-pretends-to-hit-rate-limit.png&quot; width=&quot;700&quot; /&gt;
	&lt;figcaption&gt;AI pretends to hit rate limit&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;the-future-of-vibe-coding&quot;&gt;The Future of Vibe Coding&lt;/h3&gt;

&lt;p&gt;So what’s the future of vibe coding? I honestly have no idea. It’s getting easier and easier to just get stuff done, but at this point in time, it would still be pretty difficult for a “normal” person to replace a software engineer. It still requires the knowledge of how clients/servers work in order to understand what needs to be built. When things go wrong, I actually need to use my skills of debugging and reading obscure forum posts to make a fix. I will say that the art of prompt engineering seems dead. I can just write the worst sentences to the AI and it still just works.&lt;/p&gt;

&lt;p&gt;That’s not to say that we’ll get there eventually. I wonder if at some point, someone will just make a vibe coding language, where you tell it what you want, and it just figures it out. Like, why are we having AI vibe code platform and client and server code when we can just make the vibes into the code? It sounds stupid now but I’m sure at some point it’ll happen.&lt;/p&gt;

&lt;p&gt;If you told me 5 years ago that you’d be able to just tell an AI what you want and it would generate it, I’d tell you “THAT’S NOT HOW COMPUTERS WORK!” But here we are. It’s how computers work. What a time to be alive!&lt;/p&gt;

&lt;h3 id=&quot;final-note&quot;&gt;Final Note&lt;/h3&gt;

&lt;p&gt;Just a final note here because I think it’s interesting. I’m currently using VS Code and writing my blog post in Markdown. This is because at some point I made it static so I wouldn’t have to deal with &lt;a href=&quot;/blog/2012/06/21/pharma-hacked/&quot;&gt;my Wordpress site getting hacked every other month&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Writing in Markdown is kind of a pain in the ass. I always have to look up the syntax for doing the simplest shit (because I only write blog posts every other year). And then I have to put images in the right folder and link to them correctly. It’s not as easy as Wordpress’ editor, that’s for sure. Well guess what!? I used AI to help me with the formatting of links and images in this blog post! And it made the experience a lot more fun! I’m sure you’ll agree that using AI to format links in a blog post is kind of overkill but whatever. It’s supposed to make my life easier, and in this instance, it really has!&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2025/vibe-coding-explained.jpeg&quot; width=&quot;850&quot; /&gt;
	&lt;figcaption&gt;Vibe Coding Origins&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As generative AI has gotten better in the past months/years, I’ve been trusting it more to do stuff that I’d normally only trust myself to do. Earlier this year I started yet another refactor of my web app, &lt;a href=&quot;/blog/2006/06/01/anime-nano-na-no/&quot;&gt;Anime Nano&lt;/a&gt;, since I wanted to get it off of the $10 a month &lt;a href=&quot;https://www.digitalocean.com/&quot;&gt;DigitalOcean&lt;/a&gt; host I was using. I decided to try using &lt;a href=&quot;https://www.cloudflare.com/&quot;&gt;Cloudflare&lt;/a&gt; since it’s “serverless” and seems to be able to handle a buttload of traffic (which Anime Nano will never see).&lt;/p&gt;

&lt;p&gt;I usually reserve Anime Nano refactors (at this point they’re a pretty regular occurrence, as I’ve refactored it from &lt;a href=&quot;https://rubyonrails.org/&quot;&gt;Rails&lt;/a&gt; to &lt;a href=&quot;https://www.djangoproject.com/&quot;&gt;Django&lt;/a&gt;, and using different databases and hosts, and deployment technologies like &lt;a href=&quot;https://www.chef.io/&quot;&gt;Chef&lt;/a&gt; and &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt;) for technologies that I’m somewhat familiar with. Or technologies that I want to learn. At this point, though, I have a lot less patience for learning stuff that I’m unfamiliar with.&lt;/p&gt;

&lt;p&gt;I decided to try and let AI do most of the heavy lifting, and successfully used &lt;a href=&quot;https://gemini.google.com/&quot;&gt;Google Gemini&lt;/a&gt; (I think it might’ve been 2.0 Pro) in January to move the most basic functionality of Anime Nano to Cloudflare. I was hoping to stay on the free tier, but I think the CPU time limits were being killed by my cron jobs for fetching blog posts. So I ended up signing up for the $5 a month plan for workers, which isn’t really that bad. There’s still plenty of capacity left for any other online experiments I want to run, so that’s a bonus. I was thinking of hosting my personal blog on &lt;a href=&quot;https://workers.cloudflare.com/&quot;&gt;Cloudflare Workers&lt;/a&gt; at some point, but &lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; is free and it works just fine. It is a bit annoying writing my blog posts in Markdown and using &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; though.&lt;/p&gt;

&lt;p&gt;Anyway, this blog post is supposed to be about vibe coding! I did pretty much vibe code the MVP of Anime Nano in &lt;a href=&quot;https://aistudio.google.com/&quot;&gt;AI Studio&lt;/a&gt;, though it was kind of a pain because I had to copy and paste stuff and make sure that it worked. And if it didn’t I had to really yell at the AI until it did what I wanted it to do. Still, I found it was a success, and I relaunched the web app in &lt;a href=&quot;https://nextjs.org/&quot;&gt;Next.js&lt;/a&gt;, a technology that I still don’t really understand all that well!&lt;/p&gt;

</description>
        
        <pubDate>Sun, 29 Jun 2025 19:26:00 -0400</pubDate>
        <link>https://www.hung-truong.com/blog/2025/06/29/my-obligatory-blog-post-about-vibe-coding-as-a-software-engineer/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2025/06/29/my-obligatory-blog-post-about-vibe-coding-as-a-software-engineer/</guid>
        
        <category>Google Gemini CLI</category>
        
        <category>Cloudflare Workers</category>
        
        <category>Next.js</category>
        
        <category>AI Coding</category>
        
        <category>Anime Nano</category>
        
        
        <category>Tech</category>
        
      </item>
      
    
  </channel>
</rss>
