<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hung Truong: The Blog!</title>
    <description>I say potato, you say potato...</description>
    <link>http://www.hung-truong.com//blog/</link>
    <atom:link href="http://www.hung-truong.com//blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 12 May 2020 09:36:10 -0700</pubDate>
    <lastBuildDate>Tue, 12 May 2020 09:36:10 -0700</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      
      <item>
        <title>Reverse Engineering Quibi: Protocol Buffers and HLS Streaming</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/Quibi_Logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ve had a lot of free time in the past few weeks so I decided to spend some of it working on side projects. I really enjoy reverse engineering apps, so I decided to take a look at Quibi.&lt;/p&gt;

&lt;p&gt;Quibi (short for “Quick Bites”) is a video streaming app/service that has a bunch of shows that are short. The idea is that you can sit on a bus ride and consume an episode or two, depending on how long your commute is. One of the constraints of the platform is that you can only watch these videos on a phone or tablet with the app installed.&lt;/p&gt;

&lt;p&gt;Since everyone is stuck indoors for a while this constraint is kinda stupid and most people would probably like to watch their videos on their big tv rather than huddle around a phone, which is what Emily and I had to do to watch that stupid viral show about the &lt;a href=&quot;https://twitter.com/zachraffio/status/1250273191810875392&quot;&gt;terrible wife with the golden arm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, I had an idea to write a tvOS app that would work with Quibi, so you could watch your terrible shows on your tv. Here’s what I learned trying to reverse engineer the Quibi app.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;setting-up-charles&quot;&gt;Setting up Charles&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2020/charlesproxy.jpg&quot; /&gt;
	&lt;figcaption&gt;Get out of my head Charles!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The first rule of reverse engineering club is that you should probably install &lt;a href=&quot;https://www.charlesproxy.com/&quot;&gt;Charles proxy&lt;/a&gt; on your computer, and point your phone to it. This lets you inspect network requests and figure out the API for an app. Depending on how stringent the app’s security settings are, you can either learn a lot, or very little from Charles.&lt;/p&gt;

&lt;p&gt;The way Charles works is that you basically route all of your traffic from your iPhone to your computer. You need to install a certificate on your phone and trust it, so that your phone thinks that it’s going through a secure connection when it’s really getting owned. But since you’re self-owning, it’s generally okay. I’m not going to do a full tutorial on how to do this because you can just read the docs.&lt;/p&gt;

&lt;p&gt;You can also buy a version of Charles that runs directly on your phone. I did this, but I also find it easier to work with on my computer, so I used the free version that stops working every 30 minutes. I should probably buy the full version.&lt;/p&gt;

&lt;p&gt;Anyway, once you get the proxy working on your phone and add the proper domains to the allowed list, you should start seeing your network requests and responses pop up in Charles after firing up the app and doing stuff.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/charlesinaction.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It turns out (luckily for me) that Quibi doesn’t utilize certificate pinning (at least at the time I’m writing this article). Cert pinning is a way to prevent snooping of network requests by embedding a certificate (or maybe a hash of it or a public key) to be trusted into the actual app binary. This means that adding the additional Charles certificate won’t work because the app won’t accept it. The process of pinning is kind of a pain in the ass, which is why a lot of companies don’t do it.&lt;/p&gt;

&lt;p&gt;Because Quibi doesn’t use cert pinning, that means I can observe all of the requests and responses that the app sends and receives, which makes it a lot easier to reverse engineer!&lt;/p&gt;

&lt;h3 id=&quot;authentication&quot;&gt;Authentication&lt;/h3&gt;
&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2020/quibi_oauth.png&quot; /&gt;
	&lt;figcaption&gt;This is what the Quibi OAuth request/response look like with the private stuff removed&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I won’t go too much into the way that Quibi does authentication, as it appears to be a pretty basic implementation of OAuth 2.0, using auth0 as a service provider. It doesn’t appear to use a client secret key, as I’m able to just replay the request with my username, password, client id and other parameters and get a valid access token back. One interesting thing to note here is how short the token expiration is, just 2700 seconds, or 45 minutes.&lt;/p&gt;

&lt;p&gt;Once authentication happens, the access token is used in subsequent requests as the authorization bearer header tokens.&lt;/p&gt;

&lt;p&gt;I didn’t bother to set up a new app with authentication because I wanted to get to the meat of the app, and started taking a look at how the app’s requests and responses were structured, which brings us to…&lt;/p&gt;

&lt;h3 id=&quot;protocol-buffers&quot;&gt;Protocol Buffers!&lt;/h3&gt;

&lt;p&gt;I’ll be honest, I was initially pretty annoyed when I read the line,&lt;/p&gt;

&lt;p&gt;&lt;code&gt;content-type: application/protobuf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;in Charles. That’s because &lt;a href=&quot;https://developers.google.com/protocol-buffers&quot;&gt;protocol buffers&lt;/a&gt; are pretty annoying to work with unless you actually have access to the .proto files that were used to generate the schema for the response.&lt;/p&gt;

&lt;p&gt;Luckily, I have experience working with protobufs (as the cool kids call them) because we started implementing them at Lyft last year. As a primer, protocol buffers basically define requests and responses, and their types, in a way that can be shared between servers, clients, etc. It does so in a way that reduces the amount of redundancy in the data. So instead of looking at a JSON file that labels each key/value pair for each item in an array, you just see the values, and the keys (and their types) are essentially encoded outside of the format itself. That’s probably a gross simplification of protobufs, but that’s basically how I understand their functionality in lay person terms.&lt;/p&gt;

&lt;p&gt;So getting back to Quibi, I was seeing calls to endpoints like&lt;/p&gt;

&lt;p&gt;&lt;code&gt;https://qlient-api.quibi.com/quibi.qlient.api.home.Home/GetHomeCards&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;with a bunch of wacky characters, urls, and names and descriptions of tv shows. Looking at the raw text gave me an idea of what was being returned, but there was also a lot of data that I couldn’t see because it was encoded as a protobuf response.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/protobuf_raw.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I found a tool written by someone named &lt;a href=&quot;https://github.com/mildsunrise&quot;&gt;Alba Mendez&lt;/a&gt; called &lt;a href=&quot;https://github.com/mildsunrise/protobuf-inspector&quot;&gt;protobuf-inspector&lt;/a&gt; which allows you to visualize the values in a protobuf response. Once I threw the response into this tool, it started to make a lot more sense.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/protobuf_inspector.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, I could see that the home cards were displaying structured information for each of the cards in the app. The hierarchy seemed to be a card with series info, a mp4 preview link, and then info about that particular episode. There were also some values that didn’t really seem useful, like “&amp;lt;varint&amp;gt; = 1” which could’ve meant anything, like a bool value or episode number.&lt;/p&gt;

&lt;p&gt;The tool has a way to set up object definitions, so that when you run it again, you see the key names and types that you defined. This is helpful if you’re trying to guess what something is and you want to compare it against a few different objects. Here’s what the response looked like once I tried defining some of the keys:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/protobuf_inspector_labeled.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This makes the response look a bit more logical. I really guessed some of these, so if someone from Quibi actually reads this maybe you can confirm. I probably spent more time on this than I needed, because really all I want is to show a list of shows and maybe even start watching a show.&lt;/p&gt;

&lt;p&gt;To waste even more time, I ended up defining this response in a .proto file, compiled it into Swift with swift-protobuf and got a Swift app to parse the response into real Swift structs! Here is the proto definition:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/ad4a62c9c1d86a2c187267bc353d8284.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The generated proto code in Swift is pretty big, so I’ll leave it as an exercise to the reader to run it through swift-protobuf.&lt;/p&gt;

&lt;p&gt;Here’s what it looks like running in the debugger in my app:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/proto_swift.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So with what I have written about so far, I could actually write a functioning Quibi client that supports logging in, making a request to get a list of tv show cards, and displays them, with a “live” preview, just like the real app. That doesn’t matter much unless you can actually watch the show, though.&lt;/p&gt;

&lt;h3 id=&quot;hls-and-video-streaming&quot;&gt;HLS and Video Streaming&lt;/h3&gt;

&lt;p&gt;I apologize for the anti-climactic finale of this blog post, but this is where I got stuck.&lt;/p&gt;

&lt;p&gt;Before I tried reverse engineering this app, I pretty much knew nothing about video streaming. When looking at the chain of requests that the app makes, it appears that the app hits an endpoint called “GetPlaybackInfo” and sends a payload of series id, season #, and episode # along with a mystery UUID that I haven’t seen anywhere else in the app requests/responses, then receives a link to a “license” url, a few links to .m3u8 resources and some cookies for accessing those .m3u8 resources.&lt;/p&gt;

&lt;p&gt;Then the app makes a request to the license url with some form encoded data and receives some other encoded data back. Finally, the app makes a request to one of the .m3u8 files and starts streaming the video.&lt;/p&gt;

&lt;p&gt;I did &lt;a href=&quot;https://developer.apple.com/streaming/&quot;&gt;some research&lt;/a&gt; and it looks like a .m3u8 url basically provides the client with a way or ways to display the video to the user. It can include things like different video streams with varying quality, and it looks like it even has some subtitle file support.&lt;/p&gt;

&lt;p&gt;I tried just replaying the call to the .m3u8 file with the same authentication cookie and it unfortunately didn’t work. I think that the license url provides the app with a way to decode the video, and without knowing what to send or how to decode it, I think I’m essentially stuck.&lt;/p&gt;

&lt;p&gt;I sort of didn’t expect to be able to finish this app anyway, so I’m pretty happy with how far I got. I also figure that if I try to go any further with this, Quibi will probably try to sue me or something, so it probably isn’t worth it. In any case, I did learn a lot from this project, and hopefully you have too, from reading this post. If you have any ideas on how I would proceed or if you enjoyed this post, feel free to let me know!&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/Quibi_Logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ve had a lot of free time in the past few weeks so I decided to spend some of it working on side projects. I really enjoy reverse engineering apps, so I decided to take a look at Quibi.&lt;/p&gt;

&lt;p&gt;Quibi (short for “Quick Bites”) is a video streaming app/service that has a bunch of shows that are short. The idea is that you can sit on a bus ride and consume an episode or two, depending on how long your commute is. One of the constraints of the platform is that you can only watch these videos on a phone or tablet with the app installed.&lt;/p&gt;

&lt;p&gt;Since everyone is stuck indoors for a while this constraint is kinda stupid and most people would probably like to watch their videos on their big tv rather than huddle around a phone, which is what Emily and I had to do to watch that stupid viral show about the &lt;a href=&quot;https://twitter.com/zachraffio/status/1250273191810875392&quot;&gt;terrible wife with the golden arm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, I had an idea to write a tvOS app that would work with Quibi, so you could watch your terrible shows on your tv. Here’s what I learned trying to reverse engineer the Quibi app.&lt;/p&gt;

</description>
        
        <pubDate>Tue, 12 May 2020 00:00:00 -0700</pubDate>
        <link>http://www.hung-truong.com//blog/2020/05/12/reverse-engineering-quibi/</link>
        <guid isPermaLink="true">http://www.hung-truong.com//blog/2020/05/12/reverse-engineering-quibi/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Cloning Zwift on iOS Part 4: Workout View</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workoutview.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I haven’t been working as much on my workout app since it’s been good enough for me to use as a replacement for Zwift with the most recent changes I wrote about.&lt;/p&gt;

&lt;p&gt;I got a new iPhone so I had to build and install the app on my phone again, and after firing up Xcode I decided to go ahead and add a few more things that I was meaning to add to the app. The app hasn’t had a good visualization of workouts so I ended up creating an interface for that.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;taking-cues-from-zwift&quot;&gt;Taking cues from Zwift&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/emilysshortmix.jpg&quot; /&gt;
	&lt;figcaption&gt;Emily's Short Mix, as seen in Zwift&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I didn’t want to reinvent the wheel here, so I decided to just copy how Zwift shows workouts, because it works pretty well. Not that I’m trying to write a whole 3d cycling MMORPG, but it helps to know where I am in a workout besides time elapsed and time remaining.&lt;/p&gt;

&lt;p&gt;The general idea is to code each segment with its color based on the difficulty zone, and its height based on the target wattage of that segment.&lt;/p&gt;

&lt;p&gt;I previously noted that the target wattage of a segment is based on the product of your own FTP and a multiplier. I did some reverse engineering and found that the color coded “zones” are based on this formula:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;0-60% = zone 1&lt;/li&gt;
  &lt;li&gt;61-75% = zone 2&lt;/li&gt;
  &lt;li&gt;76-89% = zone 3&lt;/li&gt;
  &lt;li&gt;89-104% = zone 4&lt;/li&gt;
  &lt;li&gt;105-118% = zone 5&lt;/li&gt;
  &lt;li&gt;119%+ = zone 6&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I then just mapped the zones with the same colors, with zone 1 being gray, 2 being blue, etc. For the warmup and cooldown segments I just take the high power of those segments.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/5a3c38401b0f8a5dffbaf543bdd67e31.js&quot;&gt;&lt;/script&gt;

&lt;h3 id=&quot;creating-a-view&quot;&gt;Creating a View&lt;/h3&gt;

&lt;p&gt;Creating custom views from scratch is really not my strong point, but I know enough about autolayout and UIKit to implement whatever views a designer throws at me. Unfortunately I don’t have designer to throw designs at me so I had to come up with something on my own.&lt;/p&gt;

&lt;p&gt;I was originally going to use a .xib file and set things up programmatically, but I realized that I really didn’t need a .xib and could do everything programmatically. I usually go with a hybrid approach to make the complicated parts easier, but in this case doing stuff in code is actually less complicated.&lt;/p&gt;

&lt;p&gt;I ended up going with a stack view based layout, where I inserted the views into a horizontal stack view. The view itself has a width anchor based on the percentage of time it takes (if it’s a 10 minute segment in a 30 minute workout then its width would be 33.3% of the container), and then a subview which has a height anchor based on the relative intensity of the segment. It works out pretty well.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/1166ad11e921cb21470feed64ffb9e3f.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;I threw the view into a UITableViewCell so I could visually get a clue what the workout I was selecting looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workoutslist.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;finally-some-progress&quot;&gt;Finally Some Progress&lt;/h3&gt;

&lt;p&gt;I was pretty happy with the stack view representation of the workouts and left it that way for a few months. Today I was refactoring some stuff and implemented something that was really bothering me for a while. In Zwift you get a really nice view of the upcoming segments, and where you are in your current segment. My app is not really good at this, and there was a bug where it not correctly display the upcoming segment sometimes because of the way it handled workout segment equality. Essentially it was looking for the index of a segment but if there were two exact same segments it would assume the first one was the correct one.&lt;/p&gt;

&lt;p&gt;Anyway, I fixed that bug by just tracking the actual index of the segment I was on.&lt;/p&gt;

&lt;p&gt;The other thing I wanted to add was a progress view, so I broke the Workout view into its own class and added a “progress” indicator, which is just a horizontal slider that slides over as you’re working out.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/c5963ec63481f39877e562da3c93a13a.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;This is what it looks like, if I was working out at 10x the regular speed (which would be easier I guess).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/ZSwiftWorkoutView.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s about it for now! I think this makes my app pretty usable and until I find another thing I need to add to it, I feel like this is a good replacement for Zwift. If you happen to need the exact same thing that I do and also happen to have the same exact exercise bike, feel free to clone the &lt;a href=&quot;https://github.com/hungtruong/Zswift&quot;&gt;Github&lt;/a&gt; repo and use the app!&lt;/p&gt;

</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workoutview.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I haven’t been working as much on my workout app since it’s been good enough for me to use as a replacement for Zwift with the most recent changes I wrote about.&lt;/p&gt;

&lt;p&gt;I got a new iPhone so I had to build and install the app on my phone again, and after firing up Xcode I decided to go ahead and add a few more things that I was meaning to add to the app. The app hasn’t had a good visualization of workouts so I ended up creating an interface for that.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 29 Sep 2019 00:00:00 -0700</pubDate>
        <link>http://www.hung-truong.com//blog/2019/05/03/making-a-zwift-clone-part-4-workout-view/</link>
        <guid isPermaLink="true">http://www.hung-truong.com//blog/2019/05/03/making-a-zwift-clone-part-4-workout-view/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Introducing BarkMode: Bark Detection + Dark Mode</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barkmode.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was in California about a month ago for work and I was able to attend a few events during WWDC week. I read a lot about the new features and APIs but didn’t really have a lot of time to mess with stuff like SwiftUI, etc. I got a stupid idea for a project but I didn’t really take the time to work on it until this weekend. Now I’d like to introduce you to my latest app: Bark Mode!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;wtf-is-bark-mode&quot;&gt;WTF is Bark Mode?&lt;/h3&gt;

&lt;p&gt;Right after the new APIs were announced at WWDC, I scanned them for anything that might be interesting. I really like being one of the first to try out a new technology, but I figured that everyone would be all over &lt;a href=&quot;https://developer.apple.com/xcode/swiftui/&quot;&gt;SwiftUI&lt;/a&gt;, and I was right. I found a somewhat obscure new feature as part of CreateML that allowed users to generate a ML model that could classify sounds.&lt;/p&gt;

&lt;center&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;This SoundAnalysis API in particular seems relevant to my typically immature interests. 💨 &lt;a href=&quot;https://twitter.com/hashtag/WWDC19?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#WWDC19&lt;/a&gt; &lt;a href=&quot;https://t.co/0Aipwa99Lu&quot;&gt;https://t.co/0Aipwa99Lu&lt;/a&gt;&lt;/p&gt;&amp;mdash; Hung Truong (@hungtruong) &lt;a href=&quot;https://twitter.com/hungtruong/status/1135947285035020290?ref_src=twsrc%5Etfw&quot;&gt;June 4, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; 
&lt;/center&gt;

&lt;p&gt;Of course the first thing I thought of was to make an app that could detect farts. I could even make it so that a fart would toggle dark mode off/on. Though I wanted to start experimenting right away, I discovered that you actually needed to have the newest version of macOS Catalina installed in addition to the newest Xcode to use Create ML, and I didn’t have my personal computer handy and couldn’t install a beta OS on my work computer, so I ended up not building it.&lt;/p&gt;

&lt;p&gt;Fast forward to this weekend, and I eventually got Catalina and all the other prerequisites I needed to get started. Instead of “FarkMode” which doesn’t really make sense, I decided to switch it up this time and detect dogs barking, since “Bark” rhymes better with “Dark” than “Fart.” Plus everybody loves dogs! Perhaps I’m finally maturing in my old age (probably not).&lt;/p&gt;

&lt;p&gt;Anyway, here’s the steps I needed to take to make my BarkMode app.&lt;/p&gt;

&lt;h3 id=&quot;creating-the-model&quot;&gt;Creating the model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/Create ML.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I had seen some videos of people making image classifiers using &lt;a href=&quot;https://developer.apple.com/documentation/createml&quot;&gt;CreateML&lt;/a&gt; in the past. It seemed as easy as creating a few folders with different labels and images of those things. You would have to create one batch for training images, and another for optionally testing the models once they were created.&lt;/p&gt;

&lt;p&gt;The process for creating an audio classifier is pretty similar except that you need to use sound files instead of images. I started by finding a bunch of stock sound effects of dogs barking. I also found a bunch of random stock sound effects of things like bells ringing, wind blowing, and other stuff that I could label as “not barking.” I feel like this is a pretty wacky way to do classification but if there’s another way that doesn’t involve downloading a bunch of random stock sound effects, that would be awesome.&lt;/p&gt;

&lt;p&gt;At first I took larger sound files and chopped them up into really short ones. This way I had a bunch of training data that I can use both to train and test my classifier.&lt;/p&gt;

&lt;p&gt;However, when I ran the Create ML app on the training data, I got an obscure error that the training had been interrupted. I saw that the app thought I only had 3 files when I had created a bunch more. I believe that Create ML is unable to train with really short audio files (less than a second). I ended up inserting the full sound files instead of the short ones. At this point I think I had about 8 or 9 files for either barking or not barking.&lt;/p&gt;

&lt;p&gt;The model was created but it had a really terrible accuracy. I decided to just add a bunch more files both of dogs barking and things that weren’t dogs barking. The more examples I added the “better” the model became.&lt;/p&gt;

&lt;h3 id=&quot;polishing-the-model&quot;&gt;Polishing the model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barktest.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Create ML has a feature where you can test your audio classifier with audio data coming from your own microphone. When I did this I noticed that the model was too biased towards no sound being barking. I’m not sure why but maybe white noise sounds more like barking than an old timey car horn?&lt;/p&gt;

&lt;p&gt;I ended up adding a bunch of white noise and recorded some nothing to add to the model. This helped quite a bit. The last thing I did was also record some audio of myself talking so that the model would not recognize me talking as barking. This was useful so I wouldn’t get false positives during my demo video (which I ended up getting anyway in a bunch of outtakes).&lt;/p&gt;

&lt;h3 id=&quot;integrating-the-model-in-the-app&quot;&gt;Integrating the model in the app&lt;/h3&gt;

&lt;p&gt;I created a new app and promptly enabled “Use SwiftUI.” Then I promptly made a new project because I couldn’t figure out where my ViewController class was! I’ll take a look at SwiftUI later but for now I’ll focus on getting my BarkMode app actually working.&lt;/p&gt;

&lt;p&gt;The Create ML app made a model that I could then copy into my app. It was really as simple as dragging it from one place to another.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/mlmodel.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking at the model in Xcode, it describes an interface with an input of “audioSamples” that takes a MultiArray of (Float32 15600). I assumed this had something to do with the number of samples per second and the bitrate of the audio. I fiddled around with the AudioToolbox framework and a few other lower level audio APIs until I discovered Apple’s documentation on the &lt;a href=&quot;https://developer.apple.com/documentation/soundanalysis&quot;&gt;SoundAnalysis&lt;/a&gt; framework which provides a much, much easier method of feeding audio to the model.&lt;/p&gt;

&lt;p&gt;I implemented the steps described and extended my ViewController to conform to the SNResultsObserving protocol. Then it was a few simple steps to write some logic to handle the app detecting no barking to barking and back to no barking, and toggling the dark mode off and on.&lt;/p&gt;

&lt;p&gt;Finally, I added a label and an image view that takes an image with different assets for the light and dark modes, which changes automatically based on the current setting.&lt;/p&gt;

&lt;p&gt;The model runs pretty quickly. Here’s a gif of the terminal output from the logging whenever some sound data comes in:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barknobark.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you want to see the app in action, I created this video to demonstrate:&lt;/p&gt;

&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/0XRxA1cEHio&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;p&gt;As it is, this app isn’t very useful for much of anything, but I could see a few potential uses for the model I trained:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Write an app that detects your dog is unhappy when it’s at home and triggers a home automation action like playing music or shooting a treat at your dog&lt;/li&gt;
  &lt;li&gt;Detect if suspicious sound at your house is a robber or just a dog&lt;/li&gt;
  &lt;li&gt;Classify different types of barks and make a dog barking translation app&lt;/li&gt;
  &lt;li&gt;For people who are allergic to dogs, advance warning of incoming dog&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I’ve posted the full project to &lt;a href=&quot;https://github.com/hungtruong/BarkMode&quot;&gt;Github&lt;/a&gt; in case you’re curious about how I implemented the app. I didn’t upload the Create ML project but it just has a bunch of sound effect files and files of me saying gibberish. The model included in the Github repo should work fine anyway.&lt;/p&gt;

&lt;p&gt;I’m pretty happy with my end result. It’s fun to play around with new APIs and now I can say that I’ve trained an ML model to classify sounds, in addition to implementing Dark Mode! Hopefully I’ll have some time to play with other APIs that were introduced to iOS soon.&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barkmode.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was in California about a month ago for work and I was able to attend a few events during WWDC week. I read a lot about the new features and APIs but didn’t really have a lot of time to mess with stuff like SwiftUI, etc. I got a stupid idea for a project but I didn’t really take the time to work on it until this weekend. Now I’d like to introduce you to my latest app: Bark Mode!&lt;/p&gt;

</description>
        
        <pubDate>Sun, 14 Jul 2019 00:00:00 -0700</pubDate>
        <link>http://www.hung-truong.com//blog/2019/07/14/introducing-bark-mode-bark-detection-dark-mode/</link>
        <guid isPermaLink="true">http://www.hung-truong.com//blog/2019/07/14/introducing-bark-mode-bark-detection-dark-mode/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Cloning Zwift on iOS Part 3: HealthKit and a WatchOS App!</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/healthkit.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ve been a bit slow to update my blog series about trying to make a clone of
Zwift, but not because I’ve stopped working on it. Rather, I’ve been able to use
the “MVP” of what I’ve built so far in parts
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;1&lt;/a&gt;
and
&lt;a href=&quot;/blog/2019/04/07/making-a-zwift-clone-part-2/&quot;&gt;2&lt;/a&gt;,
and I was finding that the time I spent working on my app could be used actually
working out. Like, I literally would write an implementation of something, but
it would take so much of my time that I couldn’t test it out and I’d have to go
to bed… Still, I was missing a few important features in my app, so I’ve been
slowly working on them in between working on my fitness.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;hooking-up-the-apple-watch&quot;&gt;Hooking up the Apple Watch&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwift-bluetooth.png&quot; /&gt;
	&lt;figcaption&gt;Zwift’s interface for connecting the Apple Watch works really well&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;One of the great things about Zwift is how much support they provide for
different fitness accessories, including the Apple Watch. Unfortunately, the
Apple Watch hardware is not set up to allow arbitrary Bluetooth connections like
my exercise bike was in part 1. Instead, to access the user’s health information
like heart rate, you need to write a full blown WatchOS app!&lt;/p&gt;

&lt;p&gt;Luckily, this wasn’t my first rodeo as I worked on the Apple Watch app for
Starbucks, so I was able to add a Watch app extension target in my project
pretty quickly.&lt;/p&gt;

&lt;p&gt;I Googled for how to get a user’s heart rate programmatically, came across a
&lt;a href=&quot;https://stackoverflow.com/questions/38158841/live-heart-rate-in-watchos-3&quot;&gt;promising StackOverflow
post&lt;/a&gt;
with a link to a &lt;a href=&quot;https://github.com/coolioxlr/watchOS-3-heartrate/blob/master/VimoHeartRate%20WatchKit%20App%20Extension/InterfaceController.swift&quot;&gt;Github
project&lt;/a&gt;,
and was able to get it implemented myself. However, as I looked at the
copy-pasta code, it seemed sort of wrong to me. The code was starting a workout
session, but then created an object query that would run a closure whenever a
new heart rate sample (older than a certain date) was added by the workout. This
seems like a really roundabout way to get heart rate samples, and I wondered if
Apple had a better API to accomplish this.&lt;/p&gt;

&lt;h3 id=&quot;implementing-healthkit&quot;&gt;Implementing HealthKit&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/healthkit_hero.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I ended up finding some Apple &lt;a href=&quot;https://developer.apple.com/documentation/healthkit/workouts_and_activity_rings/speedysloth_creating_a_workout&quot;&gt;sample
code&lt;/a&gt;
that showed a better way to fetch heart rate data. The solution is to use some
new features introduced in WatchOS 5 that allow for creation of a workout
directly on the Apple Watch. The Apple doc I linked explains it pretty well, but
the steps are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ask the user for permission to track their heart rate data&lt;/li&gt;
  &lt;li&gt;Create a workout configuration (e.g. an indoor cycling workout) and a workout
session, along with its associated workout builder&lt;/li&gt;
  &lt;li&gt;Start the session and tell the workout builder to start collecting data samples&lt;/li&gt;
  &lt;li&gt;Respond to the delegate method &lt;em&gt;“workoutBuilder(_:didCollectDataOf:)”&lt;/em&gt; to collect
a bunch of samples, including heart rate information&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In code it looks something like this:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/66b941af4f5ce3b9a66cd041fd94f3d5.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/918839f7ba4a213c92817b387a6fd91f.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Rather than add some UI to the watch app to start a workout session, the iPhone
version of &lt;em&gt;HKHealthStore&lt;/em&gt; has a function called
&lt;a href=&quot;https://developer.apple.com/documentation/healthkit/hkhealthstore/1648358-startwatchapp&quot;&gt;startWatchApp(with:completion:)&lt;/a&gt;
which will send a workout configuration to the watch to facilitate the creation
of a workout. All I need to do is call that function when my workout on the
iPhone app starts and my watch app will respond by starting a HealthKit workout
session which starts measuring things like heart rate (and calculating its own
estimated calories burned).&lt;/p&gt;

&lt;p&gt;I was now able to get the heart rate as the watch was reading it, and update
whatever display on the watch I wanted to. That was only half the story though.
In Zwift the heart rate shows up in the user interface, and I wanted to mimic
that myself. Since I couldn’t access the workout session directly from the phone
I’d have to send the heart rate info back to the main app from the watch.&lt;/p&gt;

&lt;h3 id=&quot;back-to-the-app&quot;&gt;Back to the App&lt;/h3&gt;

&lt;p&gt;This blog post isn’t about Watch apps, so I won’t go over that aspect of this
feature too much. I basically used the WatchConnectivity session to send
messages back to the app with a dictionary containing the new heart rate.&lt;/p&gt;

&lt;p&gt;And after all of that programming, I’d like to present you with the most
difficult video I’ve ever shot: on an iPad, while balancing on an exercise bike,
recording both my wrist watch with my heart rate AND the app showing the exact
same heart rate!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/zwift-part-3.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also rigged up an initial interface that shows which workout segment I’m on,
the next segment coming up, the progress through the segment and my progress for
the entire workout, along with stats like calories burned (determined by the
bike), cadence and distance traveled.&lt;/p&gt;

&lt;p&gt;At this point I have a pretty functional app! But seeing the extensive APIs of
HealthKit made me want to add more and more to my app. This is scope creep in
action. See the documentation of
&lt;a href=&quot;https://developer.apple.com/documentation/healthkit/hkworkoutbuilder&quot;&gt;HKWorkoutBuilder&lt;/a&gt;
to see all of the data and metadata you can store. I ended up sending a few more
messages from the app back to the watch so I could store more data to the
workout:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/a45a91a9d598a448db7ce3947d70304d.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;At the end of a workout, I send the start and end times, along with the total
calories burned and distance traveled. This isn’t really necessary because the
watch already makes a guess about the calories burned and the distance isn’t
real because it’s on a stationary bike. But I thought it might be interesting to
see how that data is represented.&lt;/p&gt;

&lt;p&gt;I also toyed around with sending segment data but I haven’t seen it visually
represented anywhere in the workout view. I wanted to see more detail about the
workout in the Apple Activity app so I also sent the name of the workout as the
&lt;em&gt;HKMetadataKeyWorkoutBrandName&lt;/em&gt; value, though I’m not sure that’s what it’s
intended for! Here’s what the workouts look like in the Activity app and the
Health app’s workout data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workout.png&quot; /&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2019/sample.png&quot; /&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2019/sample2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One more fun but optional thing I thought of and added was a wrist tap reminder
when I got close to the end of a segment. Sometimes I’m just in the zone and not
paying attention to the fact that I need to ramp up or ramp down for the next
section, so when there’s 5 seconds left in a segment I send a message to the
watch from the phone to tap my wrist and send a reminder:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/9ebc2061b51cd3b13b3acffbb9930398.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;One of the nice things about writing your own workout app is that you don’t need
to wait for a third party developer to implement any ideas you have for the app!
I think that’s actually the only nice thing…&lt;/p&gt;

&lt;p&gt;Anyway, I’m pretty happy with the results. Next up, I plan on adding a bit of
visual polish to the interface and maybe even create an app icon! I also want to
aggregate the data like heart rate info, watt effort, etc and keep track of
statistics and chart the data, perhaps in real time. I find it very motivating
to compare my effort in the same exact workout across different days to see if
I’m improving (maybe by measuring heart rate average).&lt;/p&gt;

&lt;p&gt;As usual, my changes are in &lt;a href=&quot;https://github.com/hungtruong/Zswift&quot;&gt;Github&lt;/a&gt; in
case you happen to have the same exact exercise bike as me or are curious how I
implemented certain things.&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/healthkit.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ve been a bit slow to update my blog series about trying to make a clone of
Zwift, but not because I’ve stopped working on it. Rather, I’ve been able to use
the “MVP” of what I’ve built so far in parts
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;1&lt;/a&gt;
and
&lt;a href=&quot;/blog/2019/04/07/making-a-zwift-clone-part-2/&quot;&gt;2&lt;/a&gt;,
and I was finding that the time I spent working on my app could be used actually
working out. Like, I literally would write an implementation of something, but
it would take so much of my time that I couldn’t test it out and I’d have to go
to bed… Still, I was missing a few important features in my app, so I’ve been
slowly working on them in between working on my fitness.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 03 May 2019 00:00:00 -0700</pubDate>
        <link>http://www.hung-truong.com//blog/2019/05/03/making-a-zwift-clone-part-3/</link>
        <guid isPermaLink="true">http://www.hung-truong.com//blog/2019/05/03/making-a-zwift-clone-part-3/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Making an iOS Zwift Clone to Save $15 a Month! Part 2: Reverse Engineering a Workout</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwiftworkoutheader.png&quot; /&gt;
	&lt;figcaption&gt;A very colorful Zwift Workout&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Last time, on “Making an iOS Zwift Clone to Save $15 a Month” I wrote about
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;learning Core Bluetooth to connect to my exercise bike and get data streaming
directly to my
app&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since writing that article, I cleaned up the implementation of the Core
Bluetooth service a bit and started supporting some additional data like
distance, calories burned and cycling cadence.&lt;/p&gt;

&lt;p&gt;While cycling on my exercise bike and staring at these numbers is fun, the
built-in screen on my bike already shows these numbers, so I essentially
recreated a subset of the official
&lt;a href=&quot;https://www.concept2.com/service/software/ergdata&quot;&gt;ergData&lt;/a&gt; app so far.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/ergdata.jpg&quot; /&gt;
	&lt;figcaption&gt;The ergData app is functional but ugly af&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I realized the next challenge would be to start a guided workout in my app and
show the target wattage alongside my actual wattage on the bike.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;enter-the-workout&quot;&gt;Enter the Workout&lt;/h3&gt;

&lt;p&gt;Zwift workouts seem pretty simple to begin with. You can set up a number of
workout segments to guide your workout. There are warmups, steady states,
intervals, free ride sections and cool-downs. At any given point in your
workout, Zwift will tell you to put in a certain amount of effort. You just try
to get your watt number to match what Zwift wants you to do.&lt;/p&gt;

&lt;p&gt;At first I figured I could come up with my own way of representing workouts, but
that would also require me to re-create all of the workouts I usually use on
Zwift. Zwift also has a nice workout editor that can use to edit preset workouts
or create completely new ones on your own.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwiftworkoutedit.png&quot; /&gt;
	&lt;figcaption&gt;Zwift’s built-in workout editor is pretty nice!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I discovered that if you copy an existing workout to edit it, the workout will
be saved as user data and will be synced between all devices. How is the workout
data saved, you ask? No, not as JSON as a sane developer would use. It’s XML.
Okay, so technically it’s a .zwo file but I know what XML looks like! Here’s a
sample of an exported workout file:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwofile.png&quot; /&gt;
	&lt;figcaption&gt;Hello darkness, my old friend&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So I went ahead and implemented an XMLParser in Swift to turn .zwo files into my
own Workout struct. I won’t bore you with the implementation details but it was
kind of a walk down memory lane dealing with the delegation-based API of
XMLParser. I’m surprised there isn’t a more modern solution in Foundation but
I’m guessing it’s because most people have moved on from XML.&lt;/p&gt;

&lt;h3 id=&quot;lets-reverse-engineering&quot;&gt;Let’s Reverse Engineering&lt;/h3&gt;

&lt;p&gt;By reading the .zwo files, I was able to figure out how Zwift represents their
workouts and basically reverse engineer the structure of a workout:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Warmups have a duration, a low and high power (which is a percentage of
&lt;a href=&quot;https://zwift.com/news/4100-zwift-how-to-understanding-finding-your-ftp&quot;&gt;FTP&lt;/a&gt;).
The warmup begins at the low power and steadily ramps up to the high power by
the end of the segment.&lt;/li&gt;
  &lt;li&gt;SteadyStates are the simplest, with just a duration and power.&lt;/li&gt;
  &lt;li&gt;Intervals are the most complicated, with a repeat number, onDuration,
offDuration, onPower and offPower. The intervals segment goes between the high
and low power levels, and repeats a certain number of times. I didn’t want to
deal with the extra complexity of this type so I just wrote some code to expand
them out into SteadyStates.&lt;/li&gt;
  &lt;li&gt;Cooldowns are like warmups but in reverse.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are also some other things like text captions that appear during a
workout, free ride segments and cadence settings that I didn’t bother to
implement because I don’t use them. The cadence stuff makes sense if you’re
using a smart trainer but my bike doesn’t support automatically changing
resistance anyway.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/55e92115f993221693f9040431ee39fe.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;I decided to represent the WorkoutSegment as a Swift enum since it needs to
support a bunch of different formats with varying levels of complexity. I’m
actually kinda curious how the Zwift team ended up representing these and if
they did something similar.&lt;/p&gt;

&lt;p&gt;From here it’s pretty simple to get stuff like the total duration or the target
wattage for a particular time offset, whether I need to calculate that (in the
case of a warmup or cooldown) or if I just return the constant value.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/718f50f48c32b324a2f25a80c78ae46b.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Aside from representing the workout segments, the actual workout object will
consist of things like the name, description, FTP and other information that
pertains to the state of the workout like time elapsed. I wrote a function that
takes the current time elapsed in the workout, loops through and finds the
corresponding segment and returns the desired wattage. There might be a more
efficient way to do that but for now it works.&lt;/p&gt;

&lt;p&gt;I also added some things to my project for displaying and choosing workouts from
a list, and actually displaying the target wattage alongside actual wattage. I
want to improve the interface now since it’s terrible, but I’ll save that for
another blog post. If you want to check out the project, &lt;a href=&quot;https://github.com/hungtruong/Zswift&quot;&gt;it’s all here on
Github&lt;/a&gt;, including the playgrounds I used
to test out the workout segment logic before adding it to the Xcode project.&lt;/p&gt;

&lt;p&gt;Next on my list of tasks is to make the workout interface a lot prettier/usable,
and to add heart rate information from my Apple Watch.&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwiftworkoutheader.png&quot; /&gt;
	&lt;figcaption&gt;A very colorful Zwift Workout&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Last time, on “Making an iOS Zwift Clone to Save $15 a Month” I wrote about
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;learning Core Bluetooth to connect to my exercise bike and get data streaming
directly to my
app&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since writing that article, I cleaned up the implementation of the Core
Bluetooth service a bit and started supporting some additional data like
distance, calories burned and cycling cadence.&lt;/p&gt;

&lt;p&gt;While cycling on my exercise bike and staring at these numbers is fun, the
built-in screen on my bike already shows these numbers, so I essentially
recreated a subset of the official
&lt;a href=&quot;https://www.concept2.com/service/software/ergdata&quot;&gt;ergData&lt;/a&gt; app so far.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/ergdata.jpg&quot; /&gt;
	&lt;figcaption&gt;The ergData app is functional but ugly af&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I realized the next challenge would be to start a guided workout in my app and
show the target wattage alongside my actual wattage on the bike.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 07 Apr 2019 00:00:00 -0700</pubDate>
        <link>http://www.hung-truong.com//blog/2019/04/07/making-a-zwift-clone-part-2/</link>
        <guid isPermaLink="true">http://www.hung-truong.com//blog/2019/04/07/making-a-zwift-clone-part-2/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Making an iOS Zwift Clone to Save $15 a Month! Part 1: Core Bluetooth</title>
        <description>&lt;p&gt;It’s been a while since I’ve worked on a personal project, but I’ve been having an itch to make some new iOS apps and yesterday morning I decided to go ahead and hack something together.&lt;/p&gt;

&lt;p&gt;I recently purchased an exercise bike called the &lt;a href=&quot;https://shop.concept2.com/bikeerg/601-bikeerg.html&quot;&gt;BikeErg&lt;/a&gt; (I think the name has something to do with the rowing machines that the manufacturer also makes). The bike has a built-in computer that keeps track of things like watts (apparently cycling is a sport that has really good analytics since it’s easy to track raw power), calories burned, cadence and other stuff. You can view the data on the monitor or use an app like &lt;a href=&quot;https://zwift.com&quot;&gt;Zwift&lt;/a&gt; to do workouts.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/bikeerg.jpg&quot; /&gt;
	&lt;figcaption&gt;The BikeErg comes with the PM5: the most advanced PM thing ever.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!--more--&gt;

&lt;p&gt;I’ve been using the BikeErg to exercise pretty regularly now, and I tried a bunch of different apps that can connect to it. Zwift is pretty much the gold standard as it has many features like 3D avatars and environments, a rich community, and lots of different workout plans for you to try. Zwift integrates with apps like MyFitnessPal and Strava, too, so I can trick people into thinking that I’ve ridden in Central Park one day and London the next.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwift.jpeg&quot; /&gt;
	&lt;figcaption&gt;Studies have shown that riding a bike in a completely white room really builds your FTP&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;While I think the feature set of Zwift is really compelling, I’m more of an old school app user. I don’t really care about the online community. I don’t really need to look at my avatar riding his bike around a futuristic city or an exploding volcano. I just want to do some directed workouts and maybe track my heart rate and my calories burned. The price of $15 a month is probably fine for people who use all of those features and get the value out of it, but I feel like I do not. &lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwift_screenshot.jpeg&quot; /&gt;
	&lt;figcaption&gt;It’s my virtual dude riding through a virtual New York with all his virtual pals&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Just to be clear here, I do think app developers deserve to be paid for their work and it’s definitely within reason for Zwift to charge this subscription given the sheer amount of support they need to provide to all of their users’ varying setups. After just implementing a small proof of concept, I have some mad respect for their dev team.&lt;/p&gt;

&lt;p&gt;However, I am cheap and I’m an iOS developer so I figured, “maybe I can roll my own fake Zwift!”&lt;/p&gt;

&lt;h3 id=&quot;enter-corebluetooth&quot;&gt;Enter CoreBluetooth&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/core_bluetooth.png&quot; /&gt;
	&lt;figcaption&gt;The more I stare at this image the less sense it makes&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I’ve been interested in Bluetooth development ever since CoreBluetooth was added to the iOS 5.0 SDK (I think the first supported device was the iPhone 4s). But every time I tried to sit down and read the documentation I got discouraged by the complexity and ended up getting distracted by some other new shiny API. Since I had a desired use case here: Make a Zwift alternative for myself, I was able to focus up some more and get something working.&lt;/p&gt;

&lt;p&gt;While the Bluetooth protocol is incredibly flexible, that flexibility also makes it incredibly complicated to get even a simple proof of concept working. If you don’t know what the special Bluetooth jargon means, it can seem really confusing. I still don’t really understand all of it but I’ve managed to hack something together that will serve as a basis for my fake Zwift app.&lt;/p&gt;

&lt;p&gt;Rather than bore you with the technical jargon and steps required to make this app, I’d rather just go through my process of figuring it out, which may be slightly more interesting.&lt;/p&gt;

&lt;h3 id=&quot;of-course-its-called-a-manager&quot;&gt;Of course it’s called a “Manager”&lt;/h3&gt;

&lt;p&gt;So the first thing I did was go to &lt;a href=&quot;https://developer.apple.com/library/archive/documentation/NetworkingInternetWeb/Conceptual/CoreBluetooth_concepts/AboutCoreBluetooth/Introduction.html#//apple_ref/doc/uid/TP40013257&quot;&gt;this document&lt;/a&gt; (which I guess is deprecated now but I didn’t notice that message when I was reading it) which goes over the Core Bluetooth framework.&lt;/p&gt;

&lt;p&gt;I found out that I needed to create a CBCentralManager, so I did that and then I tried to scan for some Bluetooth devices:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;let centralManager = CBCentralManager()&lt;br /&gt;
self.centralManager.scanForPeripherals(withServices: nil, options: nil)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I immediately got an error that I couldn’t do that since the centralManager wasn’t powered on yet. Oops! I then set the delegate of the centralManager and waited for the method “centralManagerDidUpdateState” to check that it was powered on before scanning.&lt;/p&gt;

&lt;p&gt;I soon started getting a bunch of peripherals in my next delegate method, “centralManager(_:didDiscover:advertisementData:rssi:)”&lt;/p&gt;

&lt;p&gt;Among the things I found were my laptop (over and over again even though the scan was set to not allow duplicates…), someone’s Bluetooth headset and various other things I couldn’t identify. Success! &lt;/p&gt;

&lt;p&gt;Once I filtered out the peripherals that kept on repeating, I was able to turn on the bike (by cycling a bit) and I got this message in my logs:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/peripherals.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;I successfully found my PM5. Now to connect to it and get the data. I ended up connecting to the PM5 based on the name. (After doing some reading it looks like I could connect based on the last service UUID of “CE060000-43E5-11E4-916C-0800200C9A66”). &lt;/p&gt;

&lt;p&gt;I called the “connect” function of the centralManager and later got an error because the peripheral wasn’t retained (I guess the Central doesn’t keep a strong reference, which makes sense). I tried again, this time keeping a reference to the peripheral in an array.&lt;/p&gt;

&lt;h3 id=&quot;peripherals-services-and-characteristics&quot;&gt;Peripherals, Services and Characteristics&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/central_manager.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Once I connected, I had to discover the peripheral’s services. And once that succeeded I had to discover each service’s characteristics. Once you discover those characteristics you can set the peripheral’s services’ characteristics to “notify” you when the characteristic changes. In more depth:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Connect to peripheral using the central’s “connect(_:options:)” method and retain it&lt;/li&gt;
  &lt;li&gt;Handle the “&lt;em&gt;centralManager(_:didConnect:)&lt;/em&gt;” delegate method where you set the peripheral’s delegate and call its “discoverServices(_:)” method&lt;/li&gt;
  &lt;li&gt;Handle the “peripheral(_:didDiscoverServices:)” delegate method and call the peripheral’s “discoverCharacteristics(_:for:)” for each service you want to discover characteristics for (why not all of them at this point?)&lt;/li&gt;
  &lt;li&gt;Handle the “peripheral(_:didDiscoverCharacteristicsFor:error:)” delegate method for each service’s characteristics you wanted to discover by calling the peripheral’s “setNotifyValue(_:for:)” method on each service’s characteristic that you want notifications for.&lt;/li&gt;
  &lt;li&gt;Optionally handle the “peripheral(_:didUpdateNotificationStateFor:error:)” method to see if you were able to successfully update the notification state for each peripheral’s service’s characteristic. In some cases I wasn’t able to ask for updates, perhaps those characteristics are just static data?&lt;/li&gt;
  &lt;li&gt;Handle the “peripheral(_:didUpdateValueFor:error:)” method to get the updated value for each characteristic that you wanted notifications for.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This all seems really convoluted to me and it was probably part of the reason that I always gave up on implementing Bluetooth in the past, but I think that’s more of a symptom of the complexity of the Bluetooth protocol than the CoreBluetooth API.&lt;/p&gt;

&lt;p&gt;Now all I needed to do was generate some data by cycling on the bike for a few seconds. I wasn’t quite finished yet, though. When the characteristics are updated and you start getting notified, you can inspect the new values, but those values are just &lt;a href=&quot;https://developer.apple.com/documentation/corebluetooth/cbcharacteristic/1518878-value&quot;&gt;Data objects&lt;/a&gt;. Each characteristic can hold a number of values based on how the data is structured, and that is up to whoever is implementing the Bluetooth protocol. &lt;/p&gt;

&lt;p&gt;I did some research and found &lt;a href=&quot;https://www.concept2.com/files/pdf/us/monitors/PM5_BluetoothSmartInterfaceDefinition.pdf&quot;&gt;this document&lt;/a&gt; that describes the Bluetooth specifications for the PM5 device.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/attribute_table.png&quot; /&gt;
	&lt;figcaption&gt;Just some really interesting light reading&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In that document were some tables including the one above which describes the UUID for a characteristic that includes things like elapsed time, calories, and most importantly, watts. I discovered that the data was being encoded into bytes, so I took the raw Data object and split it into an array of 8-bit Integers. Once I started printing those arrays I saw something like this:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/characteristics.png&quot; /&gt;
	&lt;figcaption&gt;I originally printed out the Base 64 string representation of the Data before reading the doc, which was a lot less useful&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Because the PM5 was originally set up for rowing machines, the documentation is a bit confusing. It refers to “strokes” which might line up with rpms on a bike? I was mainly interested in watts for my proof of concept so I found a few values in the document that mentioned watts. The table in the spec mentions “Stroke Power Lo (watts)” and has a “Stroke Power Hi” (what’s the difference?). I cobbled an interface together to test out my guess about the first value and here’s the video result:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/bikeerg_gif.gif&quot; /&gt;
	&lt;figcaption&gt;I took this video with an iPad and for once I’m not ashamed of that. Also it’s shaky cause I was cycling.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Success! I’m now able to connect my phone to my bike with my app. I have only gotten the wattage data from the bike so far, but reading through the spec it seems like there is a lot more I can pull via Bluetooth. I already know from using Zwift that I can get cadence from the bike, for example, and I saw a few other interesting things like calories, pace and distance traveled.&lt;/p&gt;

&lt;h3 id=&quot;every-journey-begins-with-a-single-corebluetooth-implementation&quot;&gt;Every Journey Begins With a Single CoreBluetooth Implementation&lt;/h3&gt;

&lt;p&gt;I titled this blog post “Part 1” in a series but I don’t know when the next step will be. My wishlist is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I want to eventually set up directed workouts in a similar fashion as Zwift&lt;/li&gt;
  &lt;li&gt;I also want to be able to track my heart rate which I can do by writing an Apple Watch app for my existing app&lt;/li&gt;
  &lt;li&gt;I want to be able to store my workout data and integrate with Apple Health&lt;/li&gt;
  &lt;li&gt;I want to import workouts or at least create them inside of the app&lt;/li&gt;
  &lt;li&gt;I want to chart the actual wattage of the bike against the guided wattage, and also show heart rate, and show histograms&lt;/li&gt;
  &lt;li&gt;I want to avoid feature creep&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I haven’t figured out which order to do these things in but for now I’ll continue to use Zwift since I already paid for the membership. My next step is probably to break out the code for connecting to the PM5 into its own project and make all of the data from it available in an easy to consume form. I’m kinda torn between that and just making the MVP for doing workouts.&lt;/p&gt;

&lt;p&gt;If I had to estimate, I probably spent more than 3 hours working on this project so far and more on writing this blog post. If I was to value my time based on what my contracting rate would be I’d probably be able to pay for more than a year of Zwift with it! So this project is really more about learning different iOS technologies than it is about saving money at this point.&lt;/p&gt;

&lt;p&gt;If you found this blog post interesting let me know! I wanted to write down my process so I could remember it, but hopefully it’s useful to anyone trying to implement CoreBluetooth. I found a bunch of sample code that connects to heart rate monitors but I didn’t find any that go through the process of writing code to a spec document. If you want to try to run this app yourself (and you happen to have the same exact bike as me), &lt;a href=&quot;https://github.com/hungtruong/Zswift/tree/blog_part_1&quot;&gt;check out the source code here&lt;/a&gt;.&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;It’s been a while since I’ve worked on a personal project, but I’ve been having an itch to make some new iOS apps and yesterday morning I decided to go ahead and hack something together.&lt;/p&gt;

&lt;p&gt;I recently purchased an exercise bike called the &lt;a href=&quot;https://shop.concept2.com/bikeerg/601-bikeerg.html&quot;&gt;BikeErg&lt;/a&gt; (I think the name has something to do with the rowing machines that the manufacturer also makes). The bike has a built-in computer that keeps track of things like watts (apparently cycling is a sport that has really good analytics since it’s easy to track raw power), calories burned, cadence and other stuff. You can view the data on the monitor or use an app like &lt;a href=&quot;https://zwift.com&quot;&gt;Zwift&lt;/a&gt; to do workouts.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/bikeerg.jpg&quot; /&gt;
	&lt;figcaption&gt;The BikeErg comes with the PM5: the most advanced PM thing ever.&lt;/figcaption&gt;
&lt;/figure&gt;

</description>
        
        <pubDate>Sun, 17 Mar 2019 00:00:00 -0700</pubDate>
        <link>http://www.hung-truong.com//blog/2019/03/17/making-a-zwift-clone-part-1/</link>
        <guid isPermaLink="true">http://www.hung-truong.com//blog/2019/03/17/making-a-zwift-clone-part-1/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Five Simple Steps to Becoming an International™ Conference Speaker!</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2018/ioscon-london.jpg&quot; /&gt;
	&lt;figcaption&gt;Me speaking at iOSCon London 2018. Image copyright &lt;a href=&quot;http://www.edtelling.com&quot;&gt;Ed
Telling&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;A few years ago, I made a goal for myself that I would start speaking at
conferences. In the past I had gone to quite a few awesome conferences and I
really admired the folks who could get on a stage, share a bunch of knowledge
and inspire an audience to do cool stuff. While I had a bit of experience with
public speaking (I spoke at some local iOS/Android meetups and moderated two
panels at SXSW back in ’11 and ’13), I wanted to try for a “real” conference
spot.&lt;/p&gt;

&lt;p&gt;I think the conference that convinced me to start applying was Swift Summit,
which I attended in 2015. There were a number of really inspiring talks, and the
community felt really fresh and welcoming. One interesting aside: I learned
after joining Lyft that no fewer than three of my current coworkers on iOS spoke
at that conference (&lt;a href=&quot;https://www.youtube.com/watch?v=FmT3A_tm9_4&quot;&gt;Keith&lt;/a&gt;,
&lt;a href=&quot;https://www.youtube.com/watch?v=P0HD5bODhZM&quot;&gt;Sam&lt;/a&gt; and
&lt;a href=&quot;https://www.youtube.com/watch?v=kz6d-ES7WtE&quot;&gt;JP&lt;/a&gt;, though only Keith was working
at Lyft at the time) and a few others attended that year. I’m not sure if that’s
correlation or causation at work.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;step-1-applying-to-speak&quot;&gt;Step 1: Applying to speak&lt;/h3&gt;

&lt;p&gt;According to my Google Doc which keeps track of my CFP applications, I started
applying heavily in 2016. There was a gap between when I wanted to apply and
when I started to apply simply because I felt I didn’t have much to talk about.
Once I had some interesting experience with iMessage and accessibility while
working at Starbucks, I figured people might want to know about those things,
too. In retrospect I think I probably waited too long. If you are passionate
about something, don’t wait until you feel you’re an expert on the subject. That
day probably (at least for me) will never come, and you can always do research
about your topic once it’s been accepted!&lt;/p&gt;

&lt;p&gt;In all of my CFPs I use a template which I learned about during my time in the
&lt;a href=&quot;https://www.twilio.com/blog/2014/01/introducing-the-twilio-heroes-nt.html&quot;&gt;Twilio
Heroes&lt;/a&gt;
program. Essentially, you need to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;State a problem or challenge&lt;/li&gt;
  &lt;li&gt;Give some general solutions (set up a straw man)&lt;/li&gt;
  &lt;li&gt;Give your specific solution that is better than #2, because you are awesome.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here’s an example of my iMessage talk, which used that template:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1) Messaging apps are huge and getting huger. With WhatsApp, WeChat and
Facebook Messenger users in the millions or billions, it’s clear that messaging
is not a fad. So how can a developer take advantage of this growth? (2) Starting
a new messaging app is probably out of the question. (3) Thankfully, Apple has
introduced iMessage app extensions in iOS 10. Learn how to use iMessage to let
your users send rich, interactive messages, and learn how to take advantage of
the network effects that iMessage app extensions allow, including updates
introduced in iOS 11 like Live Message Layouts and the Direct Send API.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;step-2-getting-rejected&quot;&gt;Step 2: Getting rejected&lt;/h3&gt;

&lt;p&gt;After using this template for talks and applying to pretty much every iOS
related conference in 2016, I ended up getting a ton of rejections. This is
totally fine and to be expected since I think most conferences get way more talk
proposals than they have spots. I also noticed that conferences tend to invite
speakers who have spoken before, which is a form of risk management but also
kind of unfair for people who have never spoken.&lt;/p&gt;

&lt;p&gt;The best way to beat this sort of chicken and egg situation is probably to aim
at local meetups which are always looking for people to give talks! Another
benefit of this is that you get to practice your talk and refine it before
taking it to a bigger stage and audience (which is exactly what I did for my
accessibility talk that I gave at iOSCon in London this year). Because I had
given my talk at an Xcoders meeting two weeks before the conference, I was able
to time my talk, practice it in front of people, get their questions to
incorporate back into the talk, and feel like I was ready once I spoke in front
of a larger crowd.&lt;/p&gt;

&lt;h3 id=&quot;step-3-getting-accepted&quot;&gt;Step 3: Getting accepted&lt;/h3&gt;

&lt;p&gt;The first year I really started getting serious about applying to conferences,
my acceptance rate was 1 of 7&lt;a href=&quot;#footnote1&quot;&gt;¹&lt;/a&gt;. Success! I attended the first
&lt;a href=&quot;http://www.codemobile.co.uk&quot;&gt;CodeMobile&lt;/a&gt; conference in Chester, UK and talked
about iMessage apps. The experience of going on stage, sharing something I
thought was interesting, and then getting feedback that it actually was
interesting to people was great! The attendees were all amazing and I finally
got to travel outside of North America for the first time in my life. I had
reached my goals and become an International™ Speaker as well as generally
becoming an iOS Conference Speaker all in one go.&lt;/p&gt;

&lt;h3 id=&quot;step-4-preparation&quot;&gt;Step 4: Preparation&lt;/h3&gt;

&lt;p&gt;The thing that most people don’t tell you, and the thing that you quickly find
out when preparing a conference talk is how much work it is to prepare! As an
audience member to some great talks, I always assumed that the talk took maybe a
handful of hours to prepare and format.&lt;/p&gt;

&lt;p&gt;In practice, a talk can take something like 40–50 hours for a 45 minute session.
That includes coming up with the proposal, learning about the subject (often
this happens after the proposal has been accepted!), coming up with a format
that will educate and inspire (and hopefully entertain) your audience, and
practicing the delivery of the material. You probably also want to tailor your
presentation for the audience you’re speaking to; are they mostly students? What
skill level would be most appropriate to present to? What kinds of jokes might
work well with your audience?&lt;/p&gt;

&lt;p&gt;A good amount of the time that I put into preparing talks is practicing. It’s
really up to you how polished you want the talk to be, and how comfortable you
will be presenting with the amount of practice you’ve put into it. As I
mentioned before, presenting early to a smaller group can really help by
creating a deadline that is sooner than the actual talk and providing a method
of gathering feedback to tweak your talk to make it even better.&lt;/p&gt;

&lt;p&gt;One thing that’s great about having a conference talk lined up is that you can
do some pretty productive stuff while procrastinating. In order to avoid
preparing my iOSCon talk, I applied to speak at a conference in New York called
!!Con. That application ended up getting accepted, so I started down a pretty
vicious cycle of speaking at more conferences!&lt;/p&gt;

&lt;p&gt;I recently came across this Twitter thread of many speakers sharing their
process and how much time they put into their talks:&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Hey, people who talk at things:&lt;br /&gt;&lt;br /&gt;How long does it take you to put a new talk together? I need like 50 hours over at least a couple of months to make something I don&amp;#39;t hate. I&amp;#39;m trying to get that down (maybe by not doing pictures?) but wondering what&amp;#39;s normal for everyone else.&lt;/p&gt;&amp;mdash; Tanya Reilly (@whereistanya) &lt;a href=&quot;https://twitter.com/whereistanya/status/995653828933496832?ref_src=twsrc%5Etfw&quot;&gt;May 13, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/center&gt;

&lt;h3 id=&quot;step-5-actually-presenting&quot;&gt;Step 5: Actually presenting&lt;/h3&gt;

&lt;p&gt;Giving the actual talk is kind of a weird experience. After putting a ton of
time into a talk upfront, I’ve found that the actual talk goes by faster than I
think it will. Once I get up and start talking, I’m more or less on autopilot,
and it’s almost like having an out of body experience. It might be because I put
all of my brain power into giving the talk, but I can’t even really remember how
I did once it’s done. Then at the end everyone claps and hopefully people ask
insightful questions and no one asks a question that’s really just a statement
that they’re trying to use to prove that they know something.&lt;/p&gt;

&lt;p&gt;Once I’ve given the talk, I can actually enjoy the conference. I’ve been lucky
that the first few conferences I’ve spoken at, I was the first speaker after the
keynote speaker on the first day. Going earlier is also nice because then people
have seen you talk and they’re usually more open about chatting with you in the
hallway.&lt;/p&gt;

&lt;h3 id=&quot;presenting-some-more&quot;&gt;Presenting some more&lt;/h3&gt;

&lt;p&gt;After speaking at &lt;a href=&quot;https://youtu.be/zXQOTEwxv-U&quot;&gt;CodeMobile&lt;/a&gt; in Chester, UK, I
didn’t get any talks accepted for about a year. I ended up speaking again in the
UK, this time in London for
&lt;a href=&quot;https://skillsmatter.com/skillscasts/11320-designing-and-building-with-accessibility-in-mind&quot;&gt;iOSCon&lt;/a&gt;
in March of 2018. Then I most recently spoke at &lt;a href=&quot;http://bangbangcon.com&quot;&gt;!!Con&lt;/a&gt;
in New York in May. I’ll be giving an updated version of my iOS accessibility
talk in Boston next month for SwiftFest.&lt;/p&gt;

&lt;p&gt;This time around my acceptance rate was 3/6, which is much better. I would
mostly chalk this up to the fact that I gave a talk at CodeMobile last year. I
also applied to some more diverse types of tech conferences since there were
fewer iOS specific ones (I think a few that I applied to didn’t actually
happen).&lt;/p&gt;

&lt;p&gt;In the future, I hope I can speak again but with the three consecutive talks I’m
doing in March/May/June this year, I’m ready for a little break before I get
back on the conference speaking circuit. I’m really happy that I’ve been given
the opportunity to share some of my experiences and the conferences I’ve been to
have been really enjoyable so far.&lt;/p&gt;

&lt;h3 id=&quot;you-should-talk-too&quot;&gt;You should talk, too!&lt;/h3&gt;

&lt;p&gt;When I first set my personal goal to talk at some conferences, I really wasn’t
sure if I had what it would take to succeed. I don’t consider myself a great
public speaker, nor would I consider myself to be an exceptional software
engineer. The secret is that neither is really a requirement to share cool stuff
with people. You just need to have some passion for something and effectively
communicate that passion by telling your story.&lt;/p&gt;

&lt;p&gt;If you’re looking for iOS conferences to apply to, there’s a well maintained
list along with CFP deadlines
&lt;a href=&quot;https://github.com/Lascorbe/CocoaConferences&quot;&gt;here&lt;/a&gt;. Good luck!&lt;/p&gt;

&lt;p&gt;Finally, if you see me at a conference, say hi! I’m always happy to chat with
people, even if I look like I’m busy or focused on other stuff.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;footnote1&quot;&gt;&lt;/span&gt;¹Actually I ended up getting another talk accepted but the conference was not
interested in reimbursing me for any travel expenses so I politely declined.
Given the amount of time it takes to prepare a good presentation, a conference
should absolutely reimburse you for speaking.&lt;/p&gt;

</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2018/ioscon-london.jpg&quot; /&gt;
	&lt;figcaption&gt;Me speaking at iOSCon London 2018. Image copyright &lt;a href=&quot;http://www.edtelling.com&quot;&gt;Ed
Telling&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;A few years ago, I made a goal for myself that I would start speaking at
conferences. In the past I had gone to quite a few awesome conferences and I
really admired the folks who could get on a stage, share a bunch of knowledge
and inspire an audience to do cool stuff. While I had a bit of experience with
public speaking (I spoke at some local iOS/Android meetups and moderated two
panels at SXSW back in ’11 and ’13), I wanted to try for a “real” conference
spot.&lt;/p&gt;

&lt;p&gt;I think the conference that convinced me to start applying was Swift Summit,
which I attended in 2015. There were a number of really inspiring talks, and the
community felt really fresh and welcoming. One interesting aside: I learned
after joining Lyft that no fewer than three of my current coworkers on iOS spoke
at that conference (&lt;a href=&quot;https://www.youtube.com/watch?v=FmT3A_tm9_4&quot;&gt;Keith&lt;/a&gt;,
&lt;a href=&quot;https://www.youtube.com/watch?v=P0HD5bODhZM&quot;&gt;Sam&lt;/a&gt; and
&lt;a href=&quot;https://www.youtube.com/watch?v=kz6d-ES7WtE&quot;&gt;JP&lt;/a&gt;, though only Keith was working
at Lyft at the time) and a few others attended that year. I’m not sure if that’s
correlation or causation at work.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 20 May 2018 00:00:00 -0700</pubDate>
        <link>http://www.hung-truong.com//blog/2018/05/20/five-simple-steps-to-becoming-an-international-conference-speaker/</link>
        <guid isPermaLink="true">http://www.hung-truong.com//blog/2018/05/20/five-simple-steps-to-becoming-an-international-conference-speaker/</guid>
        
        
        <category>Public Speaking</category>
        
        <category>Conferences</category>
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Updating My Blog and Killing Wordpress!</title>
        <description>&lt;figure&gt;
&lt;img src=&quot;/blog/static/hungs-old-blog.png&quot; /&gt;
&lt;figcaption&gt;R.I.P. old blog!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;A week or two ago I got an email from Google telling me that my blog had once again been hacked. I’ve written before about getting hacked and I basically knew how to unhack myself, but I put it off for a bit until I checked my website again and noticed that Chrome was warning me to avoid it! This was pretty much the last straw. I decided to move off of self-hosted Wordpress and switch to something a bit more secure.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;I looked at some options such as &lt;a href=&quot;https://ghost.org/&quot;&gt;Ghost&lt;/a&gt; or &lt;a href=&quot;https://gohugo.io/&quot;&gt;Hugo&lt;/a&gt; but it seemed like &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; was the most mature static blog platform, so I went with that. I used a plugin that would generate the Markdown files that I’d need to regenerate my blog. It also ended up transferring my image files, etc, which made it pretty easy to get started.&lt;/p&gt;

&lt;p&gt;I found a cool looking theme called &lt;a href=&quot;http://brianmaierjr.com/long-haul/&quot;&gt;Long Haul&lt;/a&gt; that I ended up modifying quite a bit. I updated the header image, changed up some colors and added an archives page and support for post excepts. What’s currently missing is a site search and category/tag support, but I can live with that. Comments are also gone but you should never read the comments anyway so that’s fine. I’m happy with the speed that the site loads as it’s all static and hosted on Github pages.&lt;/p&gt;

&lt;p&gt;I wouldn’t necessarily recommend Jekyll to everyone who wants to convert their blog. It’s clearly written by developers for developers as it’s insanely un-user friendly. Don’t get me wrong, the documentation is great, but it’s surprising to me that something resembling a blogging platform has been around this long without anyone setting up a WYSIWYG type of editor like the kind that Wordpress has. I ran into quite a few issues and a pretty steep learning curve while converting my blog. Now that I’m past that, hopefully I can start blogging more often again!&lt;/p&gt;

&lt;p&gt;(I checked the hilariously outdated “friends” links on my blog and most of them died around 2009 or the domain names ended up failing or being transferred to new owners, so I guess I’m one of the few remaining bloggers of the world)&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
&lt;img src=&quot;/blog/static/hungs-old-blog.png&quot; /&gt;
&lt;figcaption&gt;R.I.P. old blog!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;A week or two ago I got an email from Google telling me that my blog had once again been hacked. I’ve written before about getting hacked and I basically knew how to unhack myself, but I put it off for a bit until I checked my website again and noticed that Chrome was warning me to avoid it! This was pretty much the last straw. I decided to move off of self-hosted Wordpress and switch to something a bit more secure.&lt;/p&gt;

</description>
        
        <pubDate>Sat, 07 Apr 2018 00:00:00 -0700</pubDate>
        <link>http://www.hung-truong.com//blog/2018/04/07/updating-my-blog-and-killing-wordpress/</link>
        <guid isPermaLink="true">http://www.hung-truong.com//blog/2018/04/07/updating-my-blog-and-killing-wordpress/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>Introducing DeadRinger: An iPhone X Lockscreen Replica</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/static/deadringer_demo.gif&quot; alt=&quot;Deadringer Demo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I got an idea the other day when I was looking at my co-worker’s iPhone X sitting on a desk. I thought that I could have mistaken it for my own phone, since iPhone X comes in two colors and you have a 50% chance of having the same phone color as any other person. I was thinking that if I did mistake it as my phone and entered my passcode, I’d basically be transmitting the key to all of my personal data.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;That led me to think about how hard it would be to reproduce the iPhone X’s lock screen and spoof one of my own. It turns out that with some experience with iOS development, it’s not that hard. Thanks to the iPhone X’s Super Retina display, a screen full of black pixels is indistinguishable from a device that’s asleep. And thanks to the omission of the home button, and a public API for disabling the swipe to go home gesture, I was able to make a pretty convincing copy of the iPhone X’s lock screen. Your mileage may vary if you’re trying to spoof other iPhone X models.
Oh also I just decided to call it “DeadRinger” because I was inspired by security exploits like &lt;a href=&quot;http://heartbleed.com/&quot;&gt;HeartBleed&lt;/a&gt; that basically have their own marketing departments. I wanted my app to have a slightly scary and trendy name as well.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/blog/static/deadringer_side_by_side.gif&quot; /&gt;
&lt;figcaption&gt;On the left is the app, on the right is the real lock screen.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I uploaded the source code to a &lt;a href=&quot;https://github.com/hungtruong/DeadRinger&quot;&gt;Github repo&lt;/a&gt; which has a pretty good README file but I’ll go into a bit more detail about some interesting code implementation for the rest of this post. If you want to watch the full demo video, you can watch it below:&lt;/p&gt;
&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/0fR6mmrMFyQ&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;h4 id=&quot;usage&quot;&gt;Usage&lt;/h4&gt;
&lt;p&gt;I’m not suggesting you actually use this app but here’s the theoretical attack vector. You would load this app on to a phone that you supply (you’d have to be okay with losing it). The only change you would probably need to make is to the wallpaper image which should match the target’s phone. Swap your phone with the target’s phone and then wait for them to try to unlock their device. If the target has a case I guess you’d want to get the same one or swap it when you swap phones. You could theoretically write some networking code to transmit the entered passcode and then use it to unlock the real phone which you have in your possession.&lt;/p&gt;

&lt;p&gt;The app works by entering the inactive state, which just shows a black screen and mimics the phone being asleep. Upon raising to wake the phone or tapping on the device, the lock screen will show up, then the passcode entry view will appear shortly afterwards. The user can cancel back to the lock screen and if they want they can tap to get back to the passcode entry mode. The app isn’t sophisticated enough to handle things like pressing the physical lock button but that’s considered outside the scope of this proof of concept. The app does intercept swipes up from the bottom which would normally take the user back to the home screen.&lt;/p&gt;

&lt;h4 id=&quot;implementation&quot;&gt;Implementation&lt;/h4&gt;
&lt;p&gt;Probably the most interesting part of the project is disabling the swipe to go home gesture. You can find documentation about the iPhone X methods for deferring system gestures &lt;a href=&quot;https://developer.apple.com/documentation/uikit/uiviewcontroller?language=objc#2887625&quot;&gt;here&lt;/a&gt;. Here’s my implementation below, found in a UIViewController.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/31604eff878378e75f12d5520e8e313d.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;This prevents an initial gesture but if the user tries again then they’ll go to the home screen and your app will be defeated. Your target will also probably be really confused. There’s also code in the app that handles the visibility of the home indicator, since you want it hidden on the “inactive” screen but visible on lock screen.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/1eeea58893ea005a92d49d65e7b04270.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;With the home indicator logic out of the way, there are a few other tasks like making sure the phone doesn’t really go to sleep. This has been possible since iOS 2. The line of code to accomplish that is&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;UIApplication.shared.isIdleTimerDisabled = true &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To recreate the tilt to wake feature, I use a CMMotionManager to sample the device’s pitch. I just guessed that 0.5 would be a good value but it could probably be tweaked a bit. If the device is put back down, it will go to the inactive state, but not while a user is entering the passcode. The real device also seems to wake on a delay so I used a Timer to handle that state change.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/3e51567235cff2c08c345cb933a954db.js&quot;&gt;&lt;/script&gt;

&lt;h4 id=&quot;todo&quot;&gt;TODO&lt;/h4&gt;
&lt;p&gt;I’d say my recreation of the lock screen is just about good enough to convince most people that they’re using the real thing, but there are a few nitpicks and details that I didn’t get to in the interest of time (since I’m not actually going to use this app to steal anyone’s passcode).&lt;/p&gt;

&lt;p&gt;The camera and flashlight buttons on the lock screen have a cool 3D Touch effect that requires you to push with force to get them to fire. I didn’t implement these because I don’t want my target to take a photo, I want their passcode. The flashlight one would actually be simple enough to implement though.&lt;/p&gt;

&lt;p&gt;iOS 11.2 has a special control center indicator on the lock screen that I haven’t implemented yet. I would just need to do a check on the OS version and add the view if I’m on iOS 11.2 or higher.&lt;/p&gt;

&lt;p&gt;There are some cool transition animations in the keypad when going into the passcode entry mode in the real view that I haven’t recreated yet. The buttons animate on a slight delay from the bottom up and have a slight zoom animation in. The average person is not going to notice the difference, though.&lt;/p&gt;

&lt;p&gt;There isn’t a way to get the real notification center items from a user’s device and put them on another device so I’m not going there. For some people I guess the lack of a billion notifications would signal that something is fishy.&lt;/p&gt;

&lt;h4 id=&quot;possible-solutions&quot;&gt;Possible Solutions&lt;/h4&gt;
&lt;p&gt;I think there are a few ways to solve this “vulnerability” (which I put in quotes because it doesn’t seem too terrible to me). One solution would be to include a “personal security image” like banks do on their login screens. It’s also pretty easy to defeat, though, if you know the target’s image. Plus it’s probably too tacky for Apple to implement.&lt;/p&gt;

&lt;p&gt;Another solution would be to add a hardware feature like an LED that lights up when you’re entering something into an OS control. There’s precedent for this like how the MacBook camera causes an LED to light up when it’s on. This would also mitigate some &lt;a href=&quot;https://krausefx.com/blog/ios-privacy-stealpassword-easily-get-the-users-apple-id-password-just-by-asking&quot;&gt;other security issues&lt;/a&gt; that have popped up. As long as everything is replicable in software there’s always a way to spoof it.&lt;/p&gt;

&lt;p&gt;As a user you could also immediately cover your phone in stickers or something easily identifiable. Or keep your phone really dirty. Or find some other way to &lt;a href=&quot;https://www.cnet.com/news/apple-iphone-x-drop-test/&quot;&gt;adorn your phone in a unique way&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As our phones become the center of our universe, it makes a lot of sense to treat security as a top priority. Hopefully Apple can do something to make our lock screens more secure from spoofing!&lt;/p&gt;

&lt;h4 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h4&gt;
&lt;p&gt;Please don’t use this app to steal someone’s passcode. That’s not cool.&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;I made this app solely as a proof of concept, and I had a lot of fun figuring out the solutions to some technical problems as outlined above. It was also fun trying to get the design of the app as close to the real lock screen as possible. I learned quite a bit about supporting some new iPhone X features and hopefully you’ve learned something from this article as well! Check out my &lt;a href=&quot;https://github.com/hungtruong/DeadRinger&quot;&gt;Github repo&lt;/a&gt; for more details and feel free to contribute if you’re able to tackle any of the TODOs listed above.&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/static/deadringer_demo.gif&quot; alt=&quot;Deadringer Demo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I got an idea the other day when I was looking at my co-worker’s iPhone X sitting on a desk. I thought that I could have mistaken it for my own phone, since iPhone X comes in two colors and you have a 50% chance of having the same phone color as any other person. I was thinking that if I did mistake it as my phone and entered my passcode, I’d basically be transmitting the key to all of my personal data.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 03 Dec 2017 00:00:00 -0800</pubDate>
        <link>http://www.hung-truong.com//blog/2017/12/03/introducing-deadringer-an-iPhone-X-lockscreen-replica/</link>
        <guid isPermaLink="true">http://www.hung-truong.com//blog/2017/12/03/introducing-deadringer-an-iPhone-X-lockscreen-replica/</guid>
        
        
        <category>iPhone</category>
        
        <category>Apple</category>
        
        <category>Iphone X</category>
        
        <category>Security</category>
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>2016: A Hung Truong Year in Review</title>
        <description>&lt;p&gt;It’s been a while since I wrote a year-in-review on this blog, let alone any blog post. 2016 was a big year though, so here’s some stuff I did.&lt;/p&gt;

&lt;p&gt;In January Emily and I moved from Ann Arbor, Michigan where we had been living for a few years (Emily since grad school and me since the last time I moved back from Chicago in like 2012) to Seattle, Washington. I’ve loved Seattle ever since I moved here in 2009 to go work for &lt;a href=&quot;http://www.hung-truong.com/blog/2008/11/30/trip-to-seattle-program-manager-interview-with-microsoft/&quot;&gt;Microsoft&lt;/a&gt;, and it’s great to be back! I’m working for a coffee company in Seattle called Starbucks (yes, I get free coffee). I get to work on their iOS app and make latte art in my spare time. Here are some highlights:
&lt;!--more--&gt;&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&quot;instagram-media&quot; data-instgrm-captioned=&quot;&quot; data-instgrm-permalink=&quot;https://www.instagram.com/p/BOnIvQQgM-0/&quot; data-instgrm-version=&quot;8&quot; style=&quot; background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:658px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);&quot;&gt;&lt;div style=&quot;padding:8px;&quot;&gt; &lt;div style=&quot; background:#F8F8F8; line-height:0; margin-top:40px; padding:50.0% 0; text-align:center; width:100%;&quot;&gt; &lt;div style=&quot; background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAMUExURczMzPf399fX1+bm5mzY9AMAAADiSURBVDjLvZXbEsMgCES5/P8/t9FuRVCRmU73JWlzosgSIIZURCjo/ad+EQJJB4Hv8BFt+IDpQoCx1wjOSBFhh2XssxEIYn3ulI/6MNReE07UIWJEv8UEOWDS88LY97kqyTliJKKtuYBbruAyVh5wOHiXmpi5we58Ek028czwyuQdLKPG1Bkb4NnM+VeAnfHqn1k4+GPT6uGQcvu2h2OVuIf/gWUFyy8OWEpdyZSa3aVCqpVoVvzZZ2VTnn2wU8qzVjDDetO90GSy9mVLqtgYSy231MxrY6I2gGqjrTY0L8fxCxfCBbhWrsYYAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;p style=&quot; margin:8px 0 0 0; padding:0 4px;&quot;&gt; &lt;a href=&quot;https://www.instagram.com/p/BOnIvQQgM-0/&quot; style=&quot; color:#000; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none; word-wrap:break-word;&quot; target=&quot;_blank&quot;&gt;Latte art: Betta Fish (using almond milk)&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot; color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;&quot;&gt;A post shared by &lt;a href=&quot;https://www.instagram.com/hungtruong/&quot; style=&quot; color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px;&quot; target=&quot;_blank&quot;&gt; Hung Truong&lt;/a&gt; (@hungtruong) on &lt;time style=&quot; font-family:Arial,sans-serif; font-size:14px; line-height:17px;&quot; datetime=&quot;2016-12-29T18:42:30+00:00&quot;&gt;Dec 29, 2016 at 10:42am PST&lt;/time&gt;&lt;/p&gt;&lt;/div&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; defer=&quot;&quot; src=&quot;//www.instagram.com/embed.js&quot;&gt;&lt;/script&gt;



&lt;blockquote class=&quot;instagram-media&quot; data-instgrm-captioned=&quot;&quot; data-instgrm-permalink=&quot;https://www.instagram.com/p/BM9h2KEA_-h/&quot; data-instgrm-version=&quot;8&quot; style=&quot; background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:658px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);&quot;&gt;&lt;div style=&quot;padding:8px;&quot;&gt; &lt;div style=&quot; background:#F8F8F8; line-height:0; margin-top:40px; padding:50.0% 0; text-align:center; width:100%;&quot;&gt; &lt;div style=&quot; background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAMUExURczMzPf399fX1+bm5mzY9AMAAADiSURBVDjLvZXbEsMgCES5/P8/t9FuRVCRmU73JWlzosgSIIZURCjo/ad+EQJJB4Hv8BFt+IDpQoCx1wjOSBFhh2XssxEIYn3ulI/6MNReE07UIWJEv8UEOWDS88LY97kqyTliJKKtuYBbruAyVh5wOHiXmpi5we58Ek028czwyuQdLKPG1Bkb4NnM+VeAnfHqn1k4+GPT6uGQcvu2h2OVuIf/gWUFyy8OWEpdyZSa3aVCqpVoVvzZZ2VTnn2wU8qzVjDDetO90GSy9mVLqtgYSy231MxrY6I2gGqjrTY0L8fxCxfCBbhWrsYYAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;p style=&quot; margin:8px 0 0 0; padding:0 4px;&quot;&gt; &lt;a href=&quot;https://www.instagram.com/p/BM9h2KEA_-h/&quot; style=&quot; color:#000; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none; word-wrap:break-word;&quot; target=&quot;_blank&quot;&gt;Made a pumpkin latte art #latteart #meta #pumpkinlatte&lt;/a&gt;&lt;/p&gt; &lt;p style=&quot; color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;&quot;&gt;A post shared by &lt;a href=&quot;https://www.instagram.com/hungtruong/&quot; style=&quot; color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px;&quot; target=&quot;_blank&quot;&gt; Hung Truong&lt;/a&gt; (@hungtruong) on &lt;time style=&quot; font-family:Arial,sans-serif; font-size:14px; line-height:17px;&quot; datetime=&quot;2016-11-18T18:22:24+00:00&quot;&gt;Nov 18, 2016 at 10:22am PST&lt;/time&gt;&lt;/p&gt;&lt;/div&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; defer=&quot;&quot; src=&quot;//www.instagram.com/embed.js&quot;&gt;&lt;/script&gt;
&lt;/center&gt;

&lt;p&gt;For the first year that we’ve been here, Emily and I have been exploring the sights in Seattle, including a bunch of parks like Discovery Park, Volunteer Park (which has lots of cute &lt;a href=&quot;https://www.youtube.com/watch?v=wW_y9rlvX28&quot;&gt;baby ducks&lt;/a&gt;) and Seward Park, among others. With all the free coffee I’m getting, I’m also experimenting with different brew methods including Aeropress, cold brew, and most recently with a &lt;a href=&quot;http://amzn.to/2hE8pJu&quot;&gt;Kalita Wave&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In June &lt;a href=&quot;http://www.hung-truong.com/blog/2016/06/28/wwdc-2016-recap/&quot;&gt;I went to WWDC for the first time&lt;/a&gt; as an attendee and got to meet lots of other cool developers. I got a photo with &lt;a href=&quot;/blog/wp-content/uploads/2016/06/hair-force-one.jpg&quot; rel=&quot;lightbox[3032]&quot;&gt;Craig with the good hair&lt;/a&gt; who does the funny parts of the Apple keynotes. Next year I’m going to be &lt;a href=&quot;http://www.codemobile.co.uk/&quot;&gt;attending a conference in the UK&lt;/a&gt; as a speaker, which I’m really excited about.&lt;/p&gt;

&lt;p&gt;Last year I got a new digital camera which I started using quite a bit to capture our hikes and other interesting moments. This year I upgraded from the &lt;a href=&quot;http://amzn.to/2iQqvph&quot;&gt;Sony RX-100&lt;/a&gt; to the &lt;a href=&quot;http://amzn.to/2ipmuJK&quot;&gt;Fujifilm X-T1&lt;/a&gt;. It’s been really fun learning more about photography as a hobby and reading about lenses and techniques to get cool photo captures. Here’s a few shots that I like, taken from this year:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/blog/wp-content/uploads/2016/12/Koi.jpg&quot; rel=&quot;lightbox[3032]&quot;&gt;&lt;img src=&quot;/blog/wp-content/uploads/2016/12/Koi.jpg?resize=500%2C348&quot; alt=&quot;&quot; width=&quot;500&quot; height=&quot;348&quot; class=&quot;aligncenter size-medium wp-image-3036&quot; srcset=&quot;/blog/wp-content/uploads/2016/12/Koi.jpg?resize=500%2C348 500w, /blog/wp-content/uploads/2016/12/Koi.jpg?resize=768%2C534 768w, /blog/wp-content/uploads/2016/12/Koi.jpg?resize=1024%2C712 1024w, /blog/wp-content/uploads/2016/12/Koi.jpg?w=1280 1280w, /blog/wp-content/uploads/2016/12/Koi.jpg?w=1208 1208w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/blog/wp-content/uploads/2016/12/SPL.jpg&quot; rel=&quot;lightbox[3032]&quot;&gt;&lt;img src=&quot;/blog/wp-content/uploads/2016/12/SPL.jpg?resize=500%2C353&quot; alt=&quot;&quot; width=&quot;500&quot; height=&quot;353&quot; class=&quot;aligncenter size-medium wp-image-3037&quot; srcset=&quot;/blog/wp-content/uploads/2016/12/SPL.jpg?resize=500%2C353 500w, /blog/wp-content/uploads/2016/12/SPL.jpg?resize=768%2C542 768w, /blog/wp-content/uploads/2016/12/SPL.jpg?resize=1024%2C723 1024w, /blog/wp-content/uploads/2016/12/SPL.jpg?w=1280 1280w, /blog/wp-content/uploads/2016/12/SPL.jpg?w=1208 1208w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/blog/wp-content/uploads/2016/12/DSCF2106.jpg&quot; rel=&quot;lightbox[3032]&quot;&gt;&lt;img src=&quot;/blog/wp-content/uploads/2016/12/DSCF2106.jpg?resize=500%2C333&quot; alt=&quot;&quot; width=&quot;500&quot; height=&quot;333&quot; class=&quot;aligncenter size-medium wp-image-3038&quot; srcset=&quot;/blog/wp-content/uploads/2016/12/DSCF2106.jpg?resize=500%2C333 500w, /blog/wp-content/uploads/2016/12/DSCF2106.jpg?resize=768%2C512 768w, /blog/wp-content/uploads/2016/12/DSCF2106.jpg?resize=1024%2C682 1024w, /blog/wp-content/uploads/2016/12/DSCF2106.jpg?w=1280 1280w, /blog/wp-content/uploads/2016/12/DSCF2106.jpg?w=1208 1208w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In September I bought a bunch of supplies and built a tiny &lt;a href=&quot;http://www.hung-truong.com/blog/2016/10/07/building-a-diy-off-grid-solar-bank-on-my-roof/&quot;&gt;solar powered charging system on my roof&lt;/a&gt;. It was a fun proof of concept and I’m using it to charge some of my lower power devices, including an electric bike that I bought for my commute (when it’s not rainy). I’m guessing that it wouldn’t have enough juice yet to power an electric car, but that’s on my to do list.&lt;/p&gt;

&lt;p&gt;In October, Apple announced a new piece of hardware, so naturally &lt;a href=&quot;http://www.hung-truong.com/blog/2016/10/31/on-writing-successful-fart-apps/&quot;&gt;I wrote a fart app for it&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;That’s basically the noteworthy stuff that happened to me this year. Moving and getting used to Seattle is basically a full-time job. I’m also keeping myself busy with updating the house and automating it as much as possible using a &lt;a href=&quot;http://amzn.to/2ijHS4Q&quot;&gt;Samsung SmartThings hub&lt;/a&gt; and various light switches, etc. Other random things I’ve been working on are experimenting with Instagram by using a copious number of tags in all of my posts, buying a pair of Snapchat Spectacles and experimenting with those, and cooking new recipes with the &lt;a href=&quot;http://amzn.to/2itx058&quot;&gt;pressure cooker&lt;/a&gt; I just purchased from Amazon.&lt;/p&gt;

&lt;p&gt;I’m looking forward to doing more stuff in 2017 and hopefully blogging about it more often as well.&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;It’s been a while since I wrote a year-in-review on this blog, let alone any blog post. 2016 was a big year though, so here’s some stuff I did.&lt;/p&gt;

&lt;p&gt;In January Emily and I moved from Ann Arbor, Michigan where we had been living for a few years (Emily since grad school and me since the last time I moved back from Chicago in like 2012) to Seattle, Washington. I’ve loved Seattle ever since I moved here in 2009 to go work for &lt;a href=&quot;http://www.hung-truong.com/blog/2008/11/30/trip-to-seattle-program-manager-interview-with-microsoft/&quot;&gt;Microsoft&lt;/a&gt;, and it’s great to be back! I’m working for a coffee company in Seattle called Starbucks (yes, I get free coffee). I get to work on their iOS app and make latte art in my spare time. Here are some highlights:&lt;/p&gt;

</description>
        
        <pubDate>Sat, 31 Dec 2016 07:59:49 -0800</pubDate>
        <link>http://www.hung-truong.com//blog/2016/12/31/2016-a-hung-truong-year-in-review/</link>
        <guid isPermaLink="true">http://www.hung-truong.com//blog/2016/12/31/2016-a-hung-truong-year-in-review/</guid>
        
        
        <category>Life</category>
        
      </item>
      
    
  </channel>
</rss>
