<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hung Truong: The Blog!</title>
    <description>I say potato, you say potato...</description>
    <link>https://www.hung-truong.com/blog/</link>
    <atom:link href="https://www.hung-truong.com/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 26 Sep 2022 12:34:00 -0700</pubDate>
    <lastBuildDate>Mon, 26 Sep 2022 12:34:00 -0700</lastBuildDate>
    <generator>Jekyll v4.2.1</generator>
    
      
      <item>
        <title>Making a Trombone Champ Controller From a Trombone!</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/tchamp1920hero.jpg&quot; /&gt;
	&lt;figcaption&gt;It&apos;s everyone&apos;s favorite new trombone music rythm game!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Note: this blog post is more or less adapted from a script I used to make my YouTube video on this subject. So if you want to watch the video, I’ve embedded it here! Or if you like reading more, then go ahead and read my post below! (I can’t stand the recent trend of making content that’s video-only so I refuse to not make a blog post out of this video!)&lt;/p&gt;
&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lUXrkq2e1zk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;p&gt;I just got this new game called &lt;a href=&quot;https://store.steampowered.com/app/1059990/Trombone_Champ/&quot;&gt;“Trombone Champ”&lt;/a&gt; which is like Guitar Hero but with a trombone. It’s honestly one of the most refreshing new games I’ve seen, not only because it’s fun and has a good sense of humor, but also because it sort of reinvigorates the music game genre. I love that part of the game is subtly recognizing that you can’t really sound good on a trombone, which is hilarious. I did find that the control scheme could be improved, since you’re expected to play with a mouse (and optionally keyboard keys). So I set out to turn my soprano trombone into a real video game controller for Trombone Champ!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;why-a-trombone-controller&quot;&gt;Why a Trombone Controller?&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/tchamp-screen1.jpg&quot; /&gt;
	&lt;figcaption&gt;A screenshot of the game&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;After I got the game, I played all the levels and had a lot of fun, but I felt like something was missing. I think that driving games are more fun when you use a steering wheel, and flight simulators are more fun when you use a flight stick. Can you imagine how boring Duck Hunt would be if you had to use the regular NES controller to play it? So I decided to make a trombone controller for Trombone Champ.&lt;/p&gt;

&lt;p&gt;Luckily, I have a love of novelty instruments, and I never throw anything away, so I just happened to have a &lt;a href=&quot;https://www.wwbw.com/Jupiter-314L-Soprano-Trombone-Slide-Trumpet-460203.wwbw&quot;&gt;Jupiter soprano trombone in my closet&lt;/a&gt;. I think I origially bought it because it would be funny to use during basketball games (I used to play in the band in college).&lt;/p&gt;

&lt;h3 id=&quot;making-a-fake-mouse&quot;&gt;Making a Fake Mouse&lt;/h3&gt;

&lt;p&gt;So since I have an actual trombone, all I need now is to connect my trombone to the computer, so I can play the game with it. I did some searching and found &lt;a href=&quot;https://github.com/T-vK/ESP32-BLE-Mouse&quot;&gt;a library that can turn a esp32 microcontroller into a bluetooth mouse&lt;/a&gt;. This is perfect because then all I need to is connect some sensors to my trombone and have those move the mouse and click its buttons. Then I can connect the mouse to my computer and play the game.&lt;/p&gt;

&lt;p&gt;I decided to use a &lt;a href=&quot;https://www.dfrobot.com/product-1590.html&quot;&gt;Firebeetle ESP32 microcontroller&lt;/a&gt; because I had one lying around from a previous project, and because it comes with a connector to plug in a lithium ion battery. This is nice because I’ll be able to play the game completely wirelessly instead of needing to plug in to usb for power.&lt;/p&gt;

&lt;h3 id=&quot;adding-sensors&quot;&gt;Adding Sensors&lt;/h3&gt;

&lt;p&gt;So now I have two different problems to solve. I’ll call these the “blow problem” and the “slide problem.” Let’s start with the blow problem.&lt;/p&gt;

&lt;p&gt;I didn’t want to just add a button to my trombone to play a sound, because that’s not how wind instruments work. I want to actually blow into something. So I looked for different ways to &lt;a href=&quot;https://www.mouser.com/c/sensors/flow-sensors/?for%20use%20with=Air&quot;&gt;measure air flow&lt;/a&gt;, and it turns out these types of sensors are really expensive and didn’t make sense for my project. I ended up finding this neat &lt;a href=&quot;https://www.sparkfun.com/products/16476&quot;&gt;air pressure sensor&lt;/a&gt; that also has a port in it to attach a tube. My idea was that I could attach a tube to the trombone mouthpiece and measure the air pressure difference when I blew into it, directly into the sensor.&lt;/p&gt;

&lt;p&gt;This was actually a terrible idea though, because the tubing would need to go through the entire trombone and then come out of the bell, and it would really impede the movement of the slide.&lt;/p&gt;

&lt;p&gt;I ended up putting the whole sensor in the bell and then sealing the entire instrument with some saran wrap. When it’s on it has a red LED which I think gives it a pretty nice cyberpunk feel. Then I measured the ambient air pressure inside the trombone. In in my room it happens to be around 1011 hPa. When I blow into the horn, it causes the air pressure to go up, and I can register a click. When the air pressure goes back down, I can release it.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/trombone_air_sensor.jpg&quot; /&gt;
	&lt;figcaption&gt;The air sensor in my trombone&apos;s bell&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So with my blow problem a thing of the past, let’s move on to the slide problem. In the game, you’re supposed to use a mouse to simulate the slide of a trombone. But there are a lot of problems with this. The slide on a trombone is fixed, so if you go to first position, you hit the end of the trombone. You can’t go to negative positions, but your mouse doesn’t just stop when you hit the end of the screen. Plus on the instrument you can use muscle memory and look at the bell to figure out what note you’re gonna hit.&lt;/p&gt;

&lt;p&gt;I looked at a bunch of different ways to solve the problem of using the actual trombone slide as a controller for the game’s slide. There are various distance sensors: lidar, sonar, time of flight, you could also probably use a gyroscope and accelerometer to figure out what direction the slide is going. I think I saw someone use a wiimote’s accelerometer to make a 3d air mouse once.&lt;/p&gt;

&lt;p&gt;To be honest, I just wanted to get started on this project quickly, so I ordered the &lt;a href=&quot;https://amzn.to/3dNOUOD&quot;&gt;cheapest time of flight sensor on Amazon that had next day shipping&lt;/a&gt;. I attached it to the bell of the trombone and put a piece of cardboard on it that it can use to bounce a laser on to detect how far the laser is away from the initial position.&lt;/p&gt;

&lt;p&gt;When I actually hooked this up to my microcontroller, it ended up giving some pretty inaccurate readings but I figured I could hook it up to my mouse software and see what happens.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/trombone_distance_sensor.jpg&quot; /&gt;
	&lt;figcaption&gt;The distance sensor attached to the bell&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;
&lt;center&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;This weekend I made a Trombone Champ controller out of a real (soprano) trombone! I&amp;#39;ll have a video out shortly about the process, but for now, enjoy this video of me absolutely failing our national anthem with my new device! cc &lt;a href=&quot;https://twitter.com/HolyWowStudios?ref_src=twsrc%5Etfw&quot;&gt;@HolyWowStudios&lt;/a&gt; &lt;a href=&quot;https://t.co/SvPoxfk6GV&quot;&gt;pic.twitter.com/SvPoxfk6GV&lt;/a&gt;&lt;/p&gt;&amp;mdash; Hung Truong (@hungtruong) &lt;a href=&quot;https://twitter.com/hungtruong/status/1573854148923359232?ref_src=twsrc%5Etfw&quot;&gt;September 25, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/center&gt;

&lt;p&gt;I’ve linked my tweet of an attempt at the Star Spangled Banner above, but if you don’t want to watch a video, I’ll tell you that it did not work well! I think the time-of-flight sensor I got was faulty, as it didn’t go past 100mm with any degree of accuracy, but is rated for 2000mm.&lt;/p&gt;

&lt;p&gt;But to be honest, I’m really happy with the way this project turned out. The blow sensor works a lot better than I expected it to, as I was initially worried it wouldn’t be able to register the notes quickly enough. I also learned a lot about simulating a bluetooth mouse which I could probably use in future projects. I’m happy to share this project even though it isn’t perfect yet because this kind of prototyping is really an iterative process anyway.&lt;/p&gt;

&lt;p&gt;I also thought it was interesting how the slightly air tight seal on the trombone bell works. When you normally play an instrument, you have some back pressure, but air is also going through. I would say that I probably use more air blowing into the trombone without making any noise than I do when I’m actually playing it.  This could lead to me getting tired pretty quickly, so I might adjust the seal on the horn a bit more so I don’t have to blow as hard.&lt;/p&gt;

&lt;p&gt;As far as next steps go, I’ve already purchased a few new sensors to try out, one is an &lt;a href=&quot;https://amzn.to/3RdPcMs&quot;&gt;acoustic distance detector&lt;/a&gt;, and the other is a &lt;a href=&quot;https://amzn.to/3xTDXSe&quot;&gt;more expensive time of flight sensor from adafruit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’ll probably update this blog post when I try the new sensors, so look for an update below at some point!&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/tchamp1920hero.jpg&quot; /&gt;
	&lt;figcaption&gt;It&apos;s everyone&apos;s favorite new trombone music rythm game!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Note: this blog post is more or less adapted from a script I used to make my YouTube video on this subject. So if you want to watch the video, I’ve embedded it here! Or if you like reading more, then go ahead and read my post below! (I can’t stand the recent trend of making content that’s video-only so I refuse to not make a blog post out of this video!)&lt;/p&gt;
&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lUXrkq2e1zk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;p&gt;I just got this new game called &lt;a href=&quot;https://store.steampowered.com/app/1059990/Trombone_Champ/&quot;&gt;“Trombone Champ”&lt;/a&gt; which is like Guitar Hero but with a trombone. It’s honestly one of the most refreshing new games I’ve seen, not only because it’s fun and has a good sense of humor, but also because it sort of reinvigorates the music game genre. I love that part of the game is subtly recognizing that you can’t really sound good on a trombone, which is hilarious. I did find that the control scheme could be improved, since you’re expected to play with a mouse (and optionally keyboard keys). So I set out to turn my soprano trombone into a real video game controller for Trombone Champ!&lt;/p&gt;

</description>
        
        <pubDate>Mon, 26 Sep 2022 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2022/09/26/making-a-trombone-champ-controller-from-a-trombone/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2022/09/26/making-a-trombone-champ-controller-from-a-trombone/</guid>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Stable Diffusion: Generating Images From Words</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/ron-swanson-weasley.png&quot; /&gt;
	&lt;figcaption&gt;Ron Swanson as Ron Weasley!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For the past few months or so, I’ve been noticing a lot of news stories about &lt;a href=&quot;https://openai.com/dall-e-2/&quot;&gt;DALL·E 2&lt;/a&gt;, an AI image generator that uses GANs to create images from prompts. It was private for quite a while, and there were some similar, less powerful projects that were open. I played around with them a bit but I was waiting for a general release. I ended up getting into the DALL·E 2 beta a few weeks ago and last week I saw news that there was a new release of another project called &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;&gt;Stable Diffusion&lt;/a&gt;, so I installed it on my MacBook. The results really blew me away!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;getting-it-working&quot;&gt;Getting it working&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/typing-on-monitors.png&quot; /&gt;
	&lt;figcaption&gt;Prompt: photo from behind an asian software engineer typing a keyboard with many monitors in front of him, studio lighting bokeh&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;It wasn’t too bad getting the release of Stable Diffusion working on my Mac as I just went through some of the steps &lt;a href=&quot;https://zenn.dev/bellbind/scraps/ea15aab699dde9&quot;&gt;on this site I found&lt;/a&gt;. There were some gotchas, like needing to install Rust to compile something and maybe a few files to change around, but for the most part it worked pretty quickly. I could probably have written down the steps but I’m sure it will be a lot more simple when they add more specific support for Apple Silicon.&lt;/p&gt;

&lt;p&gt;Right now it takes about 3 minutes to create an image with my M2 MacBook Air, which is kind of slow. I ended up using &lt;a href=&quot;https://colab.research.google.com&quot;&gt;Google Collab&lt;/a&gt; which lets you use their GPUs for free. The next day though, I found I was rate limited. So I moved over to &lt;a href=&quot;https://www.kaggle.com&quot;&gt;Kaggle&lt;/a&gt; which at least tells you what your GPU limits are, and they seem pretty reasonable (like 30 hours a week at least). The Kaggle notebook has been my main workflow so far because it’s quite fast (like 10 times faster than my Mac) and the short feedback loop really helps with coming up with prompts.&lt;/p&gt;

&lt;h3 id=&quot;prompt-engineering&quot;&gt;Prompt Engineering&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/tail-of-two-kitties.png&quot; /&gt;
	&lt;figcaption&gt;Prompt: cat vs a british shorthair cat sitting in front of a giant cheeseburger in the african savannah, fujifilm x-t2 35mm golden hour&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So What is a prompt anyway? A prompt is what you type in to the image generator to describe what you want it to create for you. If you say you want a picture of a cat, it’ll likely come up with a pretty good cat. But if you want, you can also describe the cat in more detail to get a more specific image. You could add the breed, for example, or say it’s a robot cat, or make it sit on a park bench. There are so many possibilities of what kind of cat to show that the prompt ends up being incredibly important to get what you want.&lt;/p&gt;

&lt;p&gt;There’s been a lot of research done on prompt engineering. Some of it feels like a shortcut, by asking for an image in the style of a famous artist. You can also just describe the medium of the image, like a water color or illustration. I’ve noticed people on Reddit adding words like “trending on artstation” which I guess is a way to suggest that the image is aesthetically pleasing to a majority of people. You can also say something was painted badly, which is kind of hilarious.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/ugly-woman-painting.png&quot; /&gt;
	&lt;figcaption&gt;Prompt: a really ugly painting of a woman&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Part of the fun of playing around with these tools is that they’re so new that it’s possible to find words that can create certain images that no one else knows about. For me, it brings back the feeling of being on the early internet, when not everything was indexed by Google to the point where there were no hidden gems. Someone should bring back “Cool Site of the Day” but for prompts!&lt;/p&gt;

&lt;h3 id=&quot;different-techniques&quot;&gt;Different Techniques&lt;/h3&gt;

&lt;p&gt;I’ve learned that there are a few different techniques to make images, and I’m learning quite a few more. The simplest one is text to image, which I just described. The way I understand it is that basically you start with some random noise, and then two AIs work to alter the noise to turn it into the image you want by iterating changes and measuring how closely the image matches your description. That’s probably really simplified and maybe wrong but whatever, I’m not an AI engineer.&lt;/p&gt;

&lt;p&gt;Another way to create images is to start with a base image, and also feed the AI a prompt. Since you can choose the starting image (instead of just random noise), there’s a better chance that the image converges to something that resembles your input image. This gives you quite a bit more control over the final image’s general shape, composition, etc. You could feed it stick figures or a photo from your phone. I’ve seen people turn stick drawings into D&amp;amp;D character portraits using this technique.&lt;/p&gt;

&lt;p&gt;I tried this technique out by using a photo of my dog, Sodapop, sitting in the grass. The picture is pretty good, but it’s not award winning or anything. I fed the text “a watercolor illustration of a black and white cardigan corgi sitting in the middle of a green flowery meadow in front of an orange ball, masterpiece, big cute eyes”. I didn’t start with that, but I kept changing it to try and get an image that I wanted.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/sodapop-watercolor.png&quot; /&gt;
	&lt;figcaption&gt;Prompt: Sodapop vs Watercolorpop&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I also played around with different strengths and amounts of iterations. I found that if I used too many iterations, the image didn’t really resemble Sodapop anymore. He’s a black and white Corgi, which is less common, so there’s probably more of a bias towards the sable and white ones. One thing I learned is that it’s better to just generate a huge number of images and then pick the ones you like. You can save the random seed value and use it to refine the image further as well. There were a lot of really terrible looking corgi watercolor images which my computer is full of now. But there were also some fairly good ones too! The power with this AI is that it’s pretty cheap to just make more images until you get what you want.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/reject-corgi.png&quot; /&gt;
	&lt;figcaption&gt;One of many rejected generations&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;future-techniques&quot;&gt;Future Techniques&lt;/h3&gt;

&lt;p&gt;There is another technique I tried recently where someone tried to create a bigger image (right now most video cards can only do 512x512 and that’s what the model is trained on) by creating an image, upscaling it and then running the image to image process on 9 square parts of the upscaled image. When I tried this, I found that it added weird artifacts into each square piece. It was recursively trying to fit the whole prompt into each square. My prompt was a garden, and it basically tried to add a garden into each subsquare of the image. This could have been due to a current bug where the random seed on Mac doesn’t really work, but I don’t have the hardware to try it on a non-Mac right now.&lt;/p&gt;

&lt;p&gt;I’ve had a lot more fun playing around with this image generation stuff than I have in a long time with technology, so I ordered a new graphics card so I can iterate on things more quickly on my own infrastructure. There’s something really magical about using a model file that’s only a few gigabytes to basically create any image you can think of. If my internet connection ever goes down for the count, this could be my main source of entertainment.&lt;/p&gt;

&lt;p&gt;There’s a bunch of other things I want to try. There’s a technique called &lt;a href=&quot;https://textual-inversion.github.io&quot;&gt;“Textual Inversion”&lt;/a&gt; where you can sort of re-train (but not really) the model to use a personalized word. I could do this with Sodapop so I stop getting Pembroke Corgis when I want my Corgi. I was also wondering if I could use it with pictures of myself, since Stable Diffusion seems to work really well with making images with well known celebrities in them.&lt;/p&gt;

&lt;p&gt;When I first saw this technology I figured it would be good for creating blog post images (which obviously it was for this post). I’m also envisioning things like services for creating customized watercolor portraits for your dog, or custom fantasy avatars for a person. I think people have just barely scratched the surface here so hopefully there’s a lot more interesting stuff coming up.&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/ron-swanson-weasley.png&quot; /&gt;
	&lt;figcaption&gt;Ron Swanson as Ron Weasley!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For the past few months or so, I’ve been noticing a lot of news stories about &lt;a href=&quot;https://openai.com/dall-e-2/&quot;&gt;DALL·E 2&lt;/a&gt;, an AI image generator that uses GANs to create images from prompts. It was private for quite a while, and there were some similar, less powerful projects that were open. I played around with them a bit but I was waiting for a general release. I ended up getting into the DALL·E 2 beta a few weeks ago and last week I saw news that there was a new release of another project called &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;&gt;Stable Diffusion&lt;/a&gt;, so I installed it on my MacBook. The results really blew me away!&lt;/p&gt;

</description>
        
        <pubDate>Sun, 28 Aug 2022 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2022/08/28/stable-diffusion-generating-images-from-words/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2022/08/28/stable-diffusion-generating-images-from-words/</guid>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>I Created a Robot to Solve Wordle For Me</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2022/WordleScreenshot.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unless you’ve been living under a rock for the past few days, you’ve heard of this game online called Wordle. It’s been growing like crazy and the New York Times even wrote a &lt;a href=&quot;https://www.nytimes.com/2022/01/03/technology/wordle-word-game-creator.html&quot;&gt;profile on the creator&lt;/a&gt;. If you know me, you know I like automating things, so it should be no surprise that I decided to automate playing the game. Here’s a blog post on how I did it.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;wtf-is-a-wordle&quot;&gt;WTF is a Wordle?&lt;/h3&gt;

&lt;p&gt;I guess I could start out by explaining what Wordle is, in case you’re reading this blog without knowing what it is, and if you are, why not just &lt;a href=&quot;https://www.powerlanguage.co.uk/wordle/&quot;&gt;play it first&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;Anyway, the fun part of Wordle is that there isn’t really a way to cheat. You try to guess a five letter word, and the game gives you feedback for each letter in the word you guessed. Either the letter doesn’t exist at all in the target word, the letter exists in the word but not in that position, or the letter does exist in the word in that position. Letters can repeat, e.g. “silly,” and you get 6 guesses before you lose (I think). I’ve actually never lost, because 6 is a pretty good number of guesses unless you get really unlucky.&lt;/p&gt;

&lt;p&gt;If you think about it mathematically, 6 tries times 5 letters means you could theoretically narrow down 25 letters before your last guess. But that’s assuming you only use unique letters and there are words you can actually use to guess those letters (the game doesn’t allow non-words).&lt;/p&gt;

&lt;p&gt;There is the problem of narrowing the word down to maybe 1 letter difference but there’s a bunch of words that end in the same letters, e.g.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;frank&lt;/li&gt;
  &lt;li&gt;drank&lt;/li&gt;
  &lt;li&gt;crank&lt;/li&gt;
  &lt;li&gt;spank&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;etc. If you get the last three letters then you still need to narrow down what the front ones are, which you could do exploratively, but you can also just keep trying likely words.&lt;/p&gt;

&lt;p&gt;Anyway, if it isn’t obvious yet, I probably enjoy the metagame of Wordle more than playing the game itself.&lt;/p&gt;

&lt;h3 id=&quot;solving-wordle&quot;&gt;Solving Wordle&lt;/h3&gt;
&lt;p&gt;So how do you go about automating a game like Wordle? I started by trying to inspect the network requests going back and forth between the web frontend and the backend. Suprisingly, there are no network requests made because the whole game is just a javascript file, and (I think) a hash that determines what the word of the day is. The whole game is just a javascript file that you download and run.&lt;/p&gt;

&lt;p&gt;So looking at the javascript file, I saw a bunch of hard to read code (because javacript), and a few interesting arrays.&lt;/p&gt;

&lt;p&gt;The first had a bunch of pretty normal looking words. The other had words that looked real, but were less common. My guess is that there are words that can actually be used for the game because a normal person would know them, but another list of words that are valid guesses (because the game doesn’t let you put nonsense in). So I decided to take the first list of words and use it as my “dictionary” of possible choices, rather than just use a linux dictionary of all the obscure words that exist.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/wordle_list.png&quot; /&gt;
	&lt;figcaption&gt;The two word lists, possible answers on top and valid obscure words on bottom.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As far as playing the game, you have to start somewhere. I started out by keeping a list of all possible answers (copied from the source), and picking a random one. Once that happens, you get hints for which letters to keep trying and which to discard. The game code actually has three values for letter results: “present,” “absent,” and “correct.”&lt;/p&gt;

&lt;p&gt;From these, the basic algorithm to remove words from the possible choices is (for each letter):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If present, remove all words that have that letter in that specific position (for example if the word is “farts” and you guess “after,” the first ‘a’ would be present, but never in the first position). Also, remove all words that do not have that letter present in any positions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If absent, remove all words that have that letter in any position.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If correct, remove all words that do not have that letter in that position.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That’s basically it, and doing this will quickly whittle down the list of possible choices.&lt;/p&gt;

&lt;p&gt;There are some other optimizations that I tried out but I don’t think they have much of a significant effect on how quickly the bot can guess the right answer. For example, I made a sorted list of the most common letters used in English words and had the bot prefer words with the most common letters first, to narrow down results faster. It might be possible to improve this by calculating probabilities or something down to the individual word level but I’m way too lazy (for now) to try that.&lt;/p&gt;

&lt;h3 id=&quot;a-mac-app&quot;&gt;A Mac App&lt;/h3&gt;

&lt;p&gt;I ended up writing a Mac app to run the solver because I knew I could use a webview and make it evaluate some javascript commands. I wrote a few functions for doing things like keying in letters and hitting enter, and reading the results using the “shadow dom” which sounds a little bit like forbidden magic but whatever.&lt;/p&gt;

&lt;p&gt;I had to throw in some manual sleep() calls because it would go too fast if I didn’t. Also it looks more like a real person is playing if there’s a slight delay in entering letters.&lt;/p&gt;

&lt;p&gt;Here’s what the Mac app looks like solving the puzzle from Jan 8, 2021:&lt;/p&gt;

&lt;video controls=&quot;controls&quot; autoplay=&quot;true&quot; loop=&quot;true&quot; width=&quot;800&quot; height=&quot;600&quot; name=&quot;Wordle Solver Video&quot;&gt;
 &lt;source src=&quot;/blog/wp-content/uploads/2022/wordle.mov&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;I’ve found that watching my bot play the game is more interesting to me than playing the actual game. Since it’s somewhat randomized, it’s fun to see what combinations of words the app tries, and how quickly it can narrow down its results. I log the list of potential answers after each round, and it’s fun to see how the app “thinks.”&lt;/p&gt;

&lt;h3 id=&quot;one-more-thing-the-bot-but-irl&quot;&gt;One More Thing: The Bot, But IRL&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/idraw.png&quot; /&gt;
	&lt;figcaption&gt;iDraw: the second most advanced pen plotter known to me.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Making an app to automate something is cool, but I felt like something was still missing. I recently bought a mechanical pen plotter (which I should probably make another blog post about) &lt;a href=&quot;https://youtu.be/EirwSXMVa2U&quot;&gt;for making generative art&lt;/a&gt;, so I thought it would be fun to program the pen plotter to play the game on my iPad using my Apple Pencil.&lt;/p&gt;

&lt;p&gt;The pen plotter is basically a robot that has an arm and can raise and lower a pen (or Apple Pencil) and move it along an x-y axis. I bought an &lt;a href=&quot;https://uunatek.com/product/idraw-handwriting-drawing-machine/&quot;&gt;off-brand&lt;/a&gt; version of the &lt;a href=&quot;https://axidraw.com&quot;&gt;Axidraw&lt;/a&gt;, which has a similar but different board. I tried using the &lt;a href=&quot;https://axidraw.com/doc/py_api/#introduction&quot;&gt;axidraw python library&lt;/a&gt; to control the iDraw. It sort of works, but there are some weird differences. For example, the pen down movement seems to bring the pen up instead of down, so everything is reversed. Also the scaling is off, so if I want to move the pen 1 inch, it seems to move it by 2 centimeters instead. I just worked around these issues because I saved a lot of money by buying the cheaper one (it was about half the price).&lt;/p&gt;

&lt;p&gt;I wrote a script in Python to take one command line argument which is the word to guess, and hardcoded positions for each letter on the iPad screen. I was thinking I could use some kind of computer vision, AI library to detect positions and translate them for the plotter, but that’s also too much work. Maybe for a different project. The Mac app runs as before, except that it runs the local Python script when guessing, and waits for the script to complete before trying the next guess.&lt;/p&gt;

&lt;p&gt;I’m pretty happy with this setup, even though it’s a pain in the ass to get it working, because I have to set the pen height and line the iPad up with the robot correctly. I did end up getting the measurements right for the keyboard keys on the first try, which was cool. It’s just that if the iPad slides on the table then everything is off.&lt;/p&gt;

&lt;video autoplay=&quot;true&quot; loop=&quot;true&quot; width=&quot;800&quot; height=&quot;600&quot; name=&quot;Wordle Solver Bot Video&quot;&gt;
 &lt;source src=&quot;/blog/wp-content/uploads/2022/wordle_bot_demo.mov&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;I made a YouTube video that describes this more, and embellished some stuff for entertainment. So if you want to watch that, you can &lt;a href=&quot;https://www.youtube.com/watch?v=au5IPBBhiPw&quot;&gt;see it here&lt;/a&gt;, (and don’t forget to obliterate that like and subscribe button)!&lt;/p&gt;

&lt;p&gt;I also pushed the solver specific code to Github. It’s pretty ugly but if you’re interested in how it works, you can check it out. I probably should’ve used regular expressions in hindsight instead of blowing up strings into arrays but I figured that with the word list at around 2,300 words, it really doesn’t matter how inefficient I am!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/hungtruong/Wordle-Bot&quot;&gt;https://github.com/hungtruong/Wordle-Bot&lt;/a&gt;&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2022/WordleScreenshot.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unless you’ve been living under a rock for the past few days, you’ve heard of this game online called Wordle. It’s been growing like crazy and the New York Times even wrote a &lt;a href=&quot;https://www.nytimes.com/2022/01/03/technology/wordle-word-game-creator.html&quot;&gt;profile on the creator&lt;/a&gt;. If you know me, you know I like automating things, so it should be no surprise that I decided to automate playing the game. Here’s a blog post on how I did it.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 10 Jan 2022 00:00:00 -0800</pubDate>
        <link>https://www.hung-truong.com/blog/2022/01/10/i-created-a-robot-to-solve-wordle-for-me/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2022/01/10/i-created-a-robot-to-solve-wordle-for-me/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Cloning Zwift on iOS Part 5: SwiftUI and Combine</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2021/Zswift_main_screen.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I recently switched teams at Amazon to one that is using SwiftUI and Combine, so I finally have a good excuse to learn the two. I am somewhat familiar with Functional Reactive Programming from using RxSwift at Lyft, but I’ve only really dabbled a bit with SwiftUI.&lt;/p&gt;

&lt;p&gt;I decided to spend some time last weekend (and this weekend) rewriting most of my Zwift clone app to use SwiftUI and Combine, and here’s some of the stuff I learned along the way.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;moving-to-combine&quot;&gt;Moving to Combine&lt;/h3&gt;

&lt;p&gt;When I originally wrote the Zswift app, I didn’t really spend too much time on making sure the data model or architecture was the cleanest or anything. It’s really more of a hodgepodge of explorations and trying to get stuff to just work. Because of this, I probably violated a bunch of best practices. I had a “Workout” object that stored all of the different information that is needed to model a workout, but I also threw a bunch of logic and functions in there that probably didn’t make sense.&lt;/p&gt;

&lt;p&gt;For example, the Workout had info on each segment of the workout, along with the duration and amount of power for the segment. As the workout progressed, I would store the elapsed time as well as a bunch of other state variables like the current segment and other variables like time in current segment that I used to drive the UI. Since I kept a bunch of variables to keep track of state, it’s possible that some of them would get out of sync with each other, and that would cause bugs. I think I had a bunch of off-by-one errors where I would reach the end of an index and crash or the time within a segment would be off by 1 so I’d be at 2:01/2:00 as far as progress went. Those kind of bugs.&lt;/p&gt;

&lt;p&gt;In moving to Combine, my goal was to have the state of the workout flow from the one thing about the workout that actually changes: the elapsed time.&lt;/p&gt;

&lt;p&gt;The elapsed time literally decides which segment I’m in, how long I’ve been working out (duh), how long I’ve been in the current segment, how hard I should be pedaling, etc. I ended up creating a separate class to keep track of the state of the workout, and just use the workout as a static definition of the workout. I could’ve done this before migrating to use Combine, but like I said, it was working and I didn’t feel the need to refactor.&lt;/p&gt;

&lt;p&gt;In my current setup, the WorkoutManager has a @Published variable that keeps track of the elapsed time, and then I create a bunch of other publishers based on that one. I also have publishers that combine (imagine that) with other publishers. For example, I have a publisher called “timeInCurrentSegmentPublisher” that publishes the amount of time that I’ve been in the current segment. I combine this publisher with the “currentSegmentPublisher” which gives me the current segment, and use this to calculate the percentage progress for the current segment. I have to combine the two because each publisher only gives me one thing.&lt;/p&gt;

&lt;p&gt;I weighed the benefits of creating publishers with multiple tuples of values, but in most cases it didn’t make sense, and I think it goes against the concept of making the streams composable, but I did end up making one for currentSegmentPublisher since it calculates the current segment and the current segment’s index at the same time anyway. Even as I’m writing about it now, I’m not sure if the better way is to create two different publishers since it’s kinda clunky to grab the desired value from the tuple.&lt;/p&gt;

&lt;p&gt;Anyway, the result here is that my Workout object doesn’t have any more state at all. I could make it a struct but there’s some SwiftUI requirement for my Workout to be a class if I want to use it in a Modal view (which the workout detail is set up as) so for now I’ll just leave it as-is. I could also create a view model if I really wanted the Workout object to be a struct.&lt;/p&gt;

&lt;p&gt;I’m sure I could optimize the Combine publishers even more but since they’re working now I’ll just leave them.&lt;/p&gt;

&lt;h3 id=&quot;swiftui&quot;&gt;SwiftUI&lt;/h3&gt;
&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2021/xcode_swiftui_previews.png&quot; /&gt;
	&lt;figcaption&gt;An example of the live SwiftUI previews that update automatically&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The general opinion that I see from others about SwiftUI is that the more people use it, the more they like it, and that it really forces you to rethink how you define interfaces. I heard this a million times but until you actually play around with it in a non-trivial example I feel like it’s hard to really understand it.&lt;/p&gt;

&lt;p&gt;The gist is that SwiftUI is a declarative way to define your UIs instead of an Imperative way. The difference can seem subtle until you start doing stuff. What it means is that instead of telling the computer how to do something, you just tell it what you want. If that’s confusing then yeah, actually it is confusing.&lt;/p&gt;

&lt;p&gt;I guess to put it another way, in UIKit you can define views as objects, give them properties, add them to a parent view and then set some constraints on them. In this imperative example, it’s up to you to define every step to the computer to tell it what to do, and hope that your interface matches what you were thinking.&lt;/p&gt;

&lt;p&gt;In a declarative syntax, you can describe what you want, and add some modifiers to it if you need to have more control over the actual output. From there, you also define state variables or observed objects that the SwiftUI view will use to actually come up with the completed interface.&lt;/p&gt;

&lt;p&gt;Imagine that you want to show a list of dog breeds. In an imperative system you need to set up the collection view controller, fill out your functions for “cellAtIndex” and “didSelectCellAtIndex” etc, and supply the cells. In a declarative system you can say “I want a list of ‘DogViews’ that use the ‘Dog’ model” and define a “NavigationLink” to define what happens when you tap on the cell.&lt;/p&gt;

&lt;p&gt;The interesting part to me was that the SwiftUI view is an immutable struct, so you can’t modify the view as you would a UIView when states change. Of course your view can respond to changes in the data model, but all of those states need to be determined at compile time rather than runtime.&lt;/p&gt;

&lt;p&gt;One of the best parts about SwiftUI is how the “live” previews work. They aren’t exactly real time, but fast enough that the feedback loop between writing UI code and seeing the result is very tight. In the past I’ve had to write code to update a UI (or use Interface Builder), then build and run, and go to the part of the app where the UI I changed was. This could mean it would be a few seconds or minutes before I saw whether the change I just wrote actually worked. In SwiftUI you make a change and once you’re done writing it, the preview window will show it. The system isn’t perfect, of course, as sometimes it can’t compile and you need to reenable the live preview. But it’s really the best tradeoff between the WYSIWYG style of Interface Builder and the readability of programmatic view code. I’m sure the tools will improve in the future, and I’m quite certain that this ease of previewing code will be one of the factors that will motivate people to switch to SwiftUI.&lt;/p&gt;

&lt;h3 id=&quot;the-workout-view&quot;&gt;The Workout View&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2021/workout_view.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I modeled my original interface after the Zwift app. In the Zwift app, you can see the workout represented as a bunch of rectangles representing segments that get taller when the target power is higher. The width is determined by the length of the segment.&lt;/p&gt;

&lt;p&gt;I basically copied this and implemented it with a UIStackView and percentage constraints. I thought it was pretty cool but one thing that bothered me is the warmup and cooldown segments. Those have a starting value and ending value that are different (warmups start lower and end higher). I didn’t want to spend too much time drawing the start of the rectangle to be lower than the end (plus it wouldn’t technically be a rectangle at that point) so I just used the “lower” value and set them as flat rectangles.&lt;/p&gt;

&lt;p&gt;I decided to practice some custom drawing in SwiftUi to properly draw the warmup and cooldown segments and I’m pretty happy with the result. The view is drawn by taking the desired start and end heights and width, and drawing a path with it. I used “path.addArc” to get a nice rounded corner effect, though it isn’t perfect. I also draw a 1px wide vertical line to show where I am in the workout by offsetting it from the left by a percentage of the view’s width.&lt;/p&gt;

&lt;p&gt;I also went ahead and stole the color scheme from the Zwift app which makes it look a bit more polished.&lt;/p&gt;

&lt;h3 id=&quot;the-workout-detail&quot;&gt;The Workout Detail&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2021/side_by_side.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In addition to updating the workout representation view, I also decided to just overhaul the entire workout detail view (the view I see when I’m in a workout). The old interface was pretty basic with a bunch of grids and text boxes that didn’t really have any visual separation from each other, aside from the size and distribution of the labels.&lt;/p&gt;

&lt;p&gt;I ended up experimenting with the “GroupBox” component of SwiftUI which is pretty simple but makes a big difference just in terms of separating out the different interface elements. I also added some accent colors and icons to some of the text labels, and I added progress indicators to the segment time and elapsed time sections just to make them stand out more. Overall I really like the new workout detail view which is good since I’m literally the only person who uses this app.&lt;/p&gt;

&lt;p&gt;I have some more ideas for enhancements, like recreating the Apple Watch heart rate animation which shows a heart pulsing at your actual heart rate. The animation seamlessly updates when your heart rate changes, which is pretty cool.&lt;/p&gt;

&lt;p&gt;I also want to add some more tracking informtion, like a histogram (maybe with candlestick charts or sparklines) of the wattage and heart rate info. This would be nice to have but the HealthKit integration already includes the heart rate chart. And I don’t know if I care too much about the wattage graph.&lt;/p&gt;

</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2021/Zswift_main_screen.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I recently switched teams at Amazon to one that is using SwiftUI and Combine, so I finally have a good excuse to learn the two. I am somewhat familiar with Functional Reactive Programming from using RxSwift at Lyft, but I’ve only really dabbled a bit with SwiftUI.&lt;/p&gt;

&lt;p&gt;I decided to spend some time last weekend (and this weekend) rewriting most of my Zwift clone app to use SwiftUI and Combine, and here’s some of the stuff I learned along the way.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 02 May 2021 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2021/05/02/zwift-clone-swiftui-combine/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2021/05/02/zwift-clone-swiftui-combine/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Reverse Engineering Quibi: Protocol Buffers and HLS Streaming</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/Quibi_Logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ve had a lot of free time in the past few weeks so I decided to spend some of it working on side projects. I really enjoy reverse engineering apps, so I decided to take a look at Quibi.&lt;/p&gt;

&lt;p&gt;Quibi (short for “Quick Bites”) is a video streaming app/service that has a bunch of shows that are short. The idea is that you can sit on a bus ride and consume an episode or two, depending on how long your commute is. One of the constraints of the platform is that you can only watch these videos on a phone or tablet with the app installed.&lt;/p&gt;

&lt;p&gt;Since everyone is stuck indoors for a while this constraint is kinda stupid and most people would probably like to watch their videos on their big tv rather than huddle around a phone, which is what Emily and I had to do to watch that stupid viral show about the &lt;a href=&quot;https://twitter.com/zachraffio/status/1250273191810875392&quot;&gt;terrible wife with the golden arm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, I had an idea to write a tvOS app that would work with Quibi, so you could watch your terrible shows on your tv. Here’s what I learned trying to reverse engineer the Quibi app.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;setting-up-charles&quot;&gt;Setting up Charles&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2020/charlesproxy.jpg&quot; /&gt;
	&lt;figcaption&gt;Get out of my head Charles!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The first rule of reverse engineering club is that you should probably install &lt;a href=&quot;https://www.charlesproxy.com/&quot;&gt;Charles proxy&lt;/a&gt; on your computer, and point your phone to it. This lets you inspect network requests and figure out the API for an app. Depending on how stringent the app’s security settings are, you can either learn a lot, or very little from Charles.&lt;/p&gt;

&lt;p&gt;The way Charles works is that you basically route all of your traffic from your iPhone to your computer. You need to install a certificate on your phone and trust it, so that your phone thinks that it’s going through a secure connection when it’s really getting owned. But since you’re self-owning, it’s generally okay. I’m not going to do a full tutorial on how to do this because you can just read the docs.&lt;/p&gt;

&lt;p&gt;You can also buy a version of Charles that runs directly on your phone. I did this, but I also find it easier to work with on my computer, so I used the free version that stops working every 30 minutes. I should probably buy the full version.&lt;/p&gt;

&lt;p&gt;Anyway, once you get the proxy working on your phone and add the proper domains to the allowed list, you should start seeing your network requests and responses pop up in Charles after firing up the app and doing stuff.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/charlesinaction.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It turns out (luckily for me) that Quibi doesn’t utilize certificate pinning (at least at the time I’m writing this article). Cert pinning is a way to prevent snooping of network requests by embedding a certificate (or maybe a hash of it or a public key) to be trusted into the actual app binary. This means that adding the additional Charles certificate won’t work because the app won’t accept it. The process of pinning is kind of a pain in the ass, which is why a lot of companies don’t do it.&lt;/p&gt;

&lt;p&gt;Because Quibi doesn’t use cert pinning, that means I can observe all of the requests and responses that the app sends and receives, which makes it a lot easier to reverse engineer!&lt;/p&gt;

&lt;h3 id=&quot;authentication&quot;&gt;Authentication&lt;/h3&gt;
&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2020/quibi_oauth.png&quot; /&gt;
	&lt;figcaption&gt;This is what the Quibi OAuth request/response look like with the private stuff removed&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I won’t go too much into the way that Quibi does authentication, as it appears to be a pretty basic implementation of OAuth 2.0, using auth0 as a service provider. It doesn’t appear to use a client secret key, as I’m able to just replay the request with my username, password, client id and other parameters and get a valid access token back. One interesting thing to note here is how short the token expiration is, just 2700 seconds, or 45 minutes.&lt;/p&gt;

&lt;p&gt;Once authentication happens, the access token is used in subsequent requests as the authorization bearer header tokens.&lt;/p&gt;

&lt;p&gt;I didn’t bother to set up a new app with authentication because I wanted to get to the meat of the app, and started taking a look at how the app’s requests and responses were structured, which brings us to…&lt;/p&gt;

&lt;h3 id=&quot;protocol-buffers&quot;&gt;Protocol Buffers!&lt;/h3&gt;

&lt;p&gt;I’ll be honest, I was initially pretty annoyed when I read the line,&lt;/p&gt;

&lt;p&gt;&lt;code&gt;content-type: application/protobuf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;in Charles. That’s because &lt;a href=&quot;https://developers.google.com/protocol-buffers&quot;&gt;protocol buffers&lt;/a&gt; are pretty annoying to work with unless you actually have access to the .proto files that were used to generate the schema for the response.&lt;/p&gt;

&lt;p&gt;Luckily, I have experience working with protobufs (as the cool kids call them) because we started implementing them at Lyft last year. As a primer, protocol buffers basically define requests and responses, and their types, in a way that can be shared between servers, clients, etc. It does so in a way that reduces the amount of redundancy in the data. So instead of looking at a JSON file that labels each key/value pair for each item in an array, you just see the values, and the keys (and their types) are essentially encoded outside of the format itself. That’s probably a gross simplification of protobufs, but that’s basically how I understand their functionality in lay person terms.&lt;/p&gt;

&lt;p&gt;So getting back to Quibi, I was seeing calls to endpoints like&lt;/p&gt;

&lt;p&gt;&lt;code&gt;https://qlient-api.quibi.com/quibi.qlient.api.home.Home/GetHomeCards&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;with a bunch of wacky characters, urls, and names and descriptions of tv shows. Looking at the raw text gave me an idea of what was being returned, but there was also a lot of data that I couldn’t see because it was encoded as a protobuf response.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/protobuf_raw.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I found a tool written by someone named &lt;a href=&quot;https://github.com/mildsunrise&quot;&gt;Alba Mendez&lt;/a&gt; called &lt;a href=&quot;https://github.com/mildsunrise/protobuf-inspector&quot;&gt;protobuf-inspector&lt;/a&gt; which allows you to visualize the values in a protobuf response. Once I threw the response into this tool, it started to make a lot more sense.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/protobuf_inspector.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, I could see that the home cards were displaying structured information for each of the cards in the app. The hierarchy seemed to be a card with series info, a mp4 preview link, and then info about that particular episode. There were also some values that didn’t really seem useful, like “&amp;lt;varint&amp;gt; = 1” which could’ve meant anything, like a bool value or episode number.&lt;/p&gt;

&lt;p&gt;The tool has a way to set up object definitions, so that when you run it again, you see the key names and types that you defined. This is helpful if you’re trying to guess what something is and you want to compare it against a few different objects. Here’s what the response looked like once I tried defining some of the keys:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/protobuf_inspector_labeled.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This makes the response look a bit more logical. I really guessed some of these, so if someone from Quibi actually reads this maybe you can confirm. I probably spent more time on this than I needed, because really all I want is to show a list of shows and maybe even start watching a show.&lt;/p&gt;

&lt;p&gt;To waste even more time, I ended up defining this response in a .proto file, compiled it into Swift with swift-protobuf and got a Swift app to parse the response into real Swift structs! Here is the proto definition:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/ad4a62c9c1d86a2c187267bc353d8284.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The generated proto code in Swift is pretty big, so I’ll leave it as an exercise to the reader to run it through swift-protobuf.&lt;/p&gt;

&lt;p&gt;Here’s what it looks like running in the debugger in my app:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/proto_swift.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So with what I have written about so far, I could actually write a functioning Quibi client that supports logging in, making a request to get a list of tv show cards, and displays them, with a “live” preview, just like the real app. That doesn’t matter much unless you can actually watch the show, though.&lt;/p&gt;

&lt;h3 id=&quot;hls-and-video-streaming&quot;&gt;HLS and Video Streaming&lt;/h3&gt;

&lt;p&gt;I apologize for the anti-climactic finale of this blog post, but this is where I got stuck.&lt;/p&gt;

&lt;p&gt;Before I tried reverse engineering this app, I pretty much knew nothing about video streaming. When looking at the chain of requests that the app makes, it appears that the app hits an endpoint called “GetPlaybackInfo” and sends a payload of series id, season #, and episode # along with a mystery UUID that I haven’t seen anywhere else in the app requests/responses, then receives a link to a “license” url, a few links to .m3u8 resources and some cookies for accessing those .m3u8 resources.&lt;/p&gt;

&lt;p&gt;Then the app makes a request to the license url with some form encoded data and receives some other encoded data back. Finally, the app makes a request to one of the .m3u8 files and starts streaming the video.&lt;/p&gt;

&lt;p&gt;I did &lt;a href=&quot;https://developer.apple.com/streaming/&quot;&gt;some research&lt;/a&gt; and it looks like a .m3u8 url basically provides the client with a way or ways to display the video to the user. It can include things like different video streams with varying quality, and it looks like it even has some subtitle file support.&lt;/p&gt;

&lt;p&gt;I tried just replaying the call to the .m3u8 file with the same authentication cookie and it unfortunately didn’t work. I think that the license url provides the app with a way to decode the video, and without knowing what to send or how to decode it, I think I’m essentially stuck.&lt;/p&gt;

&lt;p&gt;I sort of didn’t expect to be able to finish this app anyway, so I’m pretty happy with how far I got. I also figure that if I try to go any further with this, Quibi will probably try to sue me or something, so it probably isn’t worth it. In any case, I did learn a lot from this project, and hopefully you have too, from reading this post. If you have any ideas on how I would proceed or if you enjoyed this post, feel free to let me know!&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/Quibi_Logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ve had a lot of free time in the past few weeks so I decided to spend some of it working on side projects. I really enjoy reverse engineering apps, so I decided to take a look at Quibi.&lt;/p&gt;

&lt;p&gt;Quibi (short for “Quick Bites”) is a video streaming app/service that has a bunch of shows that are short. The idea is that you can sit on a bus ride and consume an episode or two, depending on how long your commute is. One of the constraints of the platform is that you can only watch these videos on a phone or tablet with the app installed.&lt;/p&gt;

&lt;p&gt;Since everyone is stuck indoors for a while this constraint is kinda stupid and most people would probably like to watch their videos on their big tv rather than huddle around a phone, which is what Emily and I had to do to watch that stupid viral show about the &lt;a href=&quot;https://twitter.com/zachraffio/status/1250273191810875392&quot;&gt;terrible wife with the golden arm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, I had an idea to write a tvOS app that would work with Quibi, so you could watch your terrible shows on your tv. Here’s what I learned trying to reverse engineer the Quibi app.&lt;/p&gt;

</description>
        
        <pubDate>Tue, 12 May 2020 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2020/05/12/reverse-engineering-quibi/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2020/05/12/reverse-engineering-quibi/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Cloning Zwift on iOS Part 4: Workout View</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workoutview.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I haven’t been working as much on my workout app since it’s been good enough for me to use as a replacement for Zwift with the most recent changes I wrote about.&lt;/p&gt;

&lt;p&gt;I got a new iPhone so I had to build and install the app on my phone again, and after firing up Xcode I decided to go ahead and add a few more things that I was meaning to add to the app. The app hasn’t had a good visualization of workouts so I ended up creating an interface for that.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;taking-cues-from-zwift&quot;&gt;Taking cues from Zwift&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/emilysshortmix.jpg&quot; /&gt;
	&lt;figcaption&gt;Emily&apos;s Short Mix, as seen in Zwift&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I didn’t want to reinvent the wheel here, so I decided to just copy how Zwift shows workouts, because it works pretty well. Not that I’m trying to write a whole 3d cycling MMORPG, but it helps to know where I am in a workout besides time elapsed and time remaining.&lt;/p&gt;

&lt;p&gt;The general idea is to code each segment with its color based on the difficulty zone, and its height based on the target wattage of that segment.&lt;/p&gt;

&lt;p&gt;I previously noted that the target wattage of a segment is based on the product of your own FTP and a multiplier. I did some reverse engineering and found that the color coded “zones” are based on this formula:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;0-60% = zone 1&lt;/li&gt;
  &lt;li&gt;61-75% = zone 2&lt;/li&gt;
  &lt;li&gt;76-89% = zone 3&lt;/li&gt;
  &lt;li&gt;89-104% = zone 4&lt;/li&gt;
  &lt;li&gt;105-118% = zone 5&lt;/li&gt;
  &lt;li&gt;119%+ = zone 6&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I then just mapped the zones with the same colors, with zone 1 being gray, 2 being blue, etc. For the warmup and cooldown segments I just take the high power of those segments.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/5a3c38401b0f8a5dffbaf543bdd67e31.js&quot;&gt;&lt;/script&gt;

&lt;h3 id=&quot;creating-a-view&quot;&gt;Creating a View&lt;/h3&gt;

&lt;p&gt;Creating custom views from scratch is really not my strong point, but I know enough about autolayout and UIKit to implement whatever views a designer throws at me. Unfortunately I don’t have designer to throw designs at me so I had to come up with something on my own.&lt;/p&gt;

&lt;p&gt;I was originally going to use a .xib file and set things up programmatically, but I realized that I really didn’t need a .xib and could do everything programmatically. I usually go with a hybrid approach to make the complicated parts easier, but in this case doing stuff in code is actually less complicated.&lt;/p&gt;

&lt;p&gt;I ended up going with a stack view based layout, where I inserted the views into a horizontal stack view. The view itself has a width anchor based on the percentage of time it takes (if it’s a 10 minute segment in a 30 minute workout then its width would be 33.3% of the container), and then a subview which has a height anchor based on the relative intensity of the segment. It works out pretty well.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/1166ad11e921cb21470feed64ffb9e3f.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;I threw the view into a UITableViewCell so I could visually get a clue what the workout I was selecting looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workoutslist.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;finally-some-progress&quot;&gt;Finally Some Progress&lt;/h3&gt;

&lt;p&gt;I was pretty happy with the stack view representation of the workouts and left it that way for a few months. Today I was refactoring some stuff and implemented something that was really bothering me for a while. In Zwift you get a really nice view of the upcoming segments, and where you are in your current segment. My app is not really good at this, and there was a bug where it not correctly display the upcoming segment sometimes because of the way it handled workout segment equality. Essentially it was looking for the index of a segment but if there were two exact same segments it would assume the first one was the correct one.&lt;/p&gt;

&lt;p&gt;Anyway, I fixed that bug by just tracking the actual index of the segment I was on.&lt;/p&gt;

&lt;p&gt;The other thing I wanted to add was a progress view, so I broke the Workout view into its own class and added a “progress” indicator, which is just a horizontal slider that slides over as you’re working out.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/c5963ec63481f39877e562da3c93a13a.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;This is what it looks like, if I was working out at 10x the regular speed (which would be easier I guess).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/ZSwiftWorkoutView.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s about it for now! I think this makes my app pretty usable and until I find another thing I need to add to it, I feel like this is a good replacement for Zwift. If you happen to need the exact same thing that I do and also happen to have the same exact exercise bike, feel free to clone the &lt;a href=&quot;https://github.com/hungtruong/Zswift&quot;&gt;Github&lt;/a&gt; repo and use the app!&lt;/p&gt;

</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workoutview.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I haven’t been working as much on my workout app since it’s been good enough for me to use as a replacement for Zwift with the most recent changes I wrote about.&lt;/p&gt;

&lt;p&gt;I got a new iPhone so I had to build and install the app on my phone again, and after firing up Xcode I decided to go ahead and add a few more things that I was meaning to add to the app. The app hasn’t had a good visualization of workouts so I ended up creating an interface for that.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 29 Sep 2019 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2019/05/03/making-a-zwift-clone-part-4-workout-view/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2019/05/03/making-a-zwift-clone-part-4-workout-view/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Introducing BarkMode: Bark Detection + Dark Mode</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barkmode.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was in California about a month ago for work and I was able to attend a few events during WWDC week. I read a lot about the new features and APIs but didn’t really have a lot of time to mess with stuff like SwiftUI, etc. I got a stupid idea for a project but I didn’t really take the time to work on it until this weekend. Now I’d like to introduce you to my latest app: Bark Mode!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;wtf-is-bark-mode&quot;&gt;WTF is Bark Mode?&lt;/h3&gt;

&lt;p&gt;Right after the new APIs were announced at WWDC, I scanned them for anything that might be interesting. I really like being one of the first to try out a new technology, but I figured that everyone would be all over &lt;a href=&quot;https://developer.apple.com/xcode/swiftui/&quot;&gt;SwiftUI&lt;/a&gt;, and I was right. I found a somewhat obscure new feature as part of CreateML that allowed users to generate a ML model that could classify sounds.&lt;/p&gt;

&lt;center&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;This SoundAnalysis API in particular seems relevant to my typically immature interests. 💨 &lt;a href=&quot;https://twitter.com/hashtag/WWDC19?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#WWDC19&lt;/a&gt; &lt;a href=&quot;https://t.co/0Aipwa99Lu&quot;&gt;https://t.co/0Aipwa99Lu&lt;/a&gt;&lt;/p&gt;&amp;mdash; Hung Truong (@hungtruong) &lt;a href=&quot;https://twitter.com/hungtruong/status/1135947285035020290?ref_src=twsrc%5Etfw&quot;&gt;June 4, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; 
&lt;/center&gt;

&lt;p&gt;Of course the first thing I thought of was to make an app that could detect farts. I could even make it so that a fart would toggle dark mode off/on. Though I wanted to start experimenting right away, I discovered that you actually needed to have the newest version of macOS Catalina installed in addition to the newest Xcode to use Create ML, and I didn’t have my personal computer handy and couldn’t install a beta OS on my work computer, so I ended up not building it.&lt;/p&gt;

&lt;p&gt;Fast forward to this weekend, and I eventually got Catalina and all the other prerequisites I needed to get started. Instead of “FarkMode” which doesn’t really make sense, I decided to switch it up this time and detect dogs barking, since “Bark” rhymes better with “Dark” than “Fart.” Plus everybody loves dogs! Perhaps I’m finally maturing in my old age (probably not).&lt;/p&gt;

&lt;p&gt;Anyway, here’s the steps I needed to take to make my BarkMode app.&lt;/p&gt;

&lt;h3 id=&quot;creating-the-model&quot;&gt;Creating the model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/Create ML.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I had seen some videos of people making image classifiers using &lt;a href=&quot;https://developer.apple.com/documentation/createml&quot;&gt;CreateML&lt;/a&gt; in the past. It seemed as easy as creating a few folders with different labels and images of those things. You would have to create one batch for training images, and another for optionally testing the models once they were created.&lt;/p&gt;

&lt;p&gt;The process for creating an audio classifier is pretty similar except that you need to use sound files instead of images. I started by finding a bunch of stock sound effects of dogs barking. I also found a bunch of random stock sound effects of things like bells ringing, wind blowing, and other stuff that I could label as “not barking.” I feel like this is a pretty wacky way to do classification but if there’s another way that doesn’t involve downloading a bunch of random stock sound effects, that would be awesome.&lt;/p&gt;

&lt;p&gt;At first I took larger sound files and chopped them up into really short ones. This way I had a bunch of training data that I can use both to train and test my classifier.&lt;/p&gt;

&lt;p&gt;However, when I ran the Create ML app on the training data, I got an obscure error that the training had been interrupted. I saw that the app thought I only had 3 files when I had created a bunch more. I believe that Create ML is unable to train with really short audio files (less than a second). I ended up inserting the full sound files instead of the short ones. At this point I think I had about 8 or 9 files for either barking or not barking.&lt;/p&gt;

&lt;p&gt;The model was created but it had a really terrible accuracy. I decided to just add a bunch more files both of dogs barking and things that weren’t dogs barking. The more examples I added the “better” the model became.&lt;/p&gt;

&lt;h3 id=&quot;polishing-the-model&quot;&gt;Polishing the model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barktest.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Create ML has a feature where you can test your audio classifier with audio data coming from your own microphone. When I did this I noticed that the model was too biased towards no sound being barking. I’m not sure why but maybe white noise sounds more like barking than an old timey car horn?&lt;/p&gt;

&lt;p&gt;I ended up adding a bunch of white noise and recorded some nothing to add to the model. This helped quite a bit. The last thing I did was also record some audio of myself talking so that the model would not recognize me talking as barking. This was useful so I wouldn’t get false positives during my demo video (which I ended up getting anyway in a bunch of outtakes).&lt;/p&gt;

&lt;h3 id=&quot;integrating-the-model-in-the-app&quot;&gt;Integrating the model in the app&lt;/h3&gt;

&lt;p&gt;I created a new app and promptly enabled “Use SwiftUI.” Then I promptly made a new project because I couldn’t figure out where my ViewController class was! I’ll take a look at SwiftUI later but for now I’ll focus on getting my BarkMode app actually working.&lt;/p&gt;

&lt;p&gt;The Create ML app made a model that I could then copy into my app. It was really as simple as dragging it from one place to another.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/mlmodel.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking at the model in Xcode, it describes an interface with an input of “audioSamples” that takes a MultiArray of (Float32 15600). I assumed this had something to do with the number of samples per second and the bitrate of the audio. I fiddled around with the AudioToolbox framework and a few other lower level audio APIs until I discovered Apple’s documentation on the &lt;a href=&quot;https://developer.apple.com/documentation/soundanalysis&quot;&gt;SoundAnalysis&lt;/a&gt; framework which provides a much, much easier method of feeding audio to the model.&lt;/p&gt;

&lt;p&gt;I implemented the steps described and extended my ViewController to conform to the SNResultsObserving protocol. Then it was a few simple steps to write some logic to handle the app detecting no barking to barking and back to no barking, and toggling the dark mode off and on.&lt;/p&gt;

&lt;p&gt;Finally, I added a label and an image view that takes an image with different assets for the light and dark modes, which changes automatically based on the current setting.&lt;/p&gt;

&lt;p&gt;The model runs pretty quickly. Here’s a gif of the terminal output from the logging whenever some sound data comes in:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barknobark.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you want to see the app in action, I created this video to demonstrate:&lt;/p&gt;

&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/0XRxA1cEHio&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;p&gt;As it is, this app isn’t very useful for much of anything, but I could see a few potential uses for the model I trained:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Write an app that detects your dog is unhappy when it’s at home and triggers a home automation action like playing music or shooting a treat at your dog&lt;/li&gt;
  &lt;li&gt;Detect if suspicious sound at your house is a robber or just a dog&lt;/li&gt;
  &lt;li&gt;Classify different types of barks and make a dog barking translation app&lt;/li&gt;
  &lt;li&gt;For people who are allergic to dogs, advance warning of incoming dog&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I’ve posted the full project to &lt;a href=&quot;https://github.com/hungtruong/BarkMode&quot;&gt;Github&lt;/a&gt; in case you’re curious about how I implemented the app. I didn’t upload the Create ML project but it just has a bunch of sound effect files and files of me saying gibberish. The model included in the Github repo should work fine anyway.&lt;/p&gt;

&lt;p&gt;I’m pretty happy with my end result. It’s fun to play around with new APIs and now I can say that I’ve trained an ML model to classify sounds, in addition to implementing Dark Mode! Hopefully I’ll have some time to play with other APIs that were introduced to iOS soon.&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barkmode.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was in California about a month ago for work and I was able to attend a few events during WWDC week. I read a lot about the new features and APIs but didn’t really have a lot of time to mess with stuff like SwiftUI, etc. I got a stupid idea for a project but I didn’t really take the time to work on it until this weekend. Now I’d like to introduce you to my latest app: Bark Mode!&lt;/p&gt;

</description>
        
        <pubDate>Sun, 14 Jul 2019 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2019/07/14/introducing-bark-mode-bark-detection-dark-mode/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2019/07/14/introducing-bark-mode-bark-detection-dark-mode/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Cloning Zwift on iOS Part 3: HealthKit and a WatchOS App!</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/healthkit.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ve been a bit slow to update my blog series about trying to make a clone of
Zwift, but not because I’ve stopped working on it. Rather, I’ve been able to use
the “MVP” of what I’ve built so far in parts
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;1&lt;/a&gt;
and
&lt;a href=&quot;/blog/2019/04/07/making-a-zwift-clone-part-2/&quot;&gt;2&lt;/a&gt;,
and I was finding that the time I spent working on my app could be used actually
working out. Like, I literally would write an implementation of something, but
it would take so much of my time that I couldn’t test it out and I’d have to go
to bed… Still, I was missing a few important features in my app, so I’ve been
slowly working on them in between working on my fitness.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;hooking-up-the-apple-watch&quot;&gt;Hooking up the Apple Watch&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwift-bluetooth.png&quot; /&gt;
	&lt;figcaption&gt;Zwift’s interface for connecting the Apple Watch works really well&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;One of the great things about Zwift is how much support they provide for
different fitness accessories, including the Apple Watch. Unfortunately, the
Apple Watch hardware is not set up to allow arbitrary Bluetooth connections like
my exercise bike was in part 1. Instead, to access the user’s health information
like heart rate, you need to write a full blown WatchOS app!&lt;/p&gt;

&lt;p&gt;Luckily, this wasn’t my first rodeo as I worked on the Apple Watch app for
Starbucks, so I was able to add a Watch app extension target in my project
pretty quickly.&lt;/p&gt;

&lt;p&gt;I Googled for how to get a user’s heart rate programmatically, came across a
&lt;a href=&quot;https://stackoverflow.com/questions/38158841/live-heart-rate-in-watchos-3&quot;&gt;promising StackOverflow
post&lt;/a&gt;
with a link to a &lt;a href=&quot;https://github.com/coolioxlr/watchOS-3-heartrate/blob/master/VimoHeartRate%20WatchKit%20App%20Extension/InterfaceController.swift&quot;&gt;Github
project&lt;/a&gt;,
and was able to get it implemented myself. However, as I looked at the
copy-pasta code, it seemed sort of wrong to me. The code was starting a workout
session, but then created an object query that would run a closure whenever a
new heart rate sample (older than a certain date) was added by the workout. This
seems like a really roundabout way to get heart rate samples, and I wondered if
Apple had a better API to accomplish this.&lt;/p&gt;

&lt;h3 id=&quot;implementing-healthkit&quot;&gt;Implementing HealthKit&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/healthkit_hero.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I ended up finding some Apple &lt;a href=&quot;https://developer.apple.com/documentation/healthkit/workouts_and_activity_rings/speedysloth_creating_a_workout&quot;&gt;sample
code&lt;/a&gt;
that showed a better way to fetch heart rate data. The solution is to use some
new features introduced in WatchOS 5 that allow for creation of a workout
directly on the Apple Watch. The Apple doc I linked explains it pretty well, but
the steps are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ask the user for permission to track their heart rate data&lt;/li&gt;
  &lt;li&gt;Create a workout configuration (e.g. an indoor cycling workout) and a workout
session, along with its associated workout builder&lt;/li&gt;
  &lt;li&gt;Start the session and tell the workout builder to start collecting data samples&lt;/li&gt;
  &lt;li&gt;Respond to the delegate method &lt;em&gt;“workoutBuilder(_:didCollectDataOf:)”&lt;/em&gt; to collect
a bunch of samples, including heart rate information&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In code it looks something like this:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/66b941af4f5ce3b9a66cd041fd94f3d5.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/918839f7ba4a213c92817b387a6fd91f.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Rather than add some UI to the watch app to start a workout session, the iPhone
version of &lt;em&gt;HKHealthStore&lt;/em&gt; has a function called
&lt;a href=&quot;https://developer.apple.com/documentation/healthkit/hkhealthstore/1648358-startwatchapp&quot;&gt;startWatchApp(with:completion:)&lt;/a&gt;
which will send a workout configuration to the watch to facilitate the creation
of a workout. All I need to do is call that function when my workout on the
iPhone app starts and my watch app will respond by starting a HealthKit workout
session which starts measuring things like heart rate (and calculating its own
estimated calories burned).&lt;/p&gt;

&lt;p&gt;I was now able to get the heart rate as the watch was reading it, and update
whatever display on the watch I wanted to. That was only half the story though.
In Zwift the heart rate shows up in the user interface, and I wanted to mimic
that myself. Since I couldn’t access the workout session directly from the phone
I’d have to send the heart rate info back to the main app from the watch.&lt;/p&gt;

&lt;h3 id=&quot;back-to-the-app&quot;&gt;Back to the App&lt;/h3&gt;

&lt;p&gt;This blog post isn’t about Watch apps, so I won’t go over that aspect of this
feature too much. I basically used the WatchConnectivity session to send
messages back to the app with a dictionary containing the new heart rate.&lt;/p&gt;

&lt;p&gt;And after all of that programming, I’d like to present you with the most
difficult video I’ve ever shot: on an iPad, while balancing on an exercise bike,
recording both my wrist watch with my heart rate AND the app showing the exact
same heart rate!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/zwift-part-3.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also rigged up an initial interface that shows which workout segment I’m on,
the next segment coming up, the progress through the segment and my progress for
the entire workout, along with stats like calories burned (determined by the
bike), cadence and distance traveled.&lt;/p&gt;

&lt;p&gt;At this point I have a pretty functional app! But seeing the extensive APIs of
HealthKit made me want to add more and more to my app. This is scope creep in
action. See the documentation of
&lt;a href=&quot;https://developer.apple.com/documentation/healthkit/hkworkoutbuilder&quot;&gt;HKWorkoutBuilder&lt;/a&gt;
to see all of the data and metadata you can store. I ended up sending a few more
messages from the app back to the watch so I could store more data to the
workout:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/a45a91a9d598a448db7ce3947d70304d.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;At the end of a workout, I send the start and end times, along with the total
calories burned and distance traveled. This isn’t really necessary because the
watch already makes a guess about the calories burned and the distance isn’t
real because it’s on a stationary bike. But I thought it might be interesting to
see how that data is represented.&lt;/p&gt;

&lt;p&gt;I also toyed around with sending segment data but I haven’t seen it visually
represented anywhere in the workout view. I wanted to see more detail about the
workout in the Apple Activity app so I also sent the name of the workout as the
&lt;em&gt;HKMetadataKeyWorkoutBrandName&lt;/em&gt; value, though I’m not sure that’s what it’s
intended for! Here’s what the workouts look like in the Activity app and the
Health app’s workout data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workout.png&quot; /&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2019/sample.png&quot; /&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2019/sample2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One more fun but optional thing I thought of and added was a wrist tap reminder
when I got close to the end of a segment. Sometimes I’m just in the zone and not
paying attention to the fact that I need to ramp up or ramp down for the next
section, so when there’s 5 seconds left in a segment I send a message to the
watch from the phone to tap my wrist and send a reminder:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/9ebc2061b51cd3b13b3acffbb9930398.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;One of the nice things about writing your own workout app is that you don’t need
to wait for a third party developer to implement any ideas you have for the app!
I think that’s actually the only nice thing…&lt;/p&gt;

&lt;p&gt;Anyway, I’m pretty happy with the results. Next up, I plan on adding a bit of
visual polish to the interface and maybe even create an app icon! I also want to
aggregate the data like heart rate info, watt effort, etc and keep track of
statistics and chart the data, perhaps in real time. I find it very motivating
to compare my effort in the same exact workout across different days to see if
I’m improving (maybe by measuring heart rate average).&lt;/p&gt;

&lt;p&gt;As usual, my changes are in &lt;a href=&quot;https://github.com/hungtruong/Zswift&quot;&gt;Github&lt;/a&gt; in
case you happen to have the same exact exercise bike as me or are curious how I
implemented certain things.&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/healthkit.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ve been a bit slow to update my blog series about trying to make a clone of
Zwift, but not because I’ve stopped working on it. Rather, I’ve been able to use
the “MVP” of what I’ve built so far in parts
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;1&lt;/a&gt;
and
&lt;a href=&quot;/blog/2019/04/07/making-a-zwift-clone-part-2/&quot;&gt;2&lt;/a&gt;,
and I was finding that the time I spent working on my app could be used actually
working out. Like, I literally would write an implementation of something, but
it would take so much of my time that I couldn’t test it out and I’d have to go
to bed… Still, I was missing a few important features in my app, so I’ve been
slowly working on them in between working on my fitness.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 03 May 2019 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2019/05/03/making-a-zwift-clone-part-3/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2019/05/03/making-a-zwift-clone-part-3/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Making an iOS Zwift Clone to Save $15 a Month! Part 2: Reverse Engineering a Workout</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwiftworkoutheader.png&quot; /&gt;
	&lt;figcaption&gt;A very colorful Zwift Workout&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Last time, on “Making an iOS Zwift Clone to Save $15 a Month” I wrote about
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;learning Core Bluetooth to connect to my exercise bike and get data streaming
directly to my
app&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since writing that article, I cleaned up the implementation of the Core
Bluetooth service a bit and started supporting some additional data like
distance, calories burned and cycling cadence.&lt;/p&gt;

&lt;p&gt;While cycling on my exercise bike and staring at these numbers is fun, the
built-in screen on my bike already shows these numbers, so I essentially
recreated a subset of the official
&lt;a href=&quot;https://www.concept2.com/service/software/ergdata&quot;&gt;ergData&lt;/a&gt; app so far.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/ergdata.jpg&quot; /&gt;
	&lt;figcaption&gt;The ergData app is functional but ugly af&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I realized the next challenge would be to start a guided workout in my app and
show the target wattage alongside my actual wattage on the bike.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;enter-the-workout&quot;&gt;Enter the Workout&lt;/h3&gt;

&lt;p&gt;Zwift workouts seem pretty simple to begin with. You can set up a number of
workout segments to guide your workout. There are warmups, steady states,
intervals, free ride sections and cool-downs. At any given point in your
workout, Zwift will tell you to put in a certain amount of effort. You just try
to get your watt number to match what Zwift wants you to do.&lt;/p&gt;

&lt;p&gt;At first I figured I could come up with my own way of representing workouts, but
that would also require me to re-create all of the workouts I usually use on
Zwift. Zwift also has a nice workout editor that can use to edit preset workouts
or create completely new ones on your own.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwiftworkoutedit.png&quot; /&gt;
	&lt;figcaption&gt;Zwift’s built-in workout editor is pretty nice!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I discovered that if you copy an existing workout to edit it, the workout will
be saved as user data and will be synced between all devices. How is the workout
data saved, you ask? No, not as JSON as a sane developer would use. It’s XML.
Okay, so technically it’s a .zwo file but I know what XML looks like! Here’s a
sample of an exported workout file:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwofile.png&quot; /&gt;
	&lt;figcaption&gt;Hello darkness, my old friend&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So I went ahead and implemented an XMLParser in Swift to turn .zwo files into my
own Workout struct. I won’t bore you with the implementation details but it was
kind of a walk down memory lane dealing with the delegation-based API of
XMLParser. I’m surprised there isn’t a more modern solution in Foundation but
I’m guessing it’s because most people have moved on from XML.&lt;/p&gt;

&lt;h3 id=&quot;lets-reverse-engineering&quot;&gt;Let’s Reverse Engineering&lt;/h3&gt;

&lt;p&gt;By reading the .zwo files, I was able to figure out how Zwift represents their
workouts and basically reverse engineer the structure of a workout:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Warmups have a duration, a low and high power (which is a percentage of
&lt;a href=&quot;https://zwift.com/news/4100-zwift-how-to-understanding-finding-your-ftp&quot;&gt;FTP&lt;/a&gt;).
The warmup begins at the low power and steadily ramps up to the high power by
the end of the segment.&lt;/li&gt;
  &lt;li&gt;SteadyStates are the simplest, with just a duration and power.&lt;/li&gt;
  &lt;li&gt;Intervals are the most complicated, with a repeat number, onDuration,
offDuration, onPower and offPower. The intervals segment goes between the high
and low power levels, and repeats a certain number of times. I didn’t want to
deal with the extra complexity of this type so I just wrote some code to expand
them out into SteadyStates.&lt;/li&gt;
  &lt;li&gt;Cooldowns are like warmups but in reverse.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are also some other things like text captions that appear during a
workout, free ride segments and cadence settings that I didn’t bother to
implement because I don’t use them. The cadence stuff makes sense if you’re
using a smart trainer but my bike doesn’t support automatically changing
resistance anyway.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/55e92115f993221693f9040431ee39fe.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;I decided to represent the WorkoutSegment as a Swift enum since it needs to
support a bunch of different formats with varying levels of complexity. I’m
actually kinda curious how the Zwift team ended up representing these and if
they did something similar.&lt;/p&gt;

&lt;p&gt;From here it’s pretty simple to get stuff like the total duration or the target
wattage for a particular time offset, whether I need to calculate that (in the
case of a warmup or cooldown) or if I just return the constant value.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/718f50f48c32b324a2f25a80c78ae46b.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Aside from representing the workout segments, the actual workout object will
consist of things like the name, description, FTP and other information that
pertains to the state of the workout like time elapsed. I wrote a function that
takes the current time elapsed in the workout, loops through and finds the
corresponding segment and returns the desired wattage. There might be a more
efficient way to do that but for now it works.&lt;/p&gt;

&lt;p&gt;I also added some things to my project for displaying and choosing workouts from
a list, and actually displaying the target wattage alongside actual wattage. I
want to improve the interface now since it’s terrible, but I’ll save that for
another blog post. If you want to check out the project, &lt;a href=&quot;https://github.com/hungtruong/Zswift&quot;&gt;it’s all here on
Github&lt;/a&gt;, including the playgrounds I used
to test out the workout segment logic before adding it to the Xcode project.&lt;/p&gt;

&lt;p&gt;Next on my list of tasks is to make the workout interface a lot prettier/usable,
and to add heart rate information from my Apple Watch.&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwiftworkoutheader.png&quot; /&gt;
	&lt;figcaption&gt;A very colorful Zwift Workout&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Last time, on “Making an iOS Zwift Clone to Save $15 a Month” I wrote about
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;learning Core Bluetooth to connect to my exercise bike and get data streaming
directly to my
app&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since writing that article, I cleaned up the implementation of the Core
Bluetooth service a bit and started supporting some additional data like
distance, calories burned and cycling cadence.&lt;/p&gt;

&lt;p&gt;While cycling on my exercise bike and staring at these numbers is fun, the
built-in screen on my bike already shows these numbers, so I essentially
recreated a subset of the official
&lt;a href=&quot;https://www.concept2.com/service/software/ergdata&quot;&gt;ergData&lt;/a&gt; app so far.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/ergdata.jpg&quot; /&gt;
	&lt;figcaption&gt;The ergData app is functional but ugly af&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I realized the next challenge would be to start a guided workout in my app and
show the target wattage alongside my actual wattage on the bike.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 07 Apr 2019 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2019/04/07/making-a-zwift-clone-part-2/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2019/04/07/making-a-zwift-clone-part-2/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Making an iOS Zwift Clone to Save $15 a Month! Part 1: Core Bluetooth</title>
        <description>&lt;p&gt;It’s been a while since I’ve worked on a personal project, but I’ve been having an itch to make some new iOS apps and yesterday morning I decided to go ahead and hack something together.&lt;/p&gt;

&lt;p&gt;I recently purchased an exercise bike called the &lt;a href=&quot;https://shop.concept2.com/bikeerg/601-bikeerg.html&quot;&gt;BikeErg&lt;/a&gt; (I think the name has something to do with the rowing machines that the manufacturer also makes). The bike has a built-in computer that keeps track of things like watts (apparently cycling is a sport that has really good analytics since it’s easy to track raw power), calories burned, cadence and other stuff. You can view the data on the monitor or use an app like &lt;a href=&quot;https://zwift.com&quot;&gt;Zwift&lt;/a&gt; to do workouts.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/bikeerg.jpg&quot; /&gt;
	&lt;figcaption&gt;The BikeErg comes with the PM5: the most advanced PM thing ever.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!--more--&gt;

&lt;p&gt;I’ve been using the BikeErg to exercise pretty regularly now, and I tried a bunch of different apps that can connect to it. Zwift is pretty much the gold standard as it has many features like 3D avatars and environments, a rich community, and lots of different workout plans for you to try. Zwift integrates with apps like MyFitnessPal and Strava, too, so I can trick people into thinking that I’ve ridden in Central Park one day and London the next.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwift.jpeg&quot; /&gt;
	&lt;figcaption&gt;Studies have shown that riding a bike in a completely white room really builds your FTP&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;While I think the feature set of Zwift is really compelling, I’m more of an old school app user. I don’t really care about the online community. I don’t really need to look at my avatar riding his bike around a futuristic city or an exploding volcano. I just want to do some directed workouts and maybe track my heart rate and my calories burned. The price of $15 a month is probably fine for people who use all of those features and get the value out of it, but I feel like I do not. &lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwift_screenshot.jpeg&quot; /&gt;
	&lt;figcaption&gt;It’s my virtual dude riding through a virtual New York with all his virtual pals&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Just to be clear here, I do think app developers deserve to be paid for their work and it’s definitely within reason for Zwift to charge this subscription given the sheer amount of support they need to provide to all of their users’ varying setups. After just implementing a small proof of concept, I have some mad respect for their dev team.&lt;/p&gt;

&lt;p&gt;However, I am cheap and I’m an iOS developer so I figured, “maybe I can roll my own fake Zwift!”&lt;/p&gt;

&lt;h3 id=&quot;enter-corebluetooth&quot;&gt;Enter CoreBluetooth&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/core_bluetooth.png&quot; /&gt;
	&lt;figcaption&gt;The more I stare at this image the less sense it makes&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I’ve been interested in Bluetooth development ever since CoreBluetooth was added to the iOS 5.0 SDK (I think the first supported device was the iPhone 4s). But every time I tried to sit down and read the documentation I got discouraged by the complexity and ended up getting distracted by some other new shiny API. Since I had a desired use case here: Make a Zwift alternative for myself, I was able to focus up some more and get something working.&lt;/p&gt;

&lt;p&gt;While the Bluetooth protocol is incredibly flexible, that flexibility also makes it incredibly complicated to get even a simple proof of concept working. If you don’t know what the special Bluetooth jargon means, it can seem really confusing. I still don’t really understand all of it but I’ve managed to hack something together that will serve as a basis for my fake Zwift app.&lt;/p&gt;

&lt;p&gt;Rather than bore you with the technical jargon and steps required to make this app, I’d rather just go through my process of figuring it out, which may be slightly more interesting.&lt;/p&gt;

&lt;h3 id=&quot;of-course-its-called-a-manager&quot;&gt;Of course it’s called a “Manager”&lt;/h3&gt;

&lt;p&gt;So the first thing I did was go to &lt;a href=&quot;https://developer.apple.com/library/archive/documentation/NetworkingInternetWeb/Conceptual/CoreBluetooth_concepts/AboutCoreBluetooth/Introduction.html#//apple_ref/doc/uid/TP40013257&quot;&gt;this document&lt;/a&gt; (which I guess is deprecated now but I didn’t notice that message when I was reading it) which goes over the Core Bluetooth framework.&lt;/p&gt;

&lt;p&gt;I found out that I needed to create a CBCentralManager, so I did that and then I tried to scan for some Bluetooth devices:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;let centralManager = CBCentralManager()&lt;br /&gt;
self.centralManager.scanForPeripherals(withServices: nil, options: nil)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I immediately got an error that I couldn’t do that since the centralManager wasn’t powered on yet. Oops! I then set the delegate of the centralManager and waited for the method “centralManagerDidUpdateState” to check that it was powered on before scanning.&lt;/p&gt;

&lt;p&gt;I soon started getting a bunch of peripherals in my next delegate method, “centralManager(_:didDiscover:advertisementData:rssi:)”&lt;/p&gt;

&lt;p&gt;Among the things I found were my laptop (over and over again even though the scan was set to not allow duplicates…), someone’s Bluetooth headset and various other things I couldn’t identify. Success! &lt;/p&gt;

&lt;p&gt;Once I filtered out the peripherals that kept on repeating, I was able to turn on the bike (by cycling a bit) and I got this message in my logs:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/peripherals.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;I successfully found my PM5. Now to connect to it and get the data. I ended up connecting to the PM5 based on the name. (After doing some reading it looks like I could connect based on the last service UUID of “CE060000-43E5-11E4-916C-0800200C9A66”). &lt;/p&gt;

&lt;p&gt;I called the “connect” function of the centralManager and later got an error because the peripheral wasn’t retained (I guess the Central doesn’t keep a strong reference, which makes sense). I tried again, this time keeping a reference to the peripheral in an array.&lt;/p&gt;

&lt;h3 id=&quot;peripherals-services-and-characteristics&quot;&gt;Peripherals, Services and Characteristics&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/central_manager.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Once I connected, I had to discover the peripheral’s services. And once that succeeded I had to discover each service’s characteristics. Once you discover those characteristics you can set the peripheral’s services’ characteristics to “notify” you when the characteristic changes. In more depth:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Connect to peripheral using the central’s “connect(_:options:)” method and retain it&lt;/li&gt;
  &lt;li&gt;Handle the “&lt;em&gt;centralManager(_:didConnect:)&lt;/em&gt;” delegate method where you set the peripheral’s delegate and call its “discoverServices(_:)” method&lt;/li&gt;
  &lt;li&gt;Handle the “peripheral(_:didDiscoverServices:)” delegate method and call the peripheral’s “discoverCharacteristics(_:for:)” for each service you want to discover characteristics for (why not all of them at this point?)&lt;/li&gt;
  &lt;li&gt;Handle the “peripheral(_:didDiscoverCharacteristicsFor:error:)” delegate method for each service’s characteristics you wanted to discover by calling the peripheral’s “setNotifyValue(_:for:)” method on each service’s characteristic that you want notifications for.&lt;/li&gt;
  &lt;li&gt;Optionally handle the “peripheral(_:didUpdateNotificationStateFor:error:)” method to see if you were able to successfully update the notification state for each peripheral’s service’s characteristic. In some cases I wasn’t able to ask for updates, perhaps those characteristics are just static data?&lt;/li&gt;
  &lt;li&gt;Handle the “peripheral(_:didUpdateValueFor:error:)” method to get the updated value for each characteristic that you wanted notifications for.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This all seems really convoluted to me and it was probably part of the reason that I always gave up on implementing Bluetooth in the past, but I think that’s more of a symptom of the complexity of the Bluetooth protocol than the CoreBluetooth API.&lt;/p&gt;

&lt;p&gt;Now all I needed to do was generate some data by cycling on the bike for a few seconds. I wasn’t quite finished yet, though. When the characteristics are updated and you start getting notified, you can inspect the new values, but those values are just &lt;a href=&quot;https://developer.apple.com/documentation/corebluetooth/cbcharacteristic/1518878-value&quot;&gt;Data objects&lt;/a&gt;. Each characteristic can hold a number of values based on how the data is structured, and that is up to whoever is implementing the Bluetooth protocol. &lt;/p&gt;

&lt;p&gt;I did some research and found &lt;a href=&quot;https://www.concept2.com/files/pdf/us/monitors/PM5_BluetoothSmartInterfaceDefinition.pdf&quot;&gt;this document&lt;/a&gt; that describes the Bluetooth specifications for the PM5 device.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/attribute_table.png&quot; /&gt;
	&lt;figcaption&gt;Just some really interesting light reading&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In that document were some tables including the one above which describes the UUID for a characteristic that includes things like elapsed time, calories, and most importantly, watts. I discovered that the data was being encoded into bytes, so I took the raw Data object and split it into an array of 8-bit Integers. Once I started printing those arrays I saw something like this:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/characteristics.png&quot; /&gt;
	&lt;figcaption&gt;I originally printed out the Base 64 string representation of the Data before reading the doc, which was a lot less useful&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Because the PM5 was originally set up for rowing machines, the documentation is a bit confusing. It refers to “strokes” which might line up with rpms on a bike? I was mainly interested in watts for my proof of concept so I found a few values in the document that mentioned watts. The table in the spec mentions “Stroke Power Lo (watts)” and has a “Stroke Power Hi” (what’s the difference?). I cobbled an interface together to test out my guess about the first value and here’s the video result:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/bikeerg_gif.gif&quot; /&gt;
	&lt;figcaption&gt;I took this video with an iPad and for once I’m not ashamed of that. Also it’s shaky cause I was cycling.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Success! I’m now able to connect my phone to my bike with my app. I have only gotten the wattage data from the bike so far, but reading through the spec it seems like there is a lot more I can pull via Bluetooth. I already know from using Zwift that I can get cadence from the bike, for example, and I saw a few other interesting things like calories, pace and distance traveled.&lt;/p&gt;

&lt;h3 id=&quot;every-journey-begins-with-a-single-corebluetooth-implementation&quot;&gt;Every Journey Begins With a Single CoreBluetooth Implementation&lt;/h3&gt;

&lt;p&gt;I titled this blog post “Part 1” in a series but I don’t know when the next step will be. My wishlist is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I want to eventually set up directed workouts in a similar fashion as Zwift&lt;/li&gt;
  &lt;li&gt;I also want to be able to track my heart rate which I can do by writing an Apple Watch app for my existing app&lt;/li&gt;
  &lt;li&gt;I want to be able to store my workout data and integrate with Apple Health&lt;/li&gt;
  &lt;li&gt;I want to import workouts or at least create them inside of the app&lt;/li&gt;
  &lt;li&gt;I want to chart the actual wattage of the bike against the guided wattage, and also show heart rate, and show histograms&lt;/li&gt;
  &lt;li&gt;I want to avoid feature creep&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I haven’t figured out which order to do these things in but for now I’ll continue to use Zwift since I already paid for the membership. My next step is probably to break out the code for connecting to the PM5 into its own project and make all of the data from it available in an easy to consume form. I’m kinda torn between that and just making the MVP for doing workouts.&lt;/p&gt;

&lt;p&gt;If I had to estimate, I probably spent more than 3 hours working on this project so far and more on writing this blog post. If I was to value my time based on what my contracting rate would be I’d probably be able to pay for more than a year of Zwift with it! So this project is really more about learning different iOS technologies than it is about saving money at this point.&lt;/p&gt;

&lt;p&gt;If you found this blog post interesting let me know! I wanted to write down my process so I could remember it, but hopefully it’s useful to anyone trying to implement CoreBluetooth. I found a bunch of sample code that connects to heart rate monitors but I didn’t find any that go through the process of writing code to a spec document. If you want to try to run this app yourself (and you happen to have the same exact bike as me), &lt;a href=&quot;https://github.com/hungtruong/Zswift/tree/blog_part_1&quot;&gt;check out the source code here&lt;/a&gt;.&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;It’s been a while since I’ve worked on a personal project, but I’ve been having an itch to make some new iOS apps and yesterday morning I decided to go ahead and hack something together.&lt;/p&gt;

&lt;p&gt;I recently purchased an exercise bike called the &lt;a href=&quot;https://shop.concept2.com/bikeerg/601-bikeerg.html&quot;&gt;BikeErg&lt;/a&gt; (I think the name has something to do with the rowing machines that the manufacturer also makes). The bike has a built-in computer that keeps track of things like watts (apparently cycling is a sport that has really good analytics since it’s easy to track raw power), calories burned, cadence and other stuff. You can view the data on the monitor or use an app like &lt;a href=&quot;https://zwift.com&quot;&gt;Zwift&lt;/a&gt; to do workouts.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/bikeerg.jpg&quot; /&gt;
	&lt;figcaption&gt;The BikeErg comes with the PM5: the most advanced PM thing ever.&lt;/figcaption&gt;
&lt;/figure&gt;

</description>
        
        <pubDate>Sun, 17 Mar 2019 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2019/03/17/making-a-zwift-clone-part-1/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2019/03/17/making-a-zwift-clone-part-1/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
  </channel>
</rss>
