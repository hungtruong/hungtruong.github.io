<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hung Truong: The Blog!</title>
    <description>I say potato, you say potato...</description>
    <link>https://www.hung-truong.com/blog/</link>
    <atom:link href="https://www.hung-truong.com/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 20 May 2023 14:47:53 -0700</pubDate>
    <lastBuildDate>Sat, 20 May 2023 14:47:53 -0700</lastBuildDate>
    <generator>Jekyll v4.2.1</generator>
    
      
      <item>
        <title>Voice Cloning For Fun and Profit</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2023/all_along_the_way.jpg&quot; /&gt;
	&lt;figcaption&gt;Two legendary directors doing the weather report&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Note: This blog post is based on a YouTube video I made, which you can watch in the embed, but it also has some stuff that‚Äôs not in the video, so read the stuff I wrote below!&lt;/p&gt;

&lt;center&gt;
	&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/OlbVWneM6xE&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;p&gt;I was recently stuck on a really long, 6 hour flight. Luckily, I had an internet connection the whole time, and I was testing out some nifty &lt;a href=&quot;https://amzn.to/3OryWcQ&quot;&gt;AR Glasses&lt;/a&gt;, so I watched a lot of YouTube.&lt;/p&gt;

&lt;p&gt;I remembered seeing a video about AI voice clone cover songs, where someone will take the voice of a known artist, and then make them sing a song by another artist (e.g. Kanye West singing ‚ÄúCall Me Maybe‚Äù), or they create a whole new original song for the artist (e.g. Drake singing an original called &lt;a href=&quot;https://www.nytimes.com/2023/04/19/arts/music/ai-drake-the-weeknd-fake.html&quot;&gt;‚ÄúHeart on My Sleeve‚Äù&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Since I was stuck on the plane, and I had the privacy to watch whatever I wanted without my seatmates judging me for binging on a bunch of AI cover songs, I just fell deep into the rabbit hole of listening to a bunch of Kanye covers.&lt;/p&gt;

&lt;p&gt;The quality really varied. Some of the songs were unlistenable due to weird glitches in the voice. But some sounded pretty convincing. The good ones still didn‚Äôt really pass as real, but I think that with some post-processing and maybe an actual sound engineer on the case, they could be great.&lt;/p&gt;

&lt;p&gt;I was on a plane to Hawaii, and while I also had some fun doing Hawaii stuff, I also spent some of the time learning about voice cloning and trying to train some voice models myself.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;blue-skies-and-golden-sunshine&quot;&gt;Blue Skies and Golden Sunshine&lt;/h3&gt;

&lt;p&gt;The first voice that I wanted to try to clone was David Lynch‚Äôs. You might not think that his voice would be my first choice, so let me explain. Maybe a year ago, I started watching these &lt;a href=&quot;https://www.youtube.com/@DAVIDLYNCHTHEATER&quot;&gt;David Lynch weather reports&lt;/a&gt; that he was doing daily. I think the joke is that the weather in L.A. is pretty much the same all the time. But I would watch it every day, and it was pretty soothing in a pandemic world to have something that was consistent.&lt;/p&gt;

&lt;p&gt;I had some ideas on automating his weather report, which would either take pieces of his other reports, and stitch them together to make a new one (based on the actual weather of the day, of course). I looked at some AI libraries that could find someone‚Äôs face and &lt;a href=&quot;https://github.com/Rudrabha/Wav2Lip&quot;&gt;make their lips move with an audio file&lt;/a&gt;. The results were actually pretty creepy, which is appropriate for David Lynch.&lt;/p&gt;

&lt;p&gt;But I figured he might get mad at me and I don‚Äôt want David Lynch to be mad at me. Plus I kind of got sidetracked and busy and I didn‚Äôt have time to finish that project.&lt;/p&gt;

&lt;p&gt;David Lynch stopped his weather reports abruptly some time in December, and I started missing them. One of the reasons I liked listening to his reports is that he has a really funny way of talking. He talks like an old timey newsreel, and pronounces words a certain way. Like for ‚Äúday‚Äù he says it like ‚Äúdee.‚Äù&lt;/p&gt;

&lt;p&gt;I figured that if I could clone his voice, I could do my own version of his weather report. And if I could get the pronunciations correct, then it could seem like he was back doing them (in an Asian person‚Äôs body). I was also thinking about integrating deepfake technology into my version of the weather reports, but I haven‚Äôt figured that part out yet.&lt;/p&gt;

&lt;p&gt;Anyway, I trained a voice clone model of David Lynch using the audio from his weather reports. I used a library called &lt;a href=&quot;https://github.com/voicepaw/so-vits-svc-fork&quot;&gt;‚Äúso-vits-svc-fork‚Äù&lt;/a&gt; to do this, and I trained it on &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt; since I don‚Äôt have a GPU at the moment.&lt;/p&gt;

&lt;p&gt;When I tried to actually infer my voice to the David Lynch voice model, it sounded really funny. I ended up &lt;a href=&quot;https://youtu.be/-f86ZqMD9_o&quot;&gt;posting a video of it anyway though&lt;/a&gt;, and I shared it in the David Lynch subreddit. The folks there thought it was a pretty convincing impression, but I was still not happy with the results.&lt;/p&gt;

&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/-f86ZqMD9_o&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;p&gt;Eventually, while playing with the settings, I figured out that I wanted to manually transpose and set the ‚Äú‚Äìno-auto-predict-f0‚Äù flag. I also experimented with some other f0 prediction methods which made the synthesized voice sound better. I still haven‚Äôt made another weather report but I‚Äôm pretty sure I could nail it with the updated settings. I also understand now why David Lynch stopped, because it‚Äôs kind of a lot of work and I can‚Äôt imagine doing it every day.&lt;/p&gt;

&lt;h3 id=&quot;i-think-im-a-clone-now&quot;&gt;I Think I‚Äôm a Clone Now&lt;/h3&gt;

&lt;p&gt;So after the success of the David Lynch voice model, I figured I would clone my own voice. Because sometimes I wonder how it would sound if I could actually sing ‚ÄúMy Heart Will Go On‚Äù exactly like Celine Dion (currently I‚Äôm at about 90%).&lt;/p&gt;

&lt;p&gt;I took a bunch of audio from my &lt;a href=&quot;https://youtu.be/fxVCrGzEP_k&quot;&gt;previous YouTube video on Trombone Champ&lt;/a&gt;, which was about 10 minutes. Coincidentally, 10 minutes is around the suggested amount of audio to use for voice cloning.&lt;/p&gt;

&lt;p&gt;I trained a model on just me speaking, but the model ended up underperforming when it came to singing. There were some weird glitches when moving between notes, which were probably just pitches that weren‚Äôt included in the sample data.&lt;/p&gt;

&lt;p&gt;I ended up recording about 6 minutes of myself singing some various hits from the 80s and 90s, and added that to the audio data for training. The resulting model sings pretty well. Here are some audio samples in case you are interested.&lt;/p&gt;

&lt;p&gt;My Heart Will Go On:&lt;/p&gt;
&lt;audio controls=&quot;&quot;&gt;
	 &lt;source src=&quot;/blog/wp-content/uploads/2023/Hung Truong - My Heart Will Go On.mp3&quot; type=&quot;audio/mpeg&quot; /&gt;
Your browser does not support the audio element.
&lt;/audio&gt;

&lt;p&gt;I Believe I Can Fly&lt;/p&gt;
&lt;audio controls=&quot;&quot;&gt;
	 &lt;source src=&quot;/blog/wp-content/uploads/2023/Hung Truong - I Believe I Can Fly.mp3&quot; type=&quot;audio/mpeg&quot; /&gt;
Your browser does not support the audio element.
&lt;/audio&gt;

&lt;p&gt;After trying it out for myself, I‚Äôm convinced that AI voice cloning will be a pretty big deal, not just for these novelty use cases, but also for actual music production. Sure, the results aren‚Äôt perfect right now, but you can believe that as technology gets better and better, artists are going to want to use this to gain a competitive edge, just like autotune or any other kind of technology that was introduced in the past.&lt;/p&gt;

&lt;p&gt;I thought up a few use cases where this technology can be used, but I‚Äôm sure there are others:&lt;/p&gt;

&lt;h3 id=&quot;multi-lingual-media&quot;&gt;Multi-Lingual Media&lt;/h3&gt;

&lt;p&gt;Currently, the only artists who can have hits in multiple languages are the ones who are bilingual. For exmample, Shakira has version of ‚ÄúHips Don‚Äôt Lie‚Äù in Spanish and English because she knows both.&lt;/p&gt;

&lt;p&gt;But if I was an American artist who wanted greater reach in Japan, I could have a voice double singing with Japanese lyrics to one of my songs, then clone my voice into the Japanese track. It would then sound like I‚Äôm singing in Japanese. If artists can reach more fans, that‚Äôs probably a positive for them. Music is universal, and artists like Bad Bunny are proving that you don‚Äôt need to speak a language to be popular to the native speakers, but it certainly helps if you understand what the person is actually singing about.&lt;/p&gt;

&lt;h3 id=&quot;enhanced-accessibility&quot;&gt;Enhanced Accessibility&lt;/h3&gt;

&lt;p&gt;I‚Äôve been making more YouTube videos lately, and when I do, I always try to make them accessible by adding captions. This helps people who have hearing disabilities enjoy my videos. I usually only caption in English, but those captions can then be auto-translated by Google, which also expands my viewership to people who don‚Äôt speak English.&lt;/p&gt;

&lt;p&gt;To make my videos even more accessible, I could translate the captions, then produce audio using TTS. This would result in a voice that is speaking the captions of my video in the other langauge. Finally, I can apply my voice clone to the TTS audio to make it sound like I‚Äôm speaking the other language. I tried this in my voice clone video, but I had to slow down the actual video because I talk fast, and I guess German just uses more/longer words.&lt;/p&gt;

&lt;h3 id=&quot;voice-clone-collabs&quot;&gt;Voice Clone Collabs&lt;/h3&gt;

&lt;p&gt;Right now these voice clone AI covers are just working in some murky legal territory, because it‚Äôs a bunch of enthusiasts having fun and creating things. At some point, the ones who are good at this will probably end up collaborating with artists to make music together. Not all artists will do this, but &lt;a href=&quot;https://twitter.com/Grimezsz&quot;&gt;Grimes has already stated that she‚Äôll split earnings if anyone uses her voice&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;selling-voice-rights&quot;&gt;Selling Voice Rights&lt;/h3&gt;

&lt;p&gt;Bob Dylan recently sold his entire music catalog for a pretty big sum. As artists are getting older and ready to retire, I could see them offering rights to use their voice as well. I mean, if they‚Äôre not going to use it, they might as well let someone else! Imagine how much the rights to use Michael Jackson‚Äôs voice would be worth. Especially in the right hands and with a professional producer. I‚Äôm not saying that I‚Äôd rather listen to AI MJ than a real person at this point, but there‚Äôs probably going to be a market for it, whether most people want it or not.&lt;/p&gt;

&lt;h3 id=&quot;the-dark-side-of-cloning&quot;&gt;The Dark Side of Cloning&lt;/h3&gt;

&lt;p&gt;Unfortunately, not all use cases for voice cloning technology are positive or legal. Scammers have used voice cloning to convince parents that their teens were kidnapped, and others have sold fake ‚Äúleaked‚Äù tracks by famous artists to collectors.&lt;/p&gt;

&lt;p&gt;For some reason my bank wants ‚Äúvoice authorization‚Äù to be a thing, even though it‚Äôs the stupidest, insecure thing I could imagine. As if someone couldn‚Äôt get a recording of my voice. Now they could actually clone it.&lt;/p&gt;

&lt;p&gt;I‚Äôm sure there‚Äôs plenty of other bad things you can do with voice cloning, just like with image generation, and tools like Photoshop. A tool is just a tool, and people will use them for good stuff and bad stuff.&lt;/p&gt;

&lt;h3 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h3&gt;

&lt;p&gt;It was really fun to explore the possibilities of voice cloning, and it‚Äôs just another tool to think about as generative AI tools become more popular in different aspects of our lives. Not only do we have text generation and image generation, we also have speech, both from text to speech systems as well as these new speech to speech systems.&lt;/p&gt;

&lt;p&gt;I‚Äôve also been interested in text to music generation, like the one from Google that was made available recently, called &lt;a href=&quot;https://aitestkitchen.withgoogle.com/experiments/music-lm&quot;&gt;MusicLM&lt;/a&gt;MusicLM. I might try using that to make something new as well.&lt;/p&gt;

&lt;p&gt;In the meantime, it‚Äôs taking a lot of effort just to stay on top of all these things!&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2023/all_along_the_way.jpg&quot; /&gt;
	&lt;figcaption&gt;Two legendary directors doing the weather report&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Note: This blog post is based on a YouTube video I made, which you can watch in the embed, but it also has some stuff that‚Äôs not in the video, so read the stuff I wrote below!&lt;/p&gt;

&lt;center&gt;
	&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/OlbVWneM6xE&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;p&gt;I was recently stuck on a really long, 6 hour flight. Luckily, I had an internet connection the whole time, and I was testing out some nifty &lt;a href=&quot;https://amzn.to/3OryWcQ&quot;&gt;AR Glasses&lt;/a&gt;, so I watched a lot of YouTube.&lt;/p&gt;

&lt;p&gt;I remembered seeing a video about AI voice clone cover songs, where someone will take the voice of a known artist, and then make them sing a song by another artist (e.g. Kanye West singing ‚ÄúCall Me Maybe‚Äù), or they create a whole new original song for the artist (e.g. Drake singing an original called &lt;a href=&quot;https://www.nytimes.com/2023/04/19/arts/music/ai-drake-the-weeknd-fake.html&quot;&gt;‚ÄúHeart on My Sleeve‚Äù&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Since I was stuck on the plane, and I had the privacy to watch whatever I wanted without my seatmates judging me for binging on a bunch of AI cover songs, I just fell deep into the rabbit hole of listening to a bunch of Kanye covers.&lt;/p&gt;

&lt;p&gt;The quality really varied. Some of the songs were unlistenable due to weird glitches in the voice. But some sounded pretty convincing. The good ones still didn‚Äôt really pass as real, but I think that with some post-processing and maybe an actual sound engineer on the case, they could be great.&lt;/p&gt;

&lt;p&gt;I was on a plane to Hawaii, and while I also had some fun doing Hawaii stuff, I also spent some of the time learning about voice cloning and trying to train some voice models myself.&lt;/p&gt;

</description>
        
        <pubDate>Sat, 20 May 2023 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2023/05/20/voice-cloning-for-fun-and-profit/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2023/05/20/voice-cloning-for-fun-and-profit/</guid>
        
        
        <category>Tech</category>
        
        <category>AI</category>
        
      </item>
      
    
      
      <item>
        <title>Making a Trombone Champ Controller From a Trombone!</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/tchamp1920hero.jpg&quot; /&gt;
	&lt;figcaption&gt;It&apos;s everyone&apos;s favorite new trombone music rythm game!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Note: this blog post is more or less adapted from a script I used to make my YouTube video on this subject. So if you want to watch the video, I‚Äôve embedded it here! Or if you like reading more, then go ahead and read my post below! (I can‚Äôt stand the recent trend of making content that‚Äôs video-only so I refuse to not make a blog post out of this video!)&lt;/p&gt;
&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lUXrkq2e1zk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;p&gt;I just got this new game called &lt;a href=&quot;https://store.steampowered.com/app/1059990/Trombone_Champ/&quot;&gt;‚ÄúTrombone Champ‚Äù&lt;/a&gt; which is like Guitar Hero but with a trombone. It‚Äôs honestly one of the most refreshing new games I‚Äôve seen, not only because it‚Äôs fun and has a good sense of humor, but also because it sort of reinvigorates the music game genre. I love that part of the game is subtly recognizing that you can‚Äôt really sound good on a trombone, which is hilarious. I did find that the control scheme could be improved, since you‚Äôre expected to play with a mouse (and optionally keyboard keys). So I set out to turn my soprano trombone into a real video game controller for Trombone Champ!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;why-a-trombone-controller&quot;&gt;Why a Trombone Controller?&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/tchamp-screen1.jpg&quot; /&gt;
	&lt;figcaption&gt;A screenshot of the game&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;After I got the game, I played all the levels and had a lot of fun, but I felt like something was missing. I think that driving games are more fun when you use a steering wheel, and flight simulators are more fun when you use a flight stick. Can you imagine how boring Duck Hunt would be if you had to use the regular NES controller to play it? So I decided to make a trombone controller for Trombone Champ.&lt;/p&gt;

&lt;p&gt;Luckily, I have a love of novelty instruments, and I never throw anything away, so I just happened to have a &lt;a href=&quot;https://www.wwbw.com/Jupiter-314L-Soprano-Trombone-Slide-Trumpet-460203.wwbw&quot;&gt;Jupiter soprano trombone in my closet&lt;/a&gt;. I think I origially bought it because it would be funny to use during basketball games (I used to play in the band in college).&lt;/p&gt;

&lt;h3 id=&quot;making-a-fake-mouse&quot;&gt;Making a Fake Mouse&lt;/h3&gt;

&lt;p&gt;So since I have an actual trombone, all I need now is to connect my trombone to the computer, so I can play the game with it. I did some searching and found &lt;a href=&quot;https://github.com/T-vK/ESP32-BLE-Mouse&quot;&gt;a library that can turn a esp32 microcontroller into a bluetooth mouse&lt;/a&gt;. This is perfect because then all I need to is connect some sensors to my trombone and have those move the mouse and click its buttons. Then I can connect the mouse to my computer and play the game.&lt;/p&gt;

&lt;p&gt;I decided to use a &lt;a href=&quot;https://www.dfrobot.com/product-1590.html&quot;&gt;Firebeetle ESP32 microcontroller&lt;/a&gt; because I had one lying around from a previous project, and because it comes with a connector to plug in a lithium ion battery. This is nice because I‚Äôll be able to play the game completely wirelessly instead of needing to plug in to usb for power.&lt;/p&gt;

&lt;h3 id=&quot;adding-sensors&quot;&gt;Adding Sensors&lt;/h3&gt;

&lt;p&gt;So now I have two different problems to solve. I‚Äôll call these the ‚Äúblow problem‚Äù and the ‚Äúslide problem.‚Äù Let‚Äôs start with the blow problem.&lt;/p&gt;

&lt;p&gt;I didn‚Äôt want to just add a button to my trombone to play a sound, because that‚Äôs not how wind instruments work. I want to actually blow into something. So I looked for different ways to &lt;a href=&quot;https://www.mouser.com/c/sensors/flow-sensors/?for%20use%20with=Air&quot;&gt;measure air flow&lt;/a&gt;, and it turns out these types of sensors are really expensive and didn‚Äôt make sense for my project. I ended up finding this neat &lt;a href=&quot;https://www.sparkfun.com/products/16476&quot;&gt;air pressure sensor&lt;/a&gt; that also has a port in it to attach a tube. My idea was that I could attach a tube to the trombone mouthpiece and measure the air pressure difference when I blew into it, directly into the sensor.&lt;/p&gt;

&lt;p&gt;This was actually a terrible idea though, because the tubing would need to go through the entire trombone and then come out of the bell, and it would really impede the movement of the slide.&lt;/p&gt;

&lt;p&gt;I ended up putting the whole sensor in the bell and then sealing the entire instrument with some saran wrap. When it‚Äôs on it has a red LED which I think gives it a pretty nice cyberpunk feel. Then I measured the ambient air pressure inside the trombone. In in my room it happens to be around 1011 hPa. When I blow into the horn, it causes the air pressure to go up, and I can register a click. When the air pressure goes back down, I can release it.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/trombone_air_sensor.jpg&quot; /&gt;
	&lt;figcaption&gt;The air sensor in my trombone&apos;s bell&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So with my blow problem a thing of the past, let‚Äôs move on to the slide problem. In the game, you‚Äôre supposed to use a mouse to simulate the slide of a trombone. But there are a lot of problems with this. The slide on a trombone is fixed, so if you go to first position, you hit the end of the trombone. You can‚Äôt go to negative positions, but your mouse doesn‚Äôt just stop when you hit the end of the screen. Plus on the instrument you can use muscle memory and look at the bell to figure out what note you‚Äôre gonna hit.&lt;/p&gt;

&lt;p&gt;I looked at a bunch of different ways to solve the problem of using the actual trombone slide as a controller for the game‚Äôs slide. There are various distance sensors: lidar, sonar, time of flight, you could also probably use a gyroscope and accelerometer to figure out what direction the slide is going. I think I saw someone use a wiimote‚Äôs accelerometer to make a 3d air mouse once.&lt;/p&gt;

&lt;p&gt;To be honest, I just wanted to get started on this project quickly, so I ordered the &lt;a href=&quot;https://amzn.to/3dNOUOD&quot;&gt;cheapest time of flight sensor on Amazon that had next day shipping&lt;/a&gt;. I attached it to the bell of the trombone and put a piece of cardboard on it that it can use to bounce a laser on to detect how far the laser is away from the initial position.&lt;/p&gt;

&lt;p&gt;When I actually hooked this up to my microcontroller, it ended up giving some pretty inaccurate readings but I figured I could hook it up to my mouse software and see what happens.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/trombone_distance_sensor.jpg&quot; /&gt;
	&lt;figcaption&gt;The distance sensor attached to the bell&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;
&lt;center&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;This weekend I made a Trombone Champ controller out of a real (soprano) trombone! I&amp;#39;ll have a video out shortly about the process, but for now, enjoy this video of me absolutely failing our national anthem with my new device! cc &lt;a href=&quot;https://twitter.com/HolyWowStudios?ref_src=twsrc%5Etfw&quot;&gt;@HolyWowStudios&lt;/a&gt; &lt;a href=&quot;https://t.co/SvPoxfk6GV&quot;&gt;pic.twitter.com/SvPoxfk6GV&lt;/a&gt;&lt;/p&gt;&amp;mdash; Hung Truong (@hungtruong) &lt;a href=&quot;https://twitter.com/hungtruong/status/1573854148923359232?ref_src=twsrc%5Etfw&quot;&gt;September 25, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/center&gt;

&lt;p&gt;I‚Äôve linked my tweet of an attempt at the Star Spangled Banner above, but if you don‚Äôt want to watch a video, I‚Äôll tell you that it did not work well! I think the time-of-flight sensor I got was faulty, as it didn‚Äôt go past 100mm with any degree of accuracy, but is rated for 2000mm.&lt;/p&gt;

&lt;p&gt;But to be honest, I‚Äôm really happy with the way this project turned out. The blow sensor works a lot better than I expected it to, as I was initially worried it wouldn‚Äôt be able to register the notes quickly enough. I also learned a lot about simulating a bluetooth mouse which I could probably use in future projects. I‚Äôm happy to share this project even though it isn‚Äôt perfect yet because this kind of prototyping is really an iterative process anyway.&lt;/p&gt;

&lt;p&gt;I also thought it was interesting how the slightly air tight seal on the trombone bell works. When you normally play an instrument, you have some back pressure, but air is also going through. I would say that I probably use more air blowing into the trombone without making any noise than I do when I‚Äôm actually playing it.  This could lead to me getting tired pretty quickly, so I might adjust the seal on the horn a bit more so I don‚Äôt have to blow as hard.&lt;/p&gt;

&lt;p&gt;As far as next steps go, I‚Äôve already purchased a few new sensors to try out, one is an &lt;a href=&quot;https://amzn.to/3RdPcMs&quot;&gt;acoustic distance detector&lt;/a&gt;, and the other is a &lt;a href=&quot;https://amzn.to/3xTDXSe&quot;&gt;more expensive time of flight sensor from adafruit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I‚Äôll probably update this blog post when I try the new sensors, so look for an update below at some point!&lt;/p&gt;

&lt;h3 id=&quot;update-september-27-2022&quot;&gt;Update September 27, 2022&lt;/h3&gt;

&lt;p&gt;So I ended up getting the new distance sensors! I had a problem getting the acoustic sensor working (it looks like it doesn‚Äôt play well with other i2c devices at the same time) but the time-of-flight sensor from Adafruit is working a lot better. Here‚Äôs a video where I actually get a C in Oh Canada.&lt;/p&gt;

&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/u0Msn00sflU&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;p&gt;Some quick impressions on the current state of Trombone Champ Controller v2: I am going to need to seal the bell a bit better than with the current plastic wrap solution I have now. There‚Äôs quite a bit of air leak (as I had predicted before). Playing a 1 minute song is pretty easy but playing the longer ones leaves me pretty lightheaded by the end. When playing a regular trumpet, I don‚Äôt really use that much air, so getting too much oxygen is not an issue. But I am probably setting myself up for hyperventilation side effects if I don‚Äôt make another adjustment.&lt;/p&gt;

&lt;p&gt;Aside from that I am not sure if the game audio is a bit laggy or if it‚Äôs due to my screen recording software but I felt like I needed to preemptively blow a bit early for the controls to actually hit accurately. I did see that the developer made an update around audio latency so maybe that‚Äôll fix it!&lt;/p&gt;

&lt;center&gt;
	&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;ALSO, we just pushed a very very small update (&amp;lt;5mb) to v1.04 which may make overall audio latency slightly better! Between this and the improved multi-key keyboard stuff, serious rhythm game people may be *slightly* happier with Trombone Champ starting from tonightüôè&lt;/p&gt;&amp;mdash; Holy Wow (üé∫TROMBONE CHAMP IS OUT!) (@HolyWowStudios) &lt;a href=&quot;https://twitter.com/HolyWowStudios/status/1574587033825071105?ref_src=twsrc%5Etfw&quot;&gt;September 27, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/center&gt;

&lt;p&gt;I think I can also probably improve the slide accuracy by using something a bit more sturdy than a crappy piece of cardboard taped to the slide‚Ä¶ Maybe something 3d printed?&lt;/p&gt;

&lt;p&gt;I‚Äôll update this blog post as I make more improvements.&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/tchamp1920hero.jpg&quot; /&gt;
	&lt;figcaption&gt;It&apos;s everyone&apos;s favorite new trombone music rythm game!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Note: this blog post is more or less adapted from a script I used to make my YouTube video on this subject. So if you want to watch the video, I‚Äôve embedded it here! Or if you like reading more, then go ahead and read my post below! (I can‚Äôt stand the recent trend of making content that‚Äôs video-only so I refuse to not make a blog post out of this video!)&lt;/p&gt;
&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lUXrkq2e1zk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;p&gt;I just got this new game called &lt;a href=&quot;https://store.steampowered.com/app/1059990/Trombone_Champ/&quot;&gt;‚ÄúTrombone Champ‚Äù&lt;/a&gt; which is like Guitar Hero but with a trombone. It‚Äôs honestly one of the most refreshing new games I‚Äôve seen, not only because it‚Äôs fun and has a good sense of humor, but also because it sort of reinvigorates the music game genre. I love that part of the game is subtly recognizing that you can‚Äôt really sound good on a trombone, which is hilarious. I did find that the control scheme could be improved, since you‚Äôre expected to play with a mouse (and optionally keyboard keys). So I set out to turn my soprano trombone into a real video game controller for Trombone Champ!&lt;/p&gt;

</description>
        
        <pubDate>Mon, 26 Sep 2022 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2022/09/26/making-a-trombone-champ-controller-from-a-trombone/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2022/09/26/making-a-trombone-champ-controller-from-a-trombone/</guid>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Stable Diffusion: Generating Images From Words</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/ron-swanson-weasley.png&quot; /&gt;
	&lt;figcaption&gt;Ron Swanson as Ron Weasley!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For the past few months or so, I‚Äôve been noticing a lot of news stories about &lt;a href=&quot;https://openai.com/dall-e-2/&quot;&gt;DALL¬∑E 2&lt;/a&gt;, an AI image generator that uses GANs to create images from prompts. It was private for quite a while, and there were some similar, less powerful projects that were open. I played around with them a bit but I was waiting for a general release. I ended up getting into the DALL¬∑E 2 beta a few weeks ago and last week I saw news that there was a new release of another project called &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;&gt;Stable Diffusion&lt;/a&gt;, so I installed it on my MacBook. The results really blew me away!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;getting-it-working&quot;&gt;Getting it working&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/typing-on-monitors.png&quot; /&gt;
	&lt;figcaption&gt;Prompt: photo from behind an asian software engineer typing a keyboard with many monitors in front of him, studio lighting bokeh&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;It wasn‚Äôt too bad getting the release of Stable Diffusion working on my Mac as I just went through some of the steps &lt;a href=&quot;https://zenn.dev/bellbind/scraps/ea15aab699dde9&quot;&gt;on this site I found&lt;/a&gt;. There were some gotchas, like needing to install Rust to compile something and maybe a few files to change around, but for the most part it worked pretty quickly. I could probably have written down the steps but I‚Äôm sure it will be a lot more simple when they add more specific support for Apple Silicon.&lt;/p&gt;

&lt;p&gt;Right now it takes about 3 minutes to create an image with my M2 MacBook Air, which is kind of slow. I ended up using &lt;a href=&quot;https://colab.research.google.com&quot;&gt;Google Collab&lt;/a&gt; which lets you use their GPUs for free. The next day though, I found I was rate limited. So I moved over to &lt;a href=&quot;https://www.kaggle.com&quot;&gt;Kaggle&lt;/a&gt; which at least tells you what your GPU limits are, and they seem pretty reasonable (like 30 hours a week at least). The Kaggle notebook has been my main workflow so far because it‚Äôs quite fast (like 10 times faster than my Mac) and the short feedback loop really helps with coming up with prompts.&lt;/p&gt;

&lt;h3 id=&quot;prompt-engineering&quot;&gt;Prompt Engineering&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/tail-of-two-kitties.png&quot; /&gt;
	&lt;figcaption&gt;Prompt: cat vs a british shorthair cat sitting in front of a giant cheeseburger in the african savannah, fujifilm x-t2 35mm golden hour&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So What is a prompt anyway? A prompt is what you type in to the image generator to describe what you want it to create for you. If you say you want a picture of a cat, it‚Äôll likely come up with a pretty good cat. But if you want, you can also describe the cat in more detail to get a more specific image. You could add the breed, for example, or say it‚Äôs a robot cat, or make it sit on a park bench. There are so many possibilities of what kind of cat to show that the prompt ends up being incredibly important to get what you want.&lt;/p&gt;

&lt;p&gt;There‚Äôs been a lot of research done on prompt engineering. Some of it feels like a shortcut, by asking for an image in the style of a famous artist. You can also just describe the medium of the image, like a water color or illustration. I‚Äôve noticed people on Reddit adding words like ‚Äútrending on artstation‚Äù which I guess is a way to suggest that the image is aesthetically pleasing to a majority of people. You can also say something was painted badly, which is kind of hilarious.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/ugly-woman-painting.png&quot; /&gt;
	&lt;figcaption&gt;Prompt: a really ugly painting of a woman&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Part of the fun of playing around with these tools is that they‚Äôre so new that it‚Äôs possible to find words that can create certain images that no one else knows about. For me, it brings back the feeling of being on the early internet, when not everything was indexed by Google to the point where there were no hidden gems. Someone should bring back ‚ÄúCool Site of the Day‚Äù but for prompts!&lt;/p&gt;

&lt;h3 id=&quot;different-techniques&quot;&gt;Different Techniques&lt;/h3&gt;

&lt;p&gt;I‚Äôve learned that there are a few different techniques to make images, and I‚Äôm learning quite a few more. The simplest one is text to image, which I just described. The way I understand it is that basically you start with some random noise, and then two AIs work to alter the noise to turn it into the image you want by iterating changes and measuring how closely the image matches your description. That‚Äôs probably really simplified and maybe wrong but whatever, I‚Äôm not an AI engineer.&lt;/p&gt;

&lt;p&gt;Another way to create images is to start with a base image, and also feed the AI a prompt. Since you can choose the starting image (instead of just random noise), there‚Äôs a better chance that the image converges to something that resembles your input image. This gives you quite a bit more control over the final image‚Äôs general shape, composition, etc. You could feed it stick figures or a photo from your phone. I‚Äôve seen people turn stick drawings into D&amp;amp;D character portraits using this technique.&lt;/p&gt;

&lt;p&gt;I tried this technique out by using a photo of my dog, Sodapop, sitting in the grass. The picture is pretty good, but it‚Äôs not award winning or anything. I fed the text ‚Äúa watercolor illustration of a black and white cardigan corgi sitting in the middle of a green flowery meadow in front of an orange ball, masterpiece, big cute eyes‚Äù. I didn‚Äôt start with that, but I kept changing it to try and get an image that I wanted.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/sodapop-watercolor.png&quot; /&gt;
	&lt;figcaption&gt;Prompt: Sodapop vs Watercolorpop&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I also played around with different strengths and amounts of iterations. I found that if I used too many iterations, the image didn‚Äôt really resemble Sodapop anymore. He‚Äôs a black and white Corgi, which is less common, so there‚Äôs probably more of a bias towards the sable and white ones. One thing I learned is that it‚Äôs better to just generate a huge number of images and then pick the ones you like. You can save the random seed value and use it to refine the image further as well. There were a lot of really terrible looking corgi watercolor images which my computer is full of now. But there were also some fairly good ones too! The power with this AI is that it‚Äôs pretty cheap to just make more images until you get what you want.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/reject-corgi.png&quot; /&gt;
	&lt;figcaption&gt;One of many rejected generations&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;future-techniques&quot;&gt;Future Techniques&lt;/h3&gt;

&lt;p&gt;There is another technique I tried recently where someone tried to create a bigger image (right now most video cards can only do 512x512 and that‚Äôs what the model is trained on) by creating an image, upscaling it and then running the image to image process on 9 square parts of the upscaled image. When I tried this, I found that it added weird artifacts into each square piece. It was recursively trying to fit the whole prompt into each square. My prompt was a garden, and it basically tried to add a garden into each subsquare of the image. This could have been due to a current bug where the random seed on Mac doesn‚Äôt really work, but I don‚Äôt have the hardware to try it on a non-Mac right now.&lt;/p&gt;

&lt;p&gt;I‚Äôve had a lot more fun playing around with this image generation stuff than I have in a long time with technology, so I ordered a new graphics card so I can iterate on things more quickly on my own infrastructure. There‚Äôs something really magical about using a model file that‚Äôs only a few gigabytes to basically create any image you can think of. If my internet connection ever goes down for the count, this could be my main source of entertainment.&lt;/p&gt;

&lt;p&gt;There‚Äôs a bunch of other things I want to try. There‚Äôs a technique called &lt;a href=&quot;https://textual-inversion.github.io&quot;&gt;‚ÄúTextual Inversion‚Äù&lt;/a&gt; where you can sort of re-train (but not really) the model to use a personalized word. I could do this with Sodapop so I stop getting Pembroke Corgis when I want my Corgi. I was also wondering if I could use it with pictures of myself, since Stable Diffusion seems to work really well with making images with well known celebrities in them.&lt;/p&gt;

&lt;p&gt;When I first saw this technology I figured it would be good for creating blog post images (which obviously it was for this post). I‚Äôm also envisioning things like services for creating customized watercolor portraits for your dog, or custom fantasy avatars for a person. I think people have just barely scratched the surface here so hopefully there‚Äôs a lot more interesting stuff coming up.&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/ron-swanson-weasley.png&quot; /&gt;
	&lt;figcaption&gt;Ron Swanson as Ron Weasley!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For the past few months or so, I‚Äôve been noticing a lot of news stories about &lt;a href=&quot;https://openai.com/dall-e-2/&quot;&gt;DALL¬∑E 2&lt;/a&gt;, an AI image generator that uses GANs to create images from prompts. It was private for quite a while, and there were some similar, less powerful projects that were open. I played around with them a bit but I was waiting for a general release. I ended up getting into the DALL¬∑E 2 beta a few weeks ago and last week I saw news that there was a new release of another project called &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;&gt;Stable Diffusion&lt;/a&gt;, so I installed it on my MacBook. The results really blew me away!&lt;/p&gt;

</description>
        
        <pubDate>Sun, 28 Aug 2022 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2022/08/28/stable-diffusion-generating-images-from-words/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2022/08/28/stable-diffusion-generating-images-from-words/</guid>
        
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>I Created a Robot to Solve Wordle For Me</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2022/WordleScreenshot.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unless you‚Äôve been living under a rock for the past few days, you‚Äôve heard of this game online called Wordle. It‚Äôs been growing like crazy and the New York Times even wrote a &lt;a href=&quot;https://www.nytimes.com/2022/01/03/technology/wordle-word-game-creator.html&quot;&gt;profile on the creator&lt;/a&gt;. If you know me, you know I like automating things, so it should be no surprise that I decided to automate playing the game. Here‚Äôs a blog post on how I did it.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;wtf-is-a-wordle&quot;&gt;WTF is a Wordle?&lt;/h3&gt;

&lt;p&gt;I guess I could start out by explaining what Wordle is, in case you‚Äôre reading this blog without knowing what it is, and if you are, why not just &lt;a href=&quot;https://www.powerlanguage.co.uk/wordle/&quot;&gt;play it first&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;Anyway, the fun part of Wordle is that there isn‚Äôt really a way to cheat. You try to guess a five letter word, and the game gives you feedback for each letter in the word you guessed. Either the letter doesn‚Äôt exist at all in the target word, the letter exists in the word but not in that position, or the letter does exist in the word in that position. Letters can repeat, e.g. ‚Äúsilly,‚Äù and you get 6 guesses before you lose (I think). I‚Äôve actually never lost, because 6 is a pretty good number of guesses unless you get really unlucky.&lt;/p&gt;

&lt;p&gt;If you think about it mathematically, 6 tries times 5 letters means you could theoretically narrow down 25 letters before your last guess. But that‚Äôs assuming you only use unique letters and there are words you can actually use to guess those letters (the game doesn‚Äôt allow non-words).&lt;/p&gt;

&lt;p&gt;There is the problem of narrowing the word down to maybe 1 letter difference but there‚Äôs a bunch of words that end in the same letters, e.g.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;frank&lt;/li&gt;
  &lt;li&gt;drank&lt;/li&gt;
  &lt;li&gt;crank&lt;/li&gt;
  &lt;li&gt;spank&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;etc. If you get the last three letters then you still need to narrow down what the front ones are, which you could do exploratively, but you can also just keep trying likely words.&lt;/p&gt;

&lt;p&gt;Anyway, if it isn‚Äôt obvious yet, I probably enjoy the metagame of Wordle more than playing the game itself.&lt;/p&gt;

&lt;h3 id=&quot;solving-wordle&quot;&gt;Solving Wordle&lt;/h3&gt;
&lt;p&gt;So how do you go about automating a game like Wordle? I started by trying to inspect the network requests going back and forth between the web frontend and the backend. Suprisingly, there are no network requests made because the whole game is just a javascript file, and (I think) a hash that determines what the word of the day is. The whole game is just a javascript file that you download and run.&lt;/p&gt;

&lt;p&gt;So looking at the javascript file, I saw a bunch of hard to read code (because javacript), and a few interesting arrays.&lt;/p&gt;

&lt;p&gt;The first had a bunch of pretty normal looking words. The other had words that looked real, but were less common. My guess is that there are words that can actually be used for the game because a normal person would know them, but another list of words that are valid guesses (because the game doesn‚Äôt let you put nonsense in). So I decided to take the first list of words and use it as my ‚Äúdictionary‚Äù of possible choices, rather than just use a linux dictionary of all the obscure words that exist.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/wordle_list.png&quot; /&gt;
	&lt;figcaption&gt;The two word lists, possible answers on top and valid obscure words on bottom.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As far as playing the game, you have to start somewhere. I started out by keeping a list of all possible answers (copied from the source), and picking a random one. Once that happens, you get hints for which letters to keep trying and which to discard. The game code actually has three values for letter results: ‚Äúpresent,‚Äù ‚Äúabsent,‚Äù and ‚Äúcorrect.‚Äù&lt;/p&gt;

&lt;p&gt;From these, the basic algorithm to remove words from the possible choices is (for each letter):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If present, remove all words that have that letter in that specific position (for example if the word is ‚Äúfarts‚Äù and you guess ‚Äúafter,‚Äù the first ‚Äòa‚Äô would be present, but never in the first position). Also, remove all words that do not have that letter present in any positions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If absent, remove all words that have that letter in any position.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If correct, remove all words that do not have that letter in that position.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That‚Äôs basically it, and doing this will quickly whittle down the list of possible choices.&lt;/p&gt;

&lt;p&gt;There are some other optimizations that I tried out but I don‚Äôt think they have much of a significant effect on how quickly the bot can guess the right answer. For example, I made a sorted list of the most common letters used in English words and had the bot prefer words with the most common letters first, to narrow down results faster. It might be possible to improve this by calculating probabilities or something down to the individual word level but I‚Äôm way too lazy (for now) to try that.&lt;/p&gt;

&lt;h3 id=&quot;a-mac-app&quot;&gt;A Mac App&lt;/h3&gt;

&lt;p&gt;I ended up writing a Mac app to run the solver because I knew I could use a webview and make it evaluate some javascript commands. I wrote a few functions for doing things like keying in letters and hitting enter, and reading the results using the ‚Äúshadow dom‚Äù which sounds a little bit like forbidden magic but whatever.&lt;/p&gt;

&lt;p&gt;I had to throw in some manual sleep() calls because it would go too fast if I didn‚Äôt. Also it looks more like a real person is playing if there‚Äôs a slight delay in entering letters.&lt;/p&gt;

&lt;p&gt;Here‚Äôs what the Mac app looks like solving the puzzle from Jan 8, 2021:&lt;/p&gt;

&lt;video controls=&quot;controls&quot; autoplay=&quot;true&quot; loop=&quot;true&quot; width=&quot;800&quot; height=&quot;600&quot; name=&quot;Wordle Solver Video&quot;&gt;
 &lt;source src=&quot;/blog/wp-content/uploads/2022/wordle.mov&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;I‚Äôve found that watching my bot play the game is more interesting to me than playing the actual game. Since it‚Äôs somewhat randomized, it‚Äôs fun to see what combinations of words the app tries, and how quickly it can narrow down its results. I log the list of potential answers after each round, and it‚Äôs fun to see how the app ‚Äúthinks.‚Äù&lt;/p&gt;

&lt;h3 id=&quot;one-more-thing-the-bot-but-irl&quot;&gt;One More Thing: The Bot, But IRL&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2022/idraw.png&quot; /&gt;
	&lt;figcaption&gt;iDraw: the second most advanced pen plotter known to me.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Making an app to automate something is cool, but I felt like something was still missing. I recently bought a mechanical pen plotter (which I should probably make another blog post about) &lt;a href=&quot;https://youtu.be/EirwSXMVa2U&quot;&gt;for making generative art&lt;/a&gt;, so I thought it would be fun to program the pen plotter to play the game on my iPad using my Apple Pencil.&lt;/p&gt;

&lt;p&gt;The pen plotter is basically a robot that has an arm and can raise and lower a pen (or Apple Pencil) and move it along an x-y axis. I bought an &lt;a href=&quot;https://uunatek.com/product/idraw-handwriting-drawing-machine/&quot;&gt;off-brand&lt;/a&gt; version of the &lt;a href=&quot;https://axidraw.com&quot;&gt;Axidraw&lt;/a&gt;, which has a similar but different board. I tried using the &lt;a href=&quot;https://axidraw.com/doc/py_api/#introduction&quot;&gt;axidraw python library&lt;/a&gt; to control the iDraw. It sort of works, but there are some weird differences. For example, the pen down movement seems to bring the pen up instead of down, so everything is reversed. Also the scaling is off, so if I want to move the pen 1 inch, it seems to move it by 2 centimeters instead. I just worked around these issues because I saved a lot of money by buying the cheaper one (it was about half the price).&lt;/p&gt;

&lt;p&gt;I wrote a script in Python to take one command line argument which is the word to guess, and hardcoded positions for each letter on the iPad screen. I was thinking I could use some kind of computer vision, AI library to detect positions and translate them for the plotter, but that‚Äôs also too much work. Maybe for a different project. The Mac app runs as before, except that it runs the local Python script when guessing, and waits for the script to complete before trying the next guess.&lt;/p&gt;

&lt;p&gt;I‚Äôm pretty happy with this setup, even though it‚Äôs a pain in the ass to get it working, because I have to set the pen height and line the iPad up with the robot correctly. I did end up getting the measurements right for the keyboard keys on the first try, which was cool. It‚Äôs just that if the iPad slides on the table then everything is off.&lt;/p&gt;

&lt;video autoplay=&quot;true&quot; loop=&quot;true&quot; width=&quot;800&quot; height=&quot;600&quot; name=&quot;Wordle Solver Bot Video&quot;&gt;
 &lt;source src=&quot;/blog/wp-content/uploads/2022/wordle_bot_demo.mov&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;I made a YouTube video that describes this more, and embellished some stuff for entertainment. So if you want to watch that, you can &lt;a href=&quot;https://www.youtube.com/watch?v=au5IPBBhiPw&quot;&gt;see it here&lt;/a&gt;, (and don‚Äôt forget to obliterate that like and subscribe button)!&lt;/p&gt;

&lt;p&gt;I also pushed the solver specific code to Github. It‚Äôs pretty ugly but if you‚Äôre interested in how it works, you can check it out. I probably should‚Äôve used regular expressions in hindsight instead of blowing up strings into arrays but I figured that with the word list at around 2,300 words, it really doesn‚Äôt matter how inefficient I am!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/hungtruong/Wordle-Bot&quot;&gt;https://github.com/hungtruong/Wordle-Bot&lt;/a&gt;&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2022/WordleScreenshot.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unless you‚Äôve been living under a rock for the past few days, you‚Äôve heard of this game online called Wordle. It‚Äôs been growing like crazy and the New York Times even wrote a &lt;a href=&quot;https://www.nytimes.com/2022/01/03/technology/wordle-word-game-creator.html&quot;&gt;profile on the creator&lt;/a&gt;. If you know me, you know I like automating things, so it should be no surprise that I decided to automate playing the game. Here‚Äôs a blog post on how I did it.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 10 Jan 2022 00:00:00 -0800</pubDate>
        <link>https://www.hung-truong.com/blog/2022/01/10/i-created-a-robot-to-solve-wordle-for-me/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2022/01/10/i-created-a-robot-to-solve-wordle-for-me/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Cloning Zwift on iOS Part 5: SwiftUI and Combine</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2021/Zswift_main_screen.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I recently switched teams at Amazon to one that is using SwiftUI and Combine, so I finally have a good excuse to learn the two. I am somewhat familiar with Functional Reactive Programming from using RxSwift at Lyft, but I‚Äôve only really dabbled a bit with SwiftUI.&lt;/p&gt;

&lt;p&gt;I decided to spend some time last weekend (and this weekend) rewriting most of my Zwift clone app to use SwiftUI and Combine, and here‚Äôs some of the stuff I learned along the way.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;moving-to-combine&quot;&gt;Moving to Combine&lt;/h3&gt;

&lt;p&gt;When I originally wrote the Zswift app, I didn‚Äôt really spend too much time on making sure the data model or architecture was the cleanest or anything. It‚Äôs really more of a hodgepodge of explorations and trying to get stuff to just work. Because of this, I probably violated a bunch of best practices. I had a ‚ÄúWorkout‚Äù object that stored all of the different information that is needed to model a workout, but I also threw a bunch of logic and functions in there that probably didn‚Äôt make sense.&lt;/p&gt;

&lt;p&gt;For example, the Workout had info on each segment of the workout, along with the duration and amount of power for the segment. As the workout progressed, I would store the elapsed time as well as a bunch of other state variables like the current segment and other variables like time in current segment that I used to drive the UI. Since I kept a bunch of variables to keep track of state, it‚Äôs possible that some of them would get out of sync with each other, and that would cause bugs. I think I had a bunch of off-by-one errors where I would reach the end of an index and crash or the time within a segment would be off by 1 so I‚Äôd be at 2:01/2:00 as far as progress went. Those kind of bugs.&lt;/p&gt;

&lt;p&gt;In moving to Combine, my goal was to have the state of the workout flow from the one thing about the workout that actually changes: the elapsed time.&lt;/p&gt;

&lt;p&gt;The elapsed time literally decides which segment I‚Äôm in, how long I‚Äôve been working out (duh), how long I‚Äôve been in the current segment, how hard I should be pedaling, etc. I ended up creating a separate class to keep track of the state of the workout, and just use the workout as a static definition of the workout. I could‚Äôve done this before migrating to use Combine, but like I said, it was working and I didn‚Äôt feel the need to refactor.&lt;/p&gt;

&lt;p&gt;In my current setup, the WorkoutManager has a @Published variable that keeps track of the elapsed time, and then I create a bunch of other publishers based on that one. I also have publishers that combine (imagine that) with other publishers. For example, I have a publisher called ‚ÄútimeInCurrentSegmentPublisher‚Äù that publishes the amount of time that I‚Äôve been in the current segment. I combine this publisher with the ‚ÄúcurrentSegmentPublisher‚Äù which gives me the current segment, and use this to calculate the percentage progress for the current segment. I have to combine the two because each publisher only gives me one thing.&lt;/p&gt;

&lt;p&gt;I weighed the benefits of creating publishers with multiple tuples of values, but in most cases it didn‚Äôt make sense, and I think it goes against the concept of making the streams composable, but I did end up making one for currentSegmentPublisher since it calculates the current segment and the current segment‚Äôs index at the same time anyway. Even as I‚Äôm writing about it now, I‚Äôm not sure if the better way is to create two different publishers since it‚Äôs kinda clunky to grab the desired value from the tuple.&lt;/p&gt;

&lt;p&gt;Anyway, the result here is that my Workout object doesn‚Äôt have any more state at all. I could make it a struct but there‚Äôs some SwiftUI requirement for my Workout to be a class if I want to use it in a Modal view (which the workout detail is set up as) so for now I‚Äôll just leave it as-is. I could also create a view model if I really wanted the Workout object to be a struct.&lt;/p&gt;

&lt;p&gt;I‚Äôm sure I could optimize the Combine publishers even more but since they‚Äôre working now I‚Äôll just leave them.&lt;/p&gt;

&lt;h3 id=&quot;swiftui&quot;&gt;SwiftUI&lt;/h3&gt;
&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2021/xcode_swiftui_previews.png&quot; /&gt;
	&lt;figcaption&gt;An example of the live SwiftUI previews that update automatically&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The general opinion that I see from others about SwiftUI is that the more people use it, the more they like it, and that it really forces you to rethink how you define interfaces. I heard this a million times but until you actually play around with it in a non-trivial example I feel like it‚Äôs hard to really understand it.&lt;/p&gt;

&lt;p&gt;The gist is that SwiftUI is a declarative way to define your UIs instead of an Imperative way. The difference can seem subtle until you start doing stuff. What it means is that instead of telling the computer how to do something, you just tell it what you want. If that‚Äôs confusing then yeah, actually it is confusing.&lt;/p&gt;

&lt;p&gt;I guess to put it another way, in UIKit you can define views as objects, give them properties, add them to a parent view and then set some constraints on them. In this imperative example, it‚Äôs up to you to define every step to the computer to tell it what to do, and hope that your interface matches what you were thinking.&lt;/p&gt;

&lt;p&gt;In a declarative syntax, you can describe what you want, and add some modifiers to it if you need to have more control over the actual output. From there, you also define state variables or observed objects that the SwiftUI view will use to actually come up with the completed interface.&lt;/p&gt;

&lt;p&gt;Imagine that you want to show a list of dog breeds. In an imperative system you need to set up the collection view controller, fill out your functions for ‚ÄúcellAtIndex‚Äù and ‚ÄúdidSelectCellAtIndex‚Äù etc, and supply the cells. In a declarative system you can say ‚ÄúI want a list of ‚ÄòDogViews‚Äô that use the ‚ÄòDog‚Äô model‚Äù and define a ‚ÄúNavigationLink‚Äù to define what happens when you tap on the cell.&lt;/p&gt;

&lt;p&gt;The interesting part to me was that the SwiftUI view is an immutable struct, so you can‚Äôt modify the view as you would a UIView when states change. Of course your view can respond to changes in the data model, but all of those states need to be determined at compile time rather than runtime.&lt;/p&gt;

&lt;p&gt;One of the best parts about SwiftUI is how the ‚Äúlive‚Äù previews work. They aren‚Äôt exactly real time, but fast enough that the feedback loop between writing UI code and seeing the result is very tight. In the past I‚Äôve had to write code to update a UI (or use Interface Builder), then build and run, and go to the part of the app where the UI I changed was. This could mean it would be a few seconds or minutes before I saw whether the change I just wrote actually worked. In SwiftUI you make a change and once you‚Äôre done writing it, the preview window will show it. The system isn‚Äôt perfect, of course, as sometimes it can‚Äôt compile and you need to reenable the live preview. But it‚Äôs really the best tradeoff between the WYSIWYG style of Interface Builder and the readability of programmatic view code. I‚Äôm sure the tools will improve in the future, and I‚Äôm quite certain that this ease of previewing code will be one of the factors that will motivate people to switch to SwiftUI.&lt;/p&gt;

&lt;h3 id=&quot;the-workout-view&quot;&gt;The Workout View&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2021/workout_view.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I modeled my original interface after the Zwift app. In the Zwift app, you can see the workout represented as a bunch of rectangles representing segments that get taller when the target power is higher. The width is determined by the length of the segment.&lt;/p&gt;

&lt;p&gt;I basically copied this and implemented it with a UIStackView and percentage constraints. I thought it was pretty cool but one thing that bothered me is the warmup and cooldown segments. Those have a starting value and ending value that are different (warmups start lower and end higher). I didn‚Äôt want to spend too much time drawing the start of the rectangle to be lower than the end (plus it wouldn‚Äôt technically be a rectangle at that point) so I just used the ‚Äúlower‚Äù value and set them as flat rectangles.&lt;/p&gt;

&lt;p&gt;I decided to practice some custom drawing in SwiftUi to properly draw the warmup and cooldown segments and I‚Äôm pretty happy with the result. The view is drawn by taking the desired start and end heights and width, and drawing a path with it. I used ‚Äúpath.addArc‚Äù to get a nice rounded corner effect, though it isn‚Äôt perfect. I also draw a 1px wide vertical line to show where I am in the workout by offsetting it from the left by a percentage of the view‚Äôs width.&lt;/p&gt;

&lt;p&gt;I also went ahead and stole the color scheme from the Zwift app which makes it look a bit more polished.&lt;/p&gt;

&lt;h3 id=&quot;the-workout-detail&quot;&gt;The Workout Detail&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2021/side_by_side.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In addition to updating the workout representation view, I also decided to just overhaul the entire workout detail view (the view I see when I‚Äôm in a workout). The old interface was pretty basic with a bunch of grids and text boxes that didn‚Äôt really have any visual separation from each other, aside from the size and distribution of the labels.&lt;/p&gt;

&lt;p&gt;I ended up experimenting with the ‚ÄúGroupBox‚Äù component of SwiftUI which is pretty simple but makes a big difference just in terms of separating out the different interface elements. I also added some accent colors and icons to some of the text labels, and I added progress indicators to the segment time and elapsed time sections just to make them stand out more. Overall I really like the new workout detail view which is good since I‚Äôm literally the only person who uses this app.&lt;/p&gt;

&lt;p&gt;I have some more ideas for enhancements, like recreating the Apple Watch heart rate animation which shows a heart pulsing at your actual heart rate. The animation seamlessly updates when your heart rate changes, which is pretty cool.&lt;/p&gt;

&lt;p&gt;I also want to add some more tracking informtion, like a histogram (maybe with candlestick charts or sparklines) of the wattage and heart rate info. This would be nice to have but the HealthKit integration already includes the heart rate chart. And I don‚Äôt know if I care too much about the wattage graph.&lt;/p&gt;

</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2021/Zswift_main_screen.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I recently switched teams at Amazon to one that is using SwiftUI and Combine, so I finally have a good excuse to learn the two. I am somewhat familiar with Functional Reactive Programming from using RxSwift at Lyft, but I‚Äôve only really dabbled a bit with SwiftUI.&lt;/p&gt;

&lt;p&gt;I decided to spend some time last weekend (and this weekend) rewriting most of my Zwift clone app to use SwiftUI and Combine, and here‚Äôs some of the stuff I learned along the way.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 02 May 2021 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2021/05/02/zwift-clone-swiftui-combine/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2021/05/02/zwift-clone-swiftui-combine/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Reverse Engineering Quibi: Protocol Buffers and HLS Streaming</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/Quibi_Logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I‚Äôve had a lot of free time in the past few weeks so I decided to spend some of it working on side projects. I really enjoy reverse engineering apps, so I decided to take a look at Quibi.&lt;/p&gt;

&lt;p&gt;Quibi (short for ‚ÄúQuick Bites‚Äù) is a video streaming app/service that has a bunch of shows that are short. The idea is that you can sit on a bus ride and consume an episode or two, depending on how long your commute is. One of the constraints of the platform is that you can only watch these videos on a phone or tablet with the app installed.&lt;/p&gt;

&lt;p&gt;Since everyone is stuck indoors for a while this constraint is kinda stupid and most people would probably like to watch their videos on their big tv rather than huddle around a phone, which is what Emily and I had to do to watch that stupid viral show about the &lt;a href=&quot;https://twitter.com/zachraffio/status/1250273191810875392&quot;&gt;terrible wife with the golden arm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, I had an idea to write a tvOS app that would work with Quibi, so you could watch your terrible shows on your tv. Here‚Äôs what I learned trying to reverse engineer the Quibi app.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;setting-up-charles&quot;&gt;Setting up Charles&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2020/charlesproxy.jpg&quot; /&gt;
	&lt;figcaption&gt;Get out of my head Charles!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The first rule of reverse engineering club is that you should probably install &lt;a href=&quot;https://www.charlesproxy.com/&quot;&gt;Charles proxy&lt;/a&gt; on your computer, and point your phone to it. This lets you inspect network requests and figure out the API for an app. Depending on how stringent the app‚Äôs security settings are, you can either learn a lot, or very little from Charles.&lt;/p&gt;

&lt;p&gt;The way Charles works is that you basically route all of your traffic from your iPhone to your computer. You need to install a certificate on your phone and trust it, so that your phone thinks that it‚Äôs going through a secure connection when it‚Äôs really getting owned. But since you‚Äôre self-owning, it‚Äôs generally okay. I‚Äôm not going to do a full tutorial on how to do this because you can just read the docs.&lt;/p&gt;

&lt;p&gt;You can also buy a version of Charles that runs directly on your phone. I did this, but I also find it easier to work with on my computer, so I used the free version that stops working every 30 minutes. I should probably buy the full version.&lt;/p&gt;

&lt;p&gt;Anyway, once you get the proxy working on your phone and add the proper domains to the allowed list, you should start seeing your network requests and responses pop up in Charles after firing up the app and doing stuff.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/charlesinaction.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It turns out (luckily for me) that Quibi doesn‚Äôt utilize certificate pinning (at least at the time I‚Äôm writing this article). Cert pinning is a way to prevent snooping of network requests by embedding a certificate (or maybe a hash of it or a public key) to be trusted into the actual app binary. This means that adding the additional Charles certificate won‚Äôt work because the app won‚Äôt accept it. The process of pinning is kind of a pain in the ass, which is why a lot of companies don‚Äôt do it.&lt;/p&gt;

&lt;p&gt;Because Quibi doesn‚Äôt use cert pinning, that means I can observe all of the requests and responses that the app sends and receives, which makes it a lot easier to reverse engineer!&lt;/p&gt;

&lt;h3 id=&quot;authentication&quot;&gt;Authentication&lt;/h3&gt;
&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2020/quibi_oauth.png&quot; /&gt;
	&lt;figcaption&gt;This is what the Quibi OAuth request/response look like with the private stuff removed&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I won‚Äôt go too much into the way that Quibi does authentication, as it appears to be a pretty basic implementation of OAuth 2.0, using auth0 as a service provider. It doesn‚Äôt appear to use a client secret key, as I‚Äôm able to just replay the request with my username, password, client id and other parameters and get a valid access token back. One interesting thing to note here is how short the token expiration is, just 2700 seconds, or 45 minutes.&lt;/p&gt;

&lt;p&gt;Once authentication happens, the access token is used in subsequent requests as the authorization bearer header tokens.&lt;/p&gt;

&lt;p&gt;I didn‚Äôt bother to set up a new app with authentication because I wanted to get to the meat of the app, and started taking a look at how the app‚Äôs requests and responses were structured, which brings us to‚Ä¶&lt;/p&gt;

&lt;h3 id=&quot;protocol-buffers&quot;&gt;Protocol Buffers!&lt;/h3&gt;

&lt;p&gt;I‚Äôll be honest, I was initially pretty annoyed when I read the line,&lt;/p&gt;

&lt;p&gt;&lt;code&gt;content-type: application/protobuf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;in Charles. That‚Äôs because &lt;a href=&quot;https://developers.google.com/protocol-buffers&quot;&gt;protocol buffers&lt;/a&gt; are pretty annoying to work with unless you actually have access to the .proto files that were used to generate the schema for the response.&lt;/p&gt;

&lt;p&gt;Luckily, I have experience working with protobufs (as the cool kids call them) because we started implementing them at Lyft last year. As a primer, protocol buffers basically define requests and responses, and their types, in a way that can be shared between servers, clients, etc. It does so in a way that reduces the amount of redundancy in the data. So instead of looking at a JSON file that labels each key/value pair for each item in an array, you just see the values, and the keys (and their types) are essentially encoded outside of the format itself. That‚Äôs probably a gross simplification of protobufs, but that‚Äôs basically how I understand their functionality in lay person terms.&lt;/p&gt;

&lt;p&gt;So getting back to Quibi, I was seeing calls to endpoints like&lt;/p&gt;

&lt;p&gt;&lt;code&gt;https://qlient-api.quibi.com/quibi.qlient.api.home.Home/GetHomeCards&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;with a bunch of wacky characters, urls, and names and descriptions of tv shows. Looking at the raw text gave me an idea of what was being returned, but there was also a lot of data that I couldn‚Äôt see because it was encoded as a protobuf response.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/protobuf_raw.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I found a tool written by someone named &lt;a href=&quot;https://github.com/mildsunrise&quot;&gt;Alba Mendez&lt;/a&gt; called &lt;a href=&quot;https://github.com/mildsunrise/protobuf-inspector&quot;&gt;protobuf-inspector&lt;/a&gt; which allows you to visualize the values in a protobuf response. Once I threw the response into this tool, it started to make a lot more sense.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/protobuf_inspector.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, I could see that the home cards were displaying structured information for each of the cards in the app. The hierarchy seemed to be a card with series info, a mp4 preview link, and then info about that particular episode. There were also some values that didn‚Äôt really seem useful, like ‚Äú&amp;lt;varint&amp;gt; = 1‚Äù which could‚Äôve meant anything, like a bool value or episode number.&lt;/p&gt;

&lt;p&gt;The tool has a way to set up object definitions, so that when you run it again, you see the key names and types that you defined. This is helpful if you‚Äôre trying to guess what something is and you want to compare it against a few different objects. Here‚Äôs what the response looked like once I tried defining some of the keys:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/protobuf_inspector_labeled.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This makes the response look a bit more logical. I really guessed some of these, so if someone from Quibi actually reads this maybe you can confirm. I probably spent more time on this than I needed, because really all I want is to show a list of shows and maybe even start watching a show.&lt;/p&gt;

&lt;p&gt;To waste even more time, I ended up defining this response in a .proto file, compiled it into Swift with swift-protobuf and got a Swift app to parse the response into real Swift structs! Here is the proto definition:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/ad4a62c9c1d86a2c187267bc353d8284.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The generated proto code in Swift is pretty big, so I‚Äôll leave it as an exercise to the reader to run it through swift-protobuf.&lt;/p&gt;

&lt;p&gt;Here‚Äôs what it looks like running in the debugger in my app:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/proto_swift.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So with what I have written about so far, I could actually write a functioning Quibi client that supports logging in, making a request to get a list of tv show cards, and displays them, with a ‚Äúlive‚Äù preview, just like the real app. That doesn‚Äôt matter much unless you can actually watch the show, though.&lt;/p&gt;

&lt;h3 id=&quot;hls-and-video-streaming&quot;&gt;HLS and Video Streaming&lt;/h3&gt;

&lt;p&gt;I apologize for the anti-climactic finale of this blog post, but this is where I got stuck.&lt;/p&gt;

&lt;p&gt;Before I tried reverse engineering this app, I pretty much knew nothing about video streaming. When looking at the chain of requests that the app makes, it appears that the app hits an endpoint called ‚ÄúGetPlaybackInfo‚Äù and sends a payload of series id, season #, and episode # along with a mystery UUID that I haven‚Äôt seen anywhere else in the app requests/responses, then receives a link to a ‚Äúlicense‚Äù url, a few links to .m3u8 resources and some cookies for accessing those .m3u8 resources.&lt;/p&gt;

&lt;p&gt;Then the app makes a request to the license url with some form encoded data and receives some other encoded data back. Finally, the app makes a request to one of the .m3u8 files and starts streaming the video.&lt;/p&gt;

&lt;p&gt;I did &lt;a href=&quot;https://developer.apple.com/streaming/&quot;&gt;some research&lt;/a&gt; and it looks like a .m3u8 url basically provides the client with a way or ways to display the video to the user. It can include things like different video streams with varying quality, and it looks like it even has some subtitle file support.&lt;/p&gt;

&lt;p&gt;I tried just replaying the call to the .m3u8 file with the same authentication cookie and it unfortunately didn‚Äôt work. I think that the license url provides the app with a way to decode the video, and without knowing what to send or how to decode it, I think I‚Äôm essentially stuck.&lt;/p&gt;

&lt;p&gt;I sort of didn‚Äôt expect to be able to finish this app anyway, so I‚Äôm pretty happy with how far I got. I also figure that if I try to go any further with this, Quibi will probably try to sue me or something, so it probably isn‚Äôt worth it. In any case, I did learn a lot from this project, and hopefully you have too, from reading this post. If you have any ideas on how I would proceed or if you enjoyed this post, feel free to let me know!&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2020/Quibi_Logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I‚Äôve had a lot of free time in the past few weeks so I decided to spend some of it working on side projects. I really enjoy reverse engineering apps, so I decided to take a look at Quibi.&lt;/p&gt;

&lt;p&gt;Quibi (short for ‚ÄúQuick Bites‚Äù) is a video streaming app/service that has a bunch of shows that are short. The idea is that you can sit on a bus ride and consume an episode or two, depending on how long your commute is. One of the constraints of the platform is that you can only watch these videos on a phone or tablet with the app installed.&lt;/p&gt;

&lt;p&gt;Since everyone is stuck indoors for a while this constraint is kinda stupid and most people would probably like to watch their videos on their big tv rather than huddle around a phone, which is what Emily and I had to do to watch that stupid viral show about the &lt;a href=&quot;https://twitter.com/zachraffio/status/1250273191810875392&quot;&gt;terrible wife with the golden arm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, I had an idea to write a tvOS app that would work with Quibi, so you could watch your terrible shows on your tv. Here‚Äôs what I learned trying to reverse engineer the Quibi app.&lt;/p&gt;

</description>
        
        <pubDate>Tue, 12 May 2020 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2020/05/12/reverse-engineering-quibi/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2020/05/12/reverse-engineering-quibi/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Cloning Zwift on iOS Part 4: Workout View</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workoutview.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I haven‚Äôt been working as much on my workout app since it‚Äôs been good enough for me to use as a replacement for Zwift with the most recent changes I wrote about.&lt;/p&gt;

&lt;p&gt;I got a new iPhone so I had to build and install the app on my phone again, and after firing up Xcode I decided to go ahead and add a few more things that I was meaning to add to the app. The app hasn‚Äôt had a good visualization of workouts so I ended up creating an interface for that.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;taking-cues-from-zwift&quot;&gt;Taking cues from Zwift&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/emilysshortmix.jpg&quot; /&gt;
	&lt;figcaption&gt;Emily&apos;s Short Mix, as seen in Zwift&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I didn‚Äôt want to reinvent the wheel here, so I decided to just copy how Zwift shows workouts, because it works pretty well. Not that I‚Äôm trying to write a whole 3d cycling MMORPG, but it helps to know where I am in a workout besides time elapsed and time remaining.&lt;/p&gt;

&lt;p&gt;The general idea is to code each segment with its color based on the difficulty zone, and its height based on the target wattage of that segment.&lt;/p&gt;

&lt;p&gt;I previously noted that the target wattage of a segment is based on the product of your own FTP and a multiplier. I did some reverse engineering and found that the color coded ‚Äúzones‚Äù are based on this formula:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;0-60% = zone 1&lt;/li&gt;
  &lt;li&gt;61-75% = zone 2&lt;/li&gt;
  &lt;li&gt;76-89% = zone 3&lt;/li&gt;
  &lt;li&gt;89-104% = zone 4&lt;/li&gt;
  &lt;li&gt;105-118% = zone 5&lt;/li&gt;
  &lt;li&gt;119%+ = zone 6&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I then just mapped the zones with the same colors, with zone 1 being gray, 2 being blue, etc. For the warmup and cooldown segments I just take the high power of those segments.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/5a3c38401b0f8a5dffbaf543bdd67e31.js&quot;&gt;&lt;/script&gt;

&lt;h3 id=&quot;creating-a-view&quot;&gt;Creating a View&lt;/h3&gt;

&lt;p&gt;Creating custom views from scratch is really not my strong point, but I know enough about autolayout and UIKit to implement whatever views a designer throws at me. Unfortunately I don‚Äôt have designer to throw designs at me so I had to come up with something on my own.&lt;/p&gt;

&lt;p&gt;I was originally going to use a .xib file and set things up programmatically, but I realized that I really didn‚Äôt need a .xib and could do everything programmatically. I usually go with a hybrid approach to make the complicated parts easier, but in this case doing stuff in code is actually less complicated.&lt;/p&gt;

&lt;p&gt;I ended up going with a stack view based layout, where I inserted the views into a horizontal stack view. The view itself has a width anchor based on the percentage of time it takes (if it‚Äôs a 10 minute segment in a 30 minute workout then its width would be 33.3% of the container), and then a subview which has a height anchor based on the relative intensity of the segment. It works out pretty well.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/1166ad11e921cb21470feed64ffb9e3f.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;I threw the view into a UITableViewCell so I could visually get a clue what the workout I was selecting looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workoutslist.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;finally-some-progress&quot;&gt;Finally Some Progress&lt;/h3&gt;

&lt;p&gt;I was pretty happy with the stack view representation of the workouts and left it that way for a few months. Today I was refactoring some stuff and implemented something that was really bothering me for a while. In Zwift you get a really nice view of the upcoming segments, and where you are in your current segment. My app is not really good at this, and there was a bug where it not correctly display the upcoming segment sometimes because of the way it handled workout segment equality. Essentially it was looking for the index of a segment but if there were two exact same segments it would assume the first one was the correct one.&lt;/p&gt;

&lt;p&gt;Anyway, I fixed that bug by just tracking the actual index of the segment I was on.&lt;/p&gt;

&lt;p&gt;The other thing I wanted to add was a progress view, so I broke the Workout view into its own class and added a ‚Äúprogress‚Äù indicator, which is just a horizontal slider that slides over as you‚Äôre working out.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/c5963ec63481f39877e562da3c93a13a.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;This is what it looks like, if I was working out at 10x the regular speed (which would be easier I guess).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/ZSwiftWorkoutView.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That‚Äôs about it for now! I think this makes my app pretty usable and until I find another thing I need to add to it, I feel like this is a good replacement for Zwift. If you happen to need the exact same thing that I do and also happen to have the same exact exercise bike, feel free to clone the &lt;a href=&quot;https://github.com/hungtruong/Zswift&quot;&gt;Github&lt;/a&gt; repo and use the app!&lt;/p&gt;

</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workoutview.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I haven‚Äôt been working as much on my workout app since it‚Äôs been good enough for me to use as a replacement for Zwift with the most recent changes I wrote about.&lt;/p&gt;

&lt;p&gt;I got a new iPhone so I had to build and install the app on my phone again, and after firing up Xcode I decided to go ahead and add a few more things that I was meaning to add to the app. The app hasn‚Äôt had a good visualization of workouts so I ended up creating an interface for that.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 29 Sep 2019 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2019/05/03/making-a-zwift-clone-part-4-workout-view/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2019/05/03/making-a-zwift-clone-part-4-workout-view/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Introducing BarkMode: Bark Detection + Dark Mode</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barkmode.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was in California about a month ago for work and I was able to attend a few events during WWDC week. I read a lot about the new features and APIs but didn‚Äôt really have a lot of time to mess with stuff like SwiftUI, etc. I got a stupid idea for a project but I didn‚Äôt really take the time to work on it until this weekend. Now I‚Äôd like to introduce you to my latest app: Bark Mode!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;wtf-is-bark-mode&quot;&gt;WTF is Bark Mode?&lt;/h3&gt;

&lt;p&gt;Right after the new APIs were announced at WWDC, I scanned them for anything that might be interesting. I really like being one of the first to try out a new technology, but I figured that everyone would be all over &lt;a href=&quot;https://developer.apple.com/xcode/swiftui/&quot;&gt;SwiftUI&lt;/a&gt;, and I was right. I found a somewhat obscure new feature as part of CreateML that allowed users to generate a ML model that could classify sounds.&lt;/p&gt;

&lt;center&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;This SoundAnalysis API in particular seems relevant to my typically immature interests. üí® &lt;a href=&quot;https://twitter.com/hashtag/WWDC19?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#WWDC19&lt;/a&gt; &lt;a href=&quot;https://t.co/0Aipwa99Lu&quot;&gt;https://t.co/0Aipwa99Lu&lt;/a&gt;&lt;/p&gt;&amp;mdash; Hung Truong (@hungtruong) &lt;a href=&quot;https://twitter.com/hungtruong/status/1135947285035020290?ref_src=twsrc%5Etfw&quot;&gt;June 4, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; 
&lt;/center&gt;

&lt;p&gt;Of course the first thing I thought of was to make an app that could detect farts. I could even make it so that a fart would toggle dark mode off/on. Though I wanted to start experimenting right away, I discovered that you actually needed to have the newest version of macOS Catalina installed in addition to the newest Xcode to use Create ML, and I didn‚Äôt have my personal computer handy and couldn‚Äôt install a beta OS on my work computer, so I ended up not building it.&lt;/p&gt;

&lt;p&gt;Fast forward to this weekend, and I eventually got Catalina and all the other prerequisites I needed to get started. Instead of ‚ÄúFarkMode‚Äù which doesn‚Äôt really make sense, I decided to switch it up this time and detect dogs barking, since ‚ÄúBark‚Äù rhymes better with ‚ÄúDark‚Äù than ‚ÄúFart.‚Äù Plus everybody loves dogs! Perhaps I‚Äôm finally maturing in my old age (probably not).&lt;/p&gt;

&lt;p&gt;Anyway, here‚Äôs the steps I needed to take to make my BarkMode app.&lt;/p&gt;

&lt;h3 id=&quot;creating-the-model&quot;&gt;Creating the model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/Create ML.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I had seen some videos of people making image classifiers using &lt;a href=&quot;https://developer.apple.com/documentation/createml&quot;&gt;CreateML&lt;/a&gt; in the past. It seemed as easy as creating a few folders with different labels and images of those things. You would have to create one batch for training images, and another for optionally testing the models once they were created.&lt;/p&gt;

&lt;p&gt;The process for creating an audio classifier is pretty similar except that you need to use sound files instead of images. I started by finding a bunch of stock sound effects of dogs barking. I also found a bunch of random stock sound effects of things like bells ringing, wind blowing, and other stuff that I could label as ‚Äúnot barking.‚Äù I feel like this is a pretty wacky way to do classification but if there‚Äôs another way that doesn‚Äôt involve downloading a bunch of random stock sound effects, that would be awesome.&lt;/p&gt;

&lt;p&gt;At first I took larger sound files and chopped them up into really short ones. This way I had a bunch of training data that I can use both to train and test my classifier.&lt;/p&gt;

&lt;p&gt;However, when I ran the Create ML app on the training data, I got an obscure error that the training had been interrupted. I saw that the app thought I only had 3 files when I had created a bunch more. I believe that Create ML is unable to train with really short audio files (less than a second). I ended up inserting the full sound files instead of the short ones. At this point I think I had about 8 or 9 files for either barking or not barking.&lt;/p&gt;

&lt;p&gt;The model was created but it had a really terrible accuracy. I decided to just add a bunch more files both of dogs barking and things that weren‚Äôt dogs barking. The more examples I added the ‚Äúbetter‚Äù the model became.&lt;/p&gt;

&lt;h3 id=&quot;polishing-the-model&quot;&gt;Polishing the model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barktest.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Create ML has a feature where you can test your audio classifier with audio data coming from your own microphone. When I did this I noticed that the model was too biased towards no sound being barking. I‚Äôm not sure why but maybe white noise sounds more like barking than an old timey car horn?&lt;/p&gt;

&lt;p&gt;I ended up adding a bunch of white noise and recorded some nothing to add to the model. This helped quite a bit. The last thing I did was also record some audio of myself talking so that the model would not recognize me talking as barking. This was useful so I wouldn‚Äôt get false positives during my demo video (which I ended up getting anyway in a bunch of outtakes).&lt;/p&gt;

&lt;h3 id=&quot;integrating-the-model-in-the-app&quot;&gt;Integrating the model in the app&lt;/h3&gt;

&lt;p&gt;I created a new app and promptly enabled ‚ÄúUse SwiftUI.‚Äù Then I promptly made a new project because I couldn‚Äôt figure out where my ViewController class was! I‚Äôll take a look at SwiftUI later but for now I‚Äôll focus on getting my BarkMode app actually working.&lt;/p&gt;

&lt;p&gt;The Create ML app made a model that I could then copy into my app. It was really as simple as dragging it from one place to another.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/mlmodel.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking at the model in Xcode, it describes an interface with an input of ‚ÄúaudioSamples‚Äù that takes a MultiArray of (Float32 15600). I assumed this had something to do with the number of samples per second and the bitrate of the audio. I fiddled around with the AudioToolbox framework and a few other lower level audio APIs until I discovered Apple‚Äôs documentation on the &lt;a href=&quot;https://developer.apple.com/documentation/soundanalysis&quot;&gt;SoundAnalysis&lt;/a&gt; framework which provides a much, much easier method of feeding audio to the model.&lt;/p&gt;

&lt;p&gt;I implemented the steps described and extended my ViewController to conform to the SNResultsObserving protocol. Then it was a few simple steps to write some logic to handle the app detecting no barking to barking and back to no barking, and toggling the dark mode off and on.&lt;/p&gt;

&lt;p&gt;Finally, I added a label and an image view that takes an image with different assets for the light and dark modes, which changes automatically based on the current setting.&lt;/p&gt;

&lt;p&gt;The model runs pretty quickly. Here‚Äôs a gif of the terminal output from the logging whenever some sound data comes in:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barknobark.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you want to see the app in action, I created this video to demonstrate:&lt;/p&gt;

&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/0XRxA1cEHio&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;p&gt;As it is, this app isn‚Äôt very useful for much of anything, but I could see a few potential uses for the model I trained:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Write an app that detects your dog is unhappy when it‚Äôs at home and triggers a home automation action like playing music or shooting a treat at your dog&lt;/li&gt;
  &lt;li&gt;Detect if suspicious sound at your house is a robber or just a dog&lt;/li&gt;
  &lt;li&gt;Classify different types of barks and make a dog barking translation app&lt;/li&gt;
  &lt;li&gt;For people who are allergic to dogs, advance warning of incoming dog&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I‚Äôve posted the full project to &lt;a href=&quot;https://github.com/hungtruong/BarkMode&quot;&gt;Github&lt;/a&gt; in case you‚Äôre curious about how I implemented the app. I didn‚Äôt upload the Create ML project but it just has a bunch of sound effect files and files of me saying gibberish. The model included in the Github repo should work fine anyway.&lt;/p&gt;

&lt;p&gt;I‚Äôm pretty happy with my end result. It‚Äôs fun to play around with new APIs and now I can say that I‚Äôve trained an ML model to classify sounds, in addition to implementing Dark Mode! Hopefully I‚Äôll have some time to play with other APIs that were introduced to iOS soon.&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/barkmode.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was in California about a month ago for work and I was able to attend a few events during WWDC week. I read a lot about the new features and APIs but didn‚Äôt really have a lot of time to mess with stuff like SwiftUI, etc. I got a stupid idea for a project but I didn‚Äôt really take the time to work on it until this weekend. Now I‚Äôd like to introduce you to my latest app: Bark Mode!&lt;/p&gt;

</description>
        
        <pubDate>Sun, 14 Jul 2019 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2019/07/14/introducing-bark-mode-bark-detection-dark-mode/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2019/07/14/introducing-bark-mode-bark-detection-dark-mode/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Cloning Zwift on iOS Part 3: HealthKit and a WatchOS App!</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/healthkit.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I‚Äôve been a bit slow to update my blog series about trying to make a clone of
Zwift, but not because I‚Äôve stopped working on it. Rather, I‚Äôve been able to use
the ‚ÄúMVP‚Äù of what I‚Äôve built so far in parts
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;1&lt;/a&gt;
and
&lt;a href=&quot;/blog/2019/04/07/making-a-zwift-clone-part-2/&quot;&gt;2&lt;/a&gt;,
and I was finding that the time I spent working on my app could be used actually
working out. Like, I literally would write an implementation of something, but
it would take so much of my time that I couldn‚Äôt test it out and I‚Äôd have to go
to bed‚Ä¶ Still, I was missing a few important features in my app, so I‚Äôve been
slowly working on them in between working on my fitness.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;hooking-up-the-apple-watch&quot;&gt;Hooking up the Apple Watch&lt;/h3&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwift-bluetooth.png&quot; /&gt;
	&lt;figcaption&gt;Zwift‚Äôs interface for connecting the Apple Watch works really well&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;One of the great things about Zwift is how much support they provide for
different fitness accessories, including the Apple Watch. Unfortunately, the
Apple Watch hardware is not set up to allow arbitrary Bluetooth connections like
my exercise bike was in part 1. Instead, to access the user‚Äôs health information
like heart rate, you need to write a full blown WatchOS app!&lt;/p&gt;

&lt;p&gt;Luckily, this wasn‚Äôt my first rodeo as I worked on the Apple Watch app for
Starbucks, so I was able to add a Watch app extension target in my project
pretty quickly.&lt;/p&gt;

&lt;p&gt;I Googled for how to get a user‚Äôs heart rate programmatically, came across a
&lt;a href=&quot;https://stackoverflow.com/questions/38158841/live-heart-rate-in-watchos-3&quot;&gt;promising StackOverflow
post&lt;/a&gt;
with a link to a &lt;a href=&quot;https://github.com/coolioxlr/watchOS-3-heartrate/blob/master/VimoHeartRate%20WatchKit%20App%20Extension/InterfaceController.swift&quot;&gt;Github
project&lt;/a&gt;,
and was able to get it implemented myself. However, as I looked at the
copy-pasta code, it seemed sort of wrong to me. The code was starting a workout
session, but then created an object query that would run a closure whenever a
new heart rate sample (older than a certain date) was added by the workout. This
seems like a really roundabout way to get heart rate samples, and I wondered if
Apple had a better API to accomplish this.&lt;/p&gt;

&lt;h3 id=&quot;implementing-healthkit&quot;&gt;Implementing HealthKit&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/healthkit_hero.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I ended up finding some Apple &lt;a href=&quot;https://developer.apple.com/documentation/healthkit/workouts_and_activity_rings/speedysloth_creating_a_workout&quot;&gt;sample
code&lt;/a&gt;
that showed a better way to fetch heart rate data. The solution is to use some
new features introduced in WatchOS 5 that allow for creation of a workout
directly on the Apple Watch. The Apple doc I linked explains it pretty well, but
the steps are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ask the user for permission to track their heart rate data&lt;/li&gt;
  &lt;li&gt;Create a workout configuration (e.g. an indoor cycling workout) and a workout
session, along with its associated workout builder&lt;/li&gt;
  &lt;li&gt;Start the session and tell the workout builder to start collecting data samples&lt;/li&gt;
  &lt;li&gt;Respond to the delegate method &lt;em&gt;‚ÄúworkoutBuilder(_:didCollectDataOf:)‚Äù&lt;/em&gt; to collect
a bunch of samples, including heart rate information&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In code it looks something like this:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/66b941af4f5ce3b9a66cd041fd94f3d5.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/918839f7ba4a213c92817b387a6fd91f.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Rather than add some UI to the watch app to start a workout session, the iPhone
version of &lt;em&gt;HKHealthStore&lt;/em&gt; has a function called
&lt;a href=&quot;https://developer.apple.com/documentation/healthkit/hkhealthstore/1648358-startwatchapp&quot;&gt;startWatchApp(with:completion:)&lt;/a&gt;
which will send a workout configuration to the watch to facilitate the creation
of a workout. All I need to do is call that function when my workout on the
iPhone app starts and my watch app will respond by starting a HealthKit workout
session which starts measuring things like heart rate (and calculating its own
estimated calories burned).&lt;/p&gt;

&lt;p&gt;I was now able to get the heart rate as the watch was reading it, and update
whatever display on the watch I wanted to. That was only half the story though.
In Zwift the heart rate shows up in the user interface, and I wanted to mimic
that myself. Since I couldn‚Äôt access the workout session directly from the phone
I‚Äôd have to send the heart rate info back to the main app from the watch.&lt;/p&gt;

&lt;h3 id=&quot;back-to-the-app&quot;&gt;Back to the App&lt;/h3&gt;

&lt;p&gt;This blog post isn‚Äôt about Watch apps, so I won‚Äôt go over that aspect of this
feature too much. I basically used the WatchConnectivity session to send
messages back to the app with a dictionary containing the new heart rate.&lt;/p&gt;

&lt;p&gt;And after all of that programming, I‚Äôd like to present you with the most
difficult video I‚Äôve ever shot: on an iPad, while balancing on an exercise bike,
recording both my wrist watch with my heart rate AND the app showing the exact
same heart rate!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/zwift-part-3.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also rigged up an initial interface that shows which workout segment I‚Äôm on,
the next segment coming up, the progress through the segment and my progress for
the entire workout, along with stats like calories burned (determined by the
bike), cadence and distance traveled.&lt;/p&gt;

&lt;p&gt;At this point I have a pretty functional app! But seeing the extensive APIs of
HealthKit made me want to add more and more to my app. This is scope creep in
action. See the documentation of
&lt;a href=&quot;https://developer.apple.com/documentation/healthkit/hkworkoutbuilder&quot;&gt;HKWorkoutBuilder&lt;/a&gt;
to see all of the data and metadata you can store. I ended up sending a few more
messages from the app back to the watch so I could store more data to the
workout:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/a45a91a9d598a448db7ce3947d70304d.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;At the end of a workout, I send the start and end times, along with the total
calories burned and distance traveled. This isn‚Äôt really necessary because the
watch already makes a guess about the calories burned and the distance isn‚Äôt
real because it‚Äôs on a stationary bike. But I thought it might be interesting to
see how that data is represented.&lt;/p&gt;

&lt;p&gt;I also toyed around with sending segment data but I haven‚Äôt seen it visually
represented anywhere in the workout view. I wanted to see more detail about the
workout in the Apple Activity app so I also sent the name of the workout as the
&lt;em&gt;HKMetadataKeyWorkoutBrandName&lt;/em&gt; value, though I‚Äôm not sure that‚Äôs what it‚Äôs
intended for! Here‚Äôs what the workouts look like in the Activity app and the
Health app‚Äôs workout data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/workout.png&quot; /&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2019/sample.png&quot; /&gt;
&lt;img src=&quot;/blog/wp-content/uploads/2019/sample2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One more fun but optional thing I thought of and added was a wrist tap reminder
when I got close to the end of a segment. Sometimes I‚Äôm just in the zone and not
paying attention to the fact that I need to ramp up or ramp down for the next
section, so when there‚Äôs 5 seconds left in a segment I send a message to the
watch from the phone to tap my wrist and send a reminder:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/9ebc2061b51cd3b13b3acffbb9930398.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;One of the nice things about writing your own workout app is that you don‚Äôt need
to wait for a third party developer to implement any ideas you have for the app!
I think that‚Äôs actually the only nice thing‚Ä¶&lt;/p&gt;

&lt;p&gt;Anyway, I‚Äôm pretty happy with the results. Next up, I plan on adding a bit of
visual polish to the interface and maybe even create an app icon! I also want to
aggregate the data like heart rate info, watt effort, etc and keep track of
statistics and chart the data, perhaps in real time. I find it very motivating
to compare my effort in the same exact workout across different days to see if
I‚Äôm improving (maybe by measuring heart rate average).&lt;/p&gt;

&lt;p&gt;As usual, my changes are in &lt;a href=&quot;https://github.com/hungtruong/Zswift&quot;&gt;Github&lt;/a&gt; in
case you happen to have the same exact exercise bike as me or are curious how I
implemented certain things.&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;img src=&quot;/blog/wp-content/uploads/2019/healthkit.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I‚Äôve been a bit slow to update my blog series about trying to make a clone of
Zwift, but not because I‚Äôve stopped working on it. Rather, I‚Äôve been able to use
the ‚ÄúMVP‚Äù of what I‚Äôve built so far in parts
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;1&lt;/a&gt;
and
&lt;a href=&quot;/blog/2019/04/07/making-a-zwift-clone-part-2/&quot;&gt;2&lt;/a&gt;,
and I was finding that the time I spent working on my app could be used actually
working out. Like, I literally would write an implementation of something, but
it would take so much of my time that I couldn‚Äôt test it out and I‚Äôd have to go
to bed‚Ä¶ Still, I was missing a few important features in my app, so I‚Äôve been
slowly working on them in between working on my fitness.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 03 May 2019 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2019/05/03/making-a-zwift-clone-part-3/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2019/05/03/making-a-zwift-clone-part-3/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
      
      <item>
        <title>Making an iOS Zwift Clone to Save $15 a Month! Part 2: Reverse Engineering a Workout</title>
        <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwiftworkoutheader.png&quot; /&gt;
	&lt;figcaption&gt;A very colorful Zwift Workout&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Last time, on ‚ÄúMaking an iOS Zwift Clone to Save $15 a Month‚Äù I wrote about
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;learning Core Bluetooth to connect to my exercise bike and get data streaming
directly to my
app&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since writing that article, I cleaned up the implementation of the Core
Bluetooth service a bit and started supporting some additional data like
distance, calories burned and cycling cadence.&lt;/p&gt;

&lt;p&gt;While cycling on my exercise bike and staring at these numbers is fun, the
built-in screen on my bike already shows these numbers, so I essentially
recreated a subset of the official
&lt;a href=&quot;https://www.concept2.com/service/software/ergdata&quot;&gt;ergData&lt;/a&gt; app so far.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/ergdata.jpg&quot; /&gt;
	&lt;figcaption&gt;The ergData app is functional but ugly af&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I realized the next challenge would be to start a guided workout in my app and
show the target wattage alongside my actual wattage on the bike.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;enter-the-workout&quot;&gt;Enter the Workout&lt;/h3&gt;

&lt;p&gt;Zwift workouts seem pretty simple to begin with. You can set up a number of
workout segments to guide your workout. There are warmups, steady states,
intervals, free ride sections and cool-downs. At any given point in your
workout, Zwift will tell you to put in a certain amount of effort. You just try
to get your watt number to match what Zwift wants you to do.&lt;/p&gt;

&lt;p&gt;At first I figured I could come up with my own way of representing workouts, but
that would also require me to re-create all of the workouts I usually use on
Zwift. Zwift also has a nice workout editor that can use to edit preset workouts
or create completely new ones on your own.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwiftworkoutedit.png&quot; /&gt;
	&lt;figcaption&gt;Zwift‚Äôs built-in workout editor is pretty nice!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I discovered that if you copy an existing workout to edit it, the workout will
be saved as user data and will be synced between all devices. How is the workout
data saved, you ask? No, not as JSON as a sane developer would use. It‚Äôs XML.
Okay, so technically it‚Äôs a .zwo file but I know what XML looks like! Here‚Äôs a
sample of an exported workout file:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwofile.png&quot; /&gt;
	&lt;figcaption&gt;Hello darkness, my old friend&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So I went ahead and implemented an XMLParser in Swift to turn .zwo files into my
own Workout struct. I won‚Äôt bore you with the implementation details but it was
kind of a walk down memory lane dealing with the delegation-based API of
XMLParser. I‚Äôm surprised there isn‚Äôt a more modern solution in Foundation but
I‚Äôm guessing it‚Äôs because most people have moved on from XML.&lt;/p&gt;

&lt;h3 id=&quot;lets-reverse-engineering&quot;&gt;Let‚Äôs Reverse Engineering&lt;/h3&gt;

&lt;p&gt;By reading the .zwo files, I was able to figure out how Zwift represents their
workouts and basically reverse engineer the structure of a workout:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Warmups have a duration, a low and high power (which is a percentage of
&lt;a href=&quot;https://zwift.com/news/4100-zwift-how-to-understanding-finding-your-ftp&quot;&gt;FTP&lt;/a&gt;).
The warmup begins at the low power and steadily ramps up to the high power by
the end of the segment.&lt;/li&gt;
  &lt;li&gt;SteadyStates are the simplest, with just a duration and power.&lt;/li&gt;
  &lt;li&gt;Intervals are the most complicated, with a repeat number, onDuration,
offDuration, onPower and offPower. The intervals segment goes between the high
and low power levels, and repeats a certain number of times. I didn‚Äôt want to
deal with the extra complexity of this type so I just wrote some code to expand
them out into SteadyStates.&lt;/li&gt;
  &lt;li&gt;Cooldowns are like warmups but in reverse.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are also some other things like text captions that appear during a
workout, free ride segments and cadence settings that I didn‚Äôt bother to
implement because I don‚Äôt use them. The cadence stuff makes sense if you‚Äôre
using a smart trainer but my bike doesn‚Äôt support automatically changing
resistance anyway.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/55e92115f993221693f9040431ee39fe.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;I decided to represent the WorkoutSegment as a Swift enum since it needs to
support a bunch of different formats with varying levels of complexity. I‚Äôm
actually kinda curious how the Zwift team ended up representing these and if
they did something similar.&lt;/p&gt;

&lt;p&gt;From here it‚Äôs pretty simple to get stuff like the total duration or the target
wattage for a particular time offset, whether I need to calculate that (in the
case of a warmup or cooldown) or if I just return the constant value.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/hungtruong/718f50f48c32b324a2f25a80c78ae46b.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Aside from representing the workout segments, the actual workout object will
consist of things like the name, description, FTP and other information that
pertains to the state of the workout like time elapsed. I wrote a function that
takes the current time elapsed in the workout, loops through and finds the
corresponding segment and returns the desired wattage. There might be a more
efficient way to do that but for now it works.&lt;/p&gt;

&lt;p&gt;I also added some things to my project for displaying and choosing workouts from
a list, and actually displaying the target wattage alongside actual wattage. I
want to improve the interface now since it‚Äôs terrible, but I‚Äôll save that for
another blog post. If you want to check out the project, &lt;a href=&quot;https://github.com/hungtruong/Zswift&quot;&gt;it‚Äôs all here on
Github&lt;/a&gt;, including the playgrounds I used
to test out the workout segment logic before adding it to the Xcode project.&lt;/p&gt;

&lt;p&gt;Next on my list of tasks is to make the workout interface a lot prettier/usable,
and to add heart rate information from my Apple Watch.&lt;/p&gt;
</description>
        
          <description>&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/zwiftworkoutheader.png&quot; /&gt;
	&lt;figcaption&gt;A very colorful Zwift Workout&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Last time, on ‚ÄúMaking an iOS Zwift Clone to Save $15 a Month‚Äù I wrote about
&lt;a href=&quot;/blog/2019/03/17/making-a-zwift-clone-part-1/&quot;&gt;learning Core Bluetooth to connect to my exercise bike and get data streaming
directly to my
app&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since writing that article, I cleaned up the implementation of the Core
Bluetooth service a bit and started supporting some additional data like
distance, calories burned and cycling cadence.&lt;/p&gt;

&lt;p&gt;While cycling on my exercise bike and staring at these numbers is fun, the
built-in screen on my bike already shows these numbers, so I essentially
recreated a subset of the official
&lt;a href=&quot;https://www.concept2.com/service/software/ergdata&quot;&gt;ergData&lt;/a&gt; app so far.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/blog/wp-content/uploads/2019/ergdata.jpg&quot; /&gt;
	&lt;figcaption&gt;The ergData app is functional but ugly af&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I realized the next challenge would be to start a guided workout in my app and
show the target wattage alongside my actual wattage on the bike.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 07 Apr 2019 00:00:00 -0700</pubDate>
        <link>https://www.hung-truong.com/blog/2019/04/07/making-a-zwift-clone-part-2/</link>
        <guid isPermaLink="true">https://www.hung-truong.com/blog/2019/04/07/making-a-zwift-clone-part-2/</guid>
        
        
        <category>iOS</category>
        
        <category>Tech</category>
        
      </item>
      
    
  </channel>
</rss>
