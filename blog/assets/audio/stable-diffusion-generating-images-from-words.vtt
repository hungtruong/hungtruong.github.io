WEBVTT

00:00:00.000 --> 00:00:03.516
Stable Diffusion: Generating Images From Words

00:00:05.016 --> 00:00:07.813
Published on August 28, 2022.

00:00:08.813 --> 00:00:18.403
For the past few months or so, I’ve been noticing a lot of news stories about DALL·E 2, an AI image generator that uses GANs to create images from prompts.

00:00:18.903 --> 00:00:24.177
It was private for quite a while, and there were some similar, less powerful projects that were open.

00:00:24.677 --> 00:00:28.193
I played around with them a bit but I was waiting for a general release.

00:00:28.693 --> 00:00:39.322
I ended up getting into the DALL·E 2 beta a few weeks ago and last week I saw news that there was a new release of another project called Stable Diffusion, so I installed it on my MacBook.

00:00:39.822 --> 00:00:42.060
The results really blew me away!

00:00:43.060 --> 00:00:43.939
Getting it working

00:00:45.439 --> 00:00:51.432
It wasn’t too bad getting the release of Stable Diffusion working on my Mac as I just went through some of the steps on this site I found.

00:00:51.932 --> 00:00:59.923
There were some gotchas, like needing to install Rust to compile something and maybe a few files to change around, but for the most part it worked pretty quickly.

00:01:00.423 --> 00:01:07.056
I could probably have written down the steps but I’m sure it will be a lot more simple when they add more specific support for Apple Silicon.

00:01:08.056 --> 00:01:13.490
Right now it takes about 3 minutes to create an image with my M2 MacBook Air, which is kind of slow.

00:01:13.990 --> 00:01:18.785
I ended up using Google Collab which lets you use their GPUs for free.

00:01:19.285 --> 00:01:22.082
The next day though, I found I was rate limited.

00:01:22.582 --> 00:01:30.094
So I moved over to Kaggle which at least tells you what your GPU limits are, and they seem pretty reasonable (like 30 hours a week at least).

00:01:30.594 --> 00:01:40.264
The Kaggle notebook has been my main workflow so far because it’s quite fast (like 10 times faster than my Mac) and the short feedback loop really helps with coming up with prompts.

00:01:41.264 --> 00:01:42.942
Prompt Engineering

00:01:44.442 --> 00:01:45.960
So What is a prompt anyway?

00:01:46.460 --> 00:01:51.415
A prompt is what you type in to the image generator to describe what you want it to create for you.

00:01:51.915 --> 00:01:56.310
If you say you want a picture of a cat, it’ll likely come up with a pretty good cat.

00:01:56.810 --> 00:02:02.404
But if you want, you can also describe the cat in more detail to get a more specific image.

00:02:02.904 --> 00:02:08.577
You could add the breed, for example, or say it’s a robot cat, or make it sit on a park bench.

00:02:09.077 --> 00:02:15.311
There are so many possibilities of what kind of cat to show that the prompt ends up being incredibly important to get what you want.

00:02:16.311 --> 00:02:19.427
There’s been a lot of research done on prompt engineering.

00:02:19.927 --> 00:02:24.962
Some of it feels like a shortcut, by asking for an image in the style of a famous artist.

00:02:25.462 --> 00:02:29.937
You can also just describe the medium of the image, like a water color or illustration.

00:02:30.437 --> 00:02:39.227
I’ve noticed people on Reddit adding words like “trending on artstation” which I guess is a way to suggest that the image is aesthetically pleasing to a majority of people.

00:02:39.727 --> 00:02:44.362
You can also say something was painted badly, which is kind of hilarious.

00:02:45.362 --> 00:02:52.554
Part of the fun of playing around with these tools is that they’re so new that it’s possible to find words that can create certain images that no one else knows about.

00:02:53.054 --> 00:03:00.407
For me, it brings back the feeling of being on the early internet, when not everything was indexed by Google to the point where there were no hidden gems.

00:03:00.907 --> 00:03:04.423
Someone should bring back “Cool Site of the Day” but for prompts!

00:03:05.423 --> 00:03:06.382
Different Techniques

00:03:07.882 --> 00:03:12.836
I’ve learned that there are a few different techniques to make images, and I’m learning quite a few more.

00:03:13.336 --> 00:03:16.532
The simplest one is text to image, which I just described.

00:03:17.032 --> 00:03:28.541
The way I understand it is that basically you start with some random noise, and then two AIs work to alter the noise to turn it into the image you want by iterating changes and measuring how closely the image matches your description.

00:03:29.041 --> 00:03:34.555
That’s probably really simplified and maybe wrong but whatever, I’m not an AI engineer.

00:03:35.555 --> 00:03:40.669
Another way to create images is to start with a base image, and also feed the AI a prompt.

00:03:41.169 --> 00:03:48.761
Since you can choose the starting image (instead of just random noise), there’s a better chance that the image converges to something that resembles your input image.

00:03:49.261 --> 00:03:54.855
This gives you quite a bit more control over the final image’s general shape, composition, etc.

00:03:55.355 --> 00:03:58.311
You could feed it stick figures or a photo from your phone.

00:03:58.811 --> 00:04:03.766
I’ve seen people turn stick drawings into D&D character portraits using this technique.

00:04:04.766 --> 00:04:09.721
I tried this technique out by using a photo of my dog, Sodapop, sitting in the grass.

00:04:10.221 --> 00:04:13.417
The picture is pretty good, but it’s not award winning or anything.

00:04:13.917 --> 00:04:24.466
I fed the text “a watercolor illustration of a black and white cardigan corgi sitting in the middle of a green flowery meadow in front of an orange ball, masterpiece, big cute eyes”.

00:04:24.966 --> 00:04:28.882
I didn’t start with that, but I kept changing it to try and get an image that I wanted.

00:04:29.882 --> 00:04:33.797
I also played around with different strengths and amounts of iterations.

00:04:34.297 --> 00:04:39.492
I found that if I used too many iterations, the image didn’t really resemble Sodapop anymore.

00:04:39.992 --> 00:04:45.585
He’s a black and white Corgi, which is less common, so there’s probably more of a bias towards the sable and white ones.

00:04:46.085 --> 00:04:50.960
One thing I learned is that it’s better to just generate a huge number of images and then pick the ones you like.

00:04:51.460 --> 00:04:55.775
You can save the random seed value and use it to refine the image further as well.

00:04:56.275 --> 00:05:01.709
There were a lot of really terrible looking corgi watercolor images which my computer is full of now.

00:05:02.209 --> 00:05:04.766
But there were also some fairly good ones too!

00:05:05.266 --> 00:05:11.020
The power with this AI is that it’s pretty cheap to just make more images until you get what you want.

00:05:12.020 --> 00:05:13.139
Future Techniques

00:05:14.639 --> 00:05:30.943
There is another technique I tried recently where someone tried to create a bigger image (right now most video cards can only do 512x512 and that’s what the model is trained on) by creating an image, upscaling it and then running the image to image process on 9 square parts of the upscaled image.

00:05:31.443 --> 00:05:35.439
When I tried this, I found that it added weird artifacts into each square piece.

00:05:35.939 --> 00:05:39.295
It was recursively trying to fit the whole prompt into each square.

00:05:39.795 --> 00:05:45.149
My prompt was a garden, and it basically tried to add a garden into each subsquare of the image.

00:05:45.649 --> 00:05:53.001
This could have been due to a current bug where the random seed on Mac doesn’t really work, but I don’t have the hardware to try it on a non-Mac right now.

00:05:54.001 --> 00:06:04.310
I’ve had a lot more fun playing around with this image generation stuff than I have in a long time with technology, so I ordered a new graphics card so I can iterate on things more quickly on my own infrastructure.

00:06:04.810 --> 00:06:11.923
There’s something really magical about using a model file that’s only a few gigabytes to basically create any image you can think of.

00:06:12.423 --> 00:06:17.217
If my internet connection ever goes down for the count, this could be my main source of entertainment.

00:06:18.217 --> 00:06:20.375
There’s a bunch of other things I want to try.

00:06:20.875 --> 00:06:27.028
There’s a technique called “Textual Inversion” where you can sort of re-train (but not really) the model to use a personalized word.

00:06:27.528 --> 00:06:31.843
I could do this with Sodapop so I stop getting Pembroke Corgis when I want my Corgi.

00:06:32.343 --> 00:06:40.495
I was also wondering if I could use it with pictures of myself, since Stable Diffusion seems to work really well with making images with well known celebrities in them.

00:06:41.495 --> 00:06:47.728
When I first saw this technology I figured it would be good for creating blog post images (which obviously it was for this post).

00:06:48.228 --> 00:06:56.219
I’m also envisioning things like services for creating customized watercolor portraits for your dog, or custom fantasy avatars for a person.

00:06:56.719 --> 00:07:01.674
I think people have just barely scratched the surface here so hopefully there’s a lot more interesting stuff coming up.
