WEBVTT

00:00:00.000 --> 00:00:02.877
Fun With Wan 2.2 Text To Video AI

00:00:04.377 --> 00:00:07.174
Published on August 30, 2025.

00:00:08.174 --> 00:00:13.528
This blog post is pretty much also a Youtube video, so if you want, you can just watch that instead!

00:00:14.528 --> 00:00:20.361
I recently got interested in text to video AI generators after reading a post about Wan 2.2.

00:00:20.861 --> 00:00:30.931
I hadn’t really been interested in them before because they all kind of sucked, and they either took too much VRAM or were proprietary ones that you couldn’t run locally like Veo or Sora, etc.

00:00:31.931 --> 00:00:40.801
But after looking at some of the videos that Wan 2.2 could generate, and seeing that it could run on some pretty standard hardware, I decided to look into it a bit more.

00:00:41.801 --> 00:00:43.080
Making a LoRA

00:00:44.580 --> 00:00:49.215
Whenever I use these AI image generators, I always like to make images of myself.

00:00:49.715 --> 00:00:59.065
Call it narcissism or whatever, but I think it’s really fun to throw myself into impossible and weird situations, or make paintings of myself as a Roman emperor or whatever.

00:00:59.565 --> 00:01:05.239
But to make the AI generate images that look even the least bit like me, I need to train a LoRA.

00:01:05.739 --> 00:01:10.933
I’ve done this already before with image generators like Stable Diffusion and, more recently, Flux.

00:01:11.933 --> 00:01:17.527
I used a tool called ai-toolkit to train the Flux LoRA on Runpod around a year ago.

00:01:18.027 --> 00:01:27.857
The quality of that image generation model was quite good, but I quickly forgot about it after I tried making a bunch of art for my Samsung Frame TV of myself which didn’t turn out that great.

00:01:28.357 --> 00:01:34.830
When researching ways to train a LoRA for Wan 2.2, I found that ai-toolkit already had support for it.

00:01:35.330 --> 00:01:43.241
I still had the config files and image dataset that I used to train the Flux model, so I figured it would be pretty easy to do the Wan 2.2 one too.

00:01:44.241 --> 00:01:48.876
The training took about 2 hours running on a 24GB VRAM GPU.

00:01:49.376 --> 00:01:51.614
I think it was an A5000.

00:01:52.114 --> 00:01:57.787
The interesting thing about training is that I didn’t need to use videos to train the video model, just images.

00:01:58.287 --> 00:02:06.199
After I loaded the LoRA into the default ComfyUI workflow for Wan 2.2, I was able to put myself into a bunch of different situations!

00:02:07.199 --> 00:02:09.277
Making Use of AI Videos

00:02:10.777 --> 00:02:17.569
The main issue that pops up with these video generation AI models is that the length of the videos has to be pretty short.

00:02:18.069 --> 00:02:25.661
The other problem is that it’s hard to have consistency with characters and scenes between clips, especially if you’re only using text as a prompt to generate the video.

00:02:26.161 --> 00:02:34.712
I’ve seen some really shitty videos that people were really proud of where the characters all look slightly different between scenes, and the vehicle they’re riding in changes wildly.

00:02:35.212 --> 00:02:39.128
Yet somehow they think their AI video is the next summer blockbuster.

00:02:40.128 --> 00:02:47.640
When Google’s Veo came out (I think it was Veo 3 or 4?) I saw a video that someone made that I thought was really clever.

00:02:48.140 --> 00:02:51.736
It was a fake news story about a synchronized swimming team of cats.

00:02:52.236 --> 00:03:00.627
I thought it was really smart because news stories consist of a bunch of short clips, and there wouldn’t really need to be a whole lot of consistency between the scenes.

00:03:01.627 --> 00:03:05.622
Any good artist knows to work within the constraints of whatever tools they’re using.

00:03:06.122 --> 00:03:10.677
I had a grand idea to create an 80s sitcom intro where every character was myself.

00:03:11.177 --> 00:03:14.773
You know how those videos always had someone looking at the camera and smiling?

00:03:15.273 --> 00:03:16.951
Or doing something wacky?

00:03:17.451 --> 00:03:24.484
Those scenes are always just a few seconds long, and they don’t really need to be consistent between each other since they usually super random anyway.

00:03:24.984 --> 00:03:26.582
So I started prompting.

00:03:27.082 --> 00:03:29.719
Here are a few examples that I really liked:

00:03:30.719 --> 00:03:37.432
I made this revolving door shot of myself and every time I see this video I’m just amazed by the look on my (my?) face.

00:03:37.932 --> 00:03:42.007
I mean, it doesn’t even look exactly like me, but I just think it’s so funny.

00:03:43.007 --> 00:03:47.402
I like how the AI de-aged me and the setting really does look like an 80s television show.

00:03:48.402 --> 00:03:53.597
I had AI make me talk on a corded phone wearing a blazer with rolled up sleeves.

00:03:54.097 --> 00:03:56.414
It doesn’t get more 80s than this!

00:03:56.914 --> 00:04:03.787
I really liked how the lighting worked in this one, where you can see the shadow that was cast by the person as well as the cord of the phone itself!

00:04:04.787 --> 00:04:07.983
I also made AI make me totally ripped at the beach.

00:04:08.483 --> 00:04:15.595
This actually closely resembles the training images that I fed to AI, so it hardly had to do any work at all to make this video.

00:04:16.595 --> 00:04:20.191
AI made me a doctor, which is something my parents can finally be proud of.

00:04:20.691 --> 00:04:29.882
Also I made myself eat a sandwich, which  I thought was an original thought but then I realized that I stole it from Weird Al’s Like a Surgeon video (or is it Weird AI?).

00:04:30.382 --> 00:04:35.176
I love this clip because the way he (I?) chews is so, so funny to me.

00:04:36.176 --> 00:04:38.893
Anyway, here’s the whole video if you want to watch it!

00:04:39.893 --> 00:04:41.012
The Music

00:04:42.512 --> 00:04:51.622
Since I wanted to make an 80s sitcom intro, and all 80s sitcom intros have banger music in them, I was thinking I could write a banger 80s sitcom intro hit myself.

00:04:52.122 --> 00:04:59.634
But then I realized that I’m not really good at writing songs or singing, and I just know how to play the trumpet, so I figured I’d just use AI for this, too.

00:05:00.134 --> 00:05:03.890
I’m honestly not a huge fan of AI music, as a musician myself.

00:05:04.390 --> 00:05:08.865
But I figured it would be appropriate given the context of the video being completely AI too.

00:05:09.865 --> 00:05:14.500
I wrote the lyrics to my song myself, and then I used Suno to generate the music.

00:05:15.000 --> 00:05:19.395
It took quite a few generations and prompting with styles to get a halfway decent song.

00:05:19.895 --> 00:05:23.491
I think that Suno doesn’t really know how to write a unique melody.

00:05:24.491 --> 00:05:32.403
My theory for this is that there’s probably some code or something embedded in the model that prevents Suno from using a copyrighted melody from a real song.

00:05:32.903 --> 00:05:41.534
So while it can come up with a decent chord progression and the instruments and vocals all sound real, the thing is so scared of getting sued that it won’t come up with a melody.

00:05:42.034 --> 00:05:46.029
Like, can you imagine if you asked Suno for a song and it just gave you Careless Whisper?

00:05:47.029 --> 00:05:48.148
What’s Next?

00:05:49.648 --> 00:05:55.322
I make a lot of videos, whether that’s for Hoagie’s Youtube  , or my Youtube, or my other Youtube for stupid stuff.

00:05:55.822 --> 00:06:05.651
I genuinely like making videos, so it’s sometimes weird to me that people who are making these AI videos think they will replace everyone who is currently involved in video production.

00:06:06.151 --> 00:06:17.180
Like I know that “AI replacing everyone” is a trope, but another popular trope is that people are complaining because AI is making art while they do repetitive boring tasks, where it should be the other way around.

00:06:18.180 --> 00:06:25.212
There have been a lot of complaints about AI slop too, whether that’s text slop or image slop or video slop.

00:06:25.712 --> 00:06:33.944
I mentioned before that I’ve seen so many cringey videos that people post thinking that they’ve made something incredible when it’s about the worst thing I’ve ever seen.

00:06:34.944 --> 00:06:43.095
I think the problem is that when you make art, it can suck or it can be good, and you eventually get a good understanding of which it is that you made while you are learning to make it.

00:06:43.595 --> 00:06:47.031
Usually people start off bad at it, and then they get better.

00:06:47.531 --> 00:06:52.965
Through this process, you develop taste and an understanding of what works and what doesn’t.

00:06:53.465 --> 00:07:01.616
The experience you get from putting time into it will shape your ability to filter and be your harshest critic, which helps you make better art that is worth sharing with people.

00:07:02.616 --> 00:07:12.366
When someone prompts an AI to make an image, and the image comes out sufficiently pleasing, they assume that there’s some sort of skill involved at the same level of making that image from scratch.

00:07:12.866 --> 00:07:22.936
I don’t think that’s the case, and while I’m not even anti-AI, I do think that there’s something to be said about learning a craft that you dont get by asking an AI to do it (obviously).

00:07:23.936 --> 00:07:33.366
I’m not saying that I’d never use AI as part of my usual workflow to make videos (in fact, I’ve used AI that cleans up audio for my poor microphone conditions before, and it turned out pretty good).

00:07:33.866 --> 00:07:39.300
But I think AI is still at the point that CGI in movies was some time in the late 90s.

00:07:39.800 --> 00:07:44.355
Like people will never stop talking about how bad the CGI was in the Scorpion King!

00:07:44.855 --> 00:07:50.848
And I’m sure there will be uses of AI that people will talk about in a similar way that haven’t even been used yet!

00:07:51.348 --> 00:07:53.985
But at some point, maybe it’ll be good enough!
